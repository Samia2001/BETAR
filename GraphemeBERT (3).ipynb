{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11124909,"sourceType":"datasetVersion","datasetId":6710212},{"sourceId":11296641,"sourceType":"datasetVersion","datasetId":7063017},{"sourceId":56830,"sourceType":"modelInstanceVersion","modelInstanceId":47680,"modelId":65478}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nimport numpy as np\nimport pandas as pd\nimport regex  # for grapheme cluster splitting\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:42.539777Z","iopub.execute_input":"2025-04-06T13:11:42.540137Z","iopub.status.idle":"2025-04-06T13:11:42.544119Z","shell.execute_reply.started":"2025-04-06T13:11:42.540108Z","shell.execute_reply":"2025-04-06T13:11:42.543348Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df = pd.read_json(\"/kaggle/input/thesisdata1/final_train.json\")\ntd = pd.read_json(\"/kaggle/input/thesisdata1/final_test.json\")\ntd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:42.880691Z","iopub.execute_input":"2025-04-06T13:11:42.881019Z","iopub.status.idle":"2025-04-06T13:11:42.948342Z","shell.execute_reply.started":"2025-04-06T13:11:42.880991Z","shell.execute_reply":"2025-04-06T13:11:42.947477Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                                  text  \\\n0    আলমের গ্লোকোমা চিকিৎসার জন্য ১২ সপ্তাহের লেজার...   \n1    রাটাটের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...   \n2    রাটাটের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...   \n3    মাহিনের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...   \n4    মাহিনের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...   \n..                                                 ...   \n295  গবেষণায় দেখা গেছে, ব্যক্তির পারফরম্যান্স শুধু...   \n296  এপিডার্মিস স্তরটি গড়ে ০.০৫ মিমি থেকে ১.৫ মিমি...   \n297  ত্বকের প্রায় ৬৪% পানি দ্বারা গঠিত। ​ত্বকের প্...   \n298  ভিটামিন সি: ত্বকের রঙ উন্নত করতে এবং অ্যান্টিঅ...   \n299  সেন্টেলা অ্যাসিয়াটিকা: আর্দ্রতা বৃদ্ধি, স্থিত...   \n\n                                                 label  \n0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n..                                                 ...  \n295  [0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, ...  \n296  [1, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...  \n297  [0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, ...  \n298  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n299  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n\n[300 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>আলমের গ্লোকোমা চিকিৎসার জন্য ১২ সপ্তাহের লেজার...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>রাটাটের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>রাটাটের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>মাহিনের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>মাহিনের ম্যাকুলার ডিজেনারেশনের চিকিৎসার জন্য ৩...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>295</th>\n      <td>গবেষণায় দেখা গেছে, ব্যক্তির পারফরম্যান্স শুধু...</td>\n      <td>[0, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>296</th>\n      <td>এপিডার্মিস স্তরটি গড়ে ০.০৫ মিমি থেকে ১.৫ মিমি...</td>\n      <td>[1, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>297</th>\n      <td>ত্বকের প্রায় ৬৪% পানি দ্বারা গঠিত। ​ত্বকের প্...</td>\n      <td>[0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>298</th>\n      <td>ভিটামিন সি: ত্বকের রঙ উন্নত করতে এবং অ্যান্টিঅ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>299</th>\n      <td>সেন্টেলা অ্যাসিয়াটিকা: আর্দ্রতা বৃদ্ধি, স্থিত...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>300 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"raw_text       = df[\"text\"].tolist()\nraw_label      = df[\"label\"].tolist()\nraw_test_text  = td[\"text\"].tolist()\nraw_test_label = td[\"label\"].tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:43.133595Z","iopub.execute_input":"2025-04-06T13:11:43.133855Z","iopub.status.idle":"2025-04-06T13:11:43.138253Z","shell.execute_reply.started":"2025-04-06T13:11:43.133822Z","shell.execute_reply":"2025-04-06T13:11:43.137380Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model_checkpoint = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\nbert_model = AutoModel.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:45.995787Z","iopub.execute_input":"2025-04-06T13:11:45.996149Z","iopub.status.idle":"2025-04-06T13:11:47.953159Z","shell.execute_reply.started":"2025-04-06T13:11:45.996119Z","shell.execute_reply":"2025-04-06T13:11:47.952195Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Hyperparameters\nsequence_length = 256  # Max subword length\nnum_labels = 4         # e.g. 0,1,2,3\netag = 4               # Padding label\nftag = 3               # Tag for sub-word tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:47.954399Z","iopub.execute_input":"2025-04-06T13:11:47.954642Z","iopub.status.idle":"2025-04-06T13:11:47.958268Z","shell.execute_reply.started":"2025-04-06T13:11:47.954611Z","shell.execute_reply":"2025-04-06T13:11:47.957479Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# For CharBERT\nMAX_CHAR_LEN = 15      # Max graphemes per token\nCHAR_VOCAB_SIZE = 800 # Adjust as needed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:47.959360Z","iopub.execute_input":"2025-04-06T13:11:47.959661Z","iopub.status.idle":"2025-04-06T13:11:47.971673Z","shell.execute_reply.started":"2025-04-06T13:11:47.959631Z","shell.execute_reply":"2025-04-06T13:11:47.970850Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# 2) Grapheme-Level Tokenizer\n########################################\nimport unicodedata\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ndef normalize_bengali_text(text):\n    \"\"\"Normalize text to NFKC for consistent vowel/conjunct representation.\"\"\"\n    return unicodedata.normalize('NFKC', text)\n\ndef grapheme_split(word):\n    \"\"\"\n    Splits a single word into a list of grapheme clusters.\n    Using regex \\\\X to match any Unicode extended grapheme cluster.\n    \"\"\"\n    word = normalize_bengali_text(word)\n    return regex.findall(r'\\X', word)  # returns a list of graphemes\n\n# Fit a Keras tokenizer on all graphemes found in the dataset\nchar_tokenizer = Tokenizer(num_words=CHAR_VOCAB_SIZE, char_level=False, oov_token=\"<OOV>\")\n\n# Collect all graphemes from the entire dataset\nall_words = []\nfor txt in df[\"text\"].tolist():\n    # naive split by space\n    for w in txt.split():\n        # gather graphemes\n        g_list = grapheme_split(w)\n        all_words.append(\" \".join(g_list))  \n        # \" \".join(...) makes each grapheme appear as a 'token' for the Keras Tokenizer\n\n# Fit on this pseudo-labeled text\nchar_tokenizer.fit_on_texts(all_words)\n\n########################################\n# 3) Custom Tokenizer for Words\n########################################\n# You mentioned special punctuation logic; let's keep it if it’s relevant.\nspecial_character = ['(', ')','%',':','-','$','—',\"'\",';','‘','’']\npunctuation_marks = {',', '।', '!', '?'}\n\ndef custom_tokenize(text):\n    \"\"\"\n    Splits punctuation from words, etc. \n    Adjust for your language-specific rules if needed.\n    \"\"\"\n    space_split_tokens = text.split()\n    tokens1 = []\n    # 1) separate punctuation at end\n    for token in space_split_tokens:\n        if token and token[-1] in punctuation_marks:\n            tokens1.append(token[:-1])  # word w/o punctuation\n            tokens1.append(token[-1])   # punctuation as separate token\n        else:\n            tokens1.append(token)\n\n    # 2) separate special chars\n    tokens = []\n    for token in tokens1:\n        tmp = \"\"\n        for ch in token:\n            if ch not in special_character:\n                tmp += ch\n            else:\n                if tmp:\n                    tokens.append(tmp)\n                    tmp = \"\"\n                tokens.append(ch)\n        if tmp:\n            tokens.append(tmp)\n    return tokens\n\n########################################\n# 4) Dataset + Collation\n########################################\nclass NERDataset(Dataset):\n    \"\"\"\n    Creates PyTorch dataset with BERT subwords + CharBERT graphemes.\n    \"\"\"\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n        self.inputs, self.all_labels = self.build_inputs(texts, labels)\n\n    def __len__(self):\n        return len(self.all_labels)\n\n    def __getitem__(self, idx):\n        # return dictionary for a single sample\n        return {\n            \"input_ids\":      self.inputs[\"input_ids\"][idx],\n            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n            \"token_type_ids\": self.inputs[\"token_type_ids\"][idx],\n            \"char_input\":     self.inputs[\"char_input\"][idx],  # graphemes\n            \"labels\":         self.all_labels[idx]\n        }\n\n    def build_inputs(self, text_list, label_list):\n        input_ids_list, attention_mask_list, token_type_list, tag_list = [], [], [], []\n        char_sequences = []\n\n        for sentence, labels in zip(text_list, label_list):\n            tokens = custom_tokenize(sentence)\n            new_labels = []\n            tokenized_ids = []\n            char_seq = []\n\n            # 1) BERT Subword Tokenization + Expand labels\n            for word, tag in zip(tokens, labels):\n                sub_tokens = tokenizer.tokenize(word)\n                sub_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n                # Expand labels to subwords\n                labels_expanded = [tag] + ([ftag] * (len(sub_tokens) - 1))\n\n                tokenized_ids.extend(sub_ids)\n                new_labels.extend(labels_expanded)\n\n                # 2) Build Char/Grapheme sequence for each sub-token\n                # We'll do only one set of graphemes per *original word*, \n                # then replicate for sub_tokens\n                g_list = grapheme_split(word)\n                # Convert to 'text' form so Keras Tokenizer can map to int\n                # We'll just join with space\n                grapheme_text = \" \".join(g_list)  \n                grapheme_ids = char_tokenizer.texts_to_sequences([grapheme_text])[0]\n                # Pad or truncate\n                grapheme_ids = grapheme_ids[:MAX_CHAR_LEN]\n                grapheme_ids += [0] * (MAX_CHAR_LEN - len(grapheme_ids))\n\n                # For each sub-token of the word, we replicate the same char seq\n                for _ in sub_tokens:\n                    char_seq.append(grapheme_ids)\n\n            # 3) Add [CLS] and [SEP]\n            tokenized_ids = [tokenizer.cls_token_id] + tokenized_ids + [tokenizer.sep_token_id]\n            new_labels = [0] + new_labels + [0]\n            attention_mask = [1] * len(tokenized_ids)\n            token_type_ids = [0] * len(tokenized_ids)\n\n            # For char_input, replicate [CLS]/[SEP] as zero-vector\n            cls_chars = [0] * MAX_CHAR_LEN\n            sep_chars = [0] * MAX_CHAR_LEN\n            char_seq = [cls_chars] + char_seq + [sep_chars]\n\n            # 4) Truncate & Pad to sequence_length\n            cur_len = len(tokenized_ids)\n            if cur_len > sequence_length:\n                tokenized_ids = tokenized_ids[:sequence_length]\n                new_labels   = new_labels[:sequence_length]\n                attention_mask = attention_mask[:sequence_length]\n                token_type_ids = token_type_ids[:sequence_length]\n                char_seq = char_seq[:sequence_length]\n            elif cur_len < sequence_length:\n                pad_len = sequence_length - cur_len\n                tokenized_ids   += [0] * pad_len\n                new_labels      += [etag] * pad_len\n                attention_mask  += [0] * pad_len\n                token_type_ids  += [0] * pad_len\n                for _ in range(pad_len):\n                    char_seq.append([0]*MAX_CHAR_LEN)\n\n            # Final shape checks\n            # assert len(tokenized_ids) == sequence_length\n            # assert len(new_labels)    == sequence_length\n            # assert len(attention_mask)== sequence_length\n            # assert len(token_type_ids)== sequence_length\n            # assert len(char_seq)      == sequence_length\n            # Final shape checks\n            if not (len(tokenized_ids) == sequence_length and \n                    len(new_labels) == sequence_length and \n                    len(attention_mask) == sequence_length and \n                    len(token_type_ids) == sequence_length and \n                    len(char_seq) == sequence_length):\n                \n                print(\"\\n❗ Assertion failed on one sample:\")\n                print(\"Original sentence:\", sentence)\n                print(\"Tokenized words:\", tokens)\n                print(\"Subword token IDs:\", tokenized_ids)\n                print(\"New labels:\", new_labels)\n                print(\"char_seq shape:\", len(char_seq))\n                print(\"Lengths => tokenized_ids:\", len(tokenized_ids), \n                      \"| new_labels:\", len(new_labels), \n                      \"| attention_mask:\", len(attention_mask), \n                      \"| token_type_ids:\", len(token_type_ids), \n                      \"| char_seq:\", len(char_seq))\n                \n                raise AssertionError(\"Mismatch in sequence lengths\")\n            \n            # If passes, continue with tensor conversion\n\n\n            # Convert to tensors\n            input_ids_list.append(torch.tensor(tokenized_ids, dtype=torch.long))\n            attention_mask_list.append(torch.tensor(attention_mask, dtype=torch.long))\n            token_type_list.append(torch.tensor(token_type_ids, dtype=torch.long))\n            tag_list.append(torch.tensor(new_labels, dtype=torch.long))\n            char_sequences.append(torch.tensor(char_seq, dtype=torch.long))\n\n        inputs_dict = {\n            \"input_ids\":      torch.stack(input_ids_list),\n            \"attention_mask\": torch.stack(attention_mask_list),\n            \"token_type_ids\": torch.stack(token_type_list),\n            \"char_input\":     torch.stack(char_sequences)\n        }\n        labels_tensor = torch.stack(tag_list)\n        return inputs_dict, labels_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:47.972550Z","iopub.execute_input":"2025-04-06T13:11:47.972820Z","iopub.status.idle":"2025-04-06T13:11:49.902038Z","shell.execute_reply.started":"2025-04-06T13:11:47.972787Z","shell.execute_reply":"2025-04-06T13:11:49.901330Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"########################################\n# Build Datasets & DataLoaders\n########################################\nimport random\ndata = list(zip(raw_text, raw_label))\ntdata= list(zip(raw_test_text, raw_test_label))\nrandom.shuffle(data)\nrandom.shuffle(tdata)\n\nshuffled_text,  shuffled_label  = zip(*data)\nshuffled_ttext, shuffled_tlabel = zip(*tdata)\n\nsplit_1 = int(0.8 * len(shuffled_text))\ntrain_dataset = NERDataset(shuffled_text[:split_1],  shuffled_label[:split_1])\nvalid_dataset = NERDataset(shuffled_text[split_1:], shuffled_label[split_1:])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n\n########################################\n# 5) Define the CharBERT Model\n########################################\nimport torch.nn as nn\n\nclass CharEncoder(nn.Module):\n    \"\"\"\n    A GRU-based encoder for graphemes, akin to CharBERT's character encoder.\n    \"\"\"\n    def __init__(self, char_vocab_size, char_embed_dim=30, hidden_dim=32):\n        super().__init__()\n        self.char_embedding = nn.Embedding(char_vocab_size, char_embed_dim, padding_idx=0)\n        self.gru = nn.GRU(\n            input_size=char_embed_dim,\n            hidden_size=hidden_dim,\n            batch_first=True,\n            bidirectional=True\n        )\n    def forward(self, char_input):\n        \"\"\"\n        char_input: (batch, seq_len, MAX_CHAR_LEN)\n        Output: (batch, seq_len, hidden_dim*2)\n        \"\"\"\n        b, sl, cl = char_input.shape\n        # Flatten for GRU\n        char_input = char_input.view(b*sl, cl)\n\n        emb = self.char_embedding(char_input)  # (b*sl, cl, char_embed_dim)\n        # forward GRU\n        _, h_n = self.gru(emb)  # h_n: (2, b*sl, hidden_dim)\n        # concat final states from both directions\n        char_rep = torch.cat([h_n[0], h_n[1]], dim=-1)  # (b*sl, hidden_dim*2)\n\n        # reshape back\n        char_rep = char_rep.view(b, sl, -1)\n        return char_rep\n\nclass CharBERT_NER(nn.Module):\n    def __init__(self, bert_model, num_labels, char_vocab_size):\n        super().__init__()\n        self.bert = bert_model\n        hidden_dim = self.bert.config.hidden_size  # 1024 for xlm-roberta-large\n\n        # Char GRU\n        self.char_encoder = CharEncoder(char_vocab_size, char_embed_dim=30, hidden_dim=32)\n        # Output dimension from char_encoder = 128 (64*2)\n\n        # BiLSTM for final sequence modeling\n        combined_dim = hidden_dim + 64\n        self.lstm = nn.LSTM(\n            input_size=combined_dim,\n            hidden_size=128,\n            num_layers=2,\n            bidirectional=True,\n            batch_first=True,\n            dropout=0.3\n        )\n        self.fc = nn.Linear(128*2, num_labels)\n\n    def forward(self, input_ids, attention_mask, token_type_ids, char_input):\n        # 1) BERT forward\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        sequence_output = outputs.last_hidden_state  # (batch, seq_len, 1024)\n\n        # 2) Char Encoder (GRU)\n        char_repr = self.char_encoder(char_input)    # (batch, seq_len, 128)\n\n        # 3) Fuse representations\n        fused = torch.cat([sequence_output, char_repr], dim=-1)  # (batch, seq_len, 1024+128)\n\n        # 4) BiLSTM\n        lstm_out, _ = self.lstm(fused)               # (batch, seq_len, 1024)\n        logits = self.fc(lstm_out)                   # (batch, seq_len, num_labels)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:11:51.090473Z","iopub.execute_input":"2025-04-06T13:11:51.090860Z","iopub.status.idle":"2025-04-06T13:12:04.103350Z","shell.execute_reply.started":"2025-04-06T13:11:51.090811Z","shell.execute_reply":"2025-04-06T13:12:04.102669Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"test_dataset = NERDataset(shuffled_ttext, shuffled_tlabel)\ntest_loader  = DataLoader(test_dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:46.373492Z","iopub.execute_input":"2025-04-06T13:13:46.373810Z","iopub.status.idle":"2025-04-06T13:13:48.227698Z","shell.execute_reply.started":"2025-04-06T13:13:46.373785Z","shell.execute_reply":"2025-04-06T13:13:48.227017Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import torch.optim as optim\nfrom torchmetrics import Precision, Recall, F1Score, Accuracy\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CharBERT_NER(bert_model, num_labels, CHAR_VOCAB_SIZE).to(device)\n\noptimizer = optim.AdamW([\n    {\"params\": model.bert.parameters(), \"lr\": 5e-6},  \n    {\"params\": model.lstm.parameters(), \"lr\": 2e-4},  \n    {\"params\": model.fc.parameters(),   \"lr\": 2e-4},\n    {\"params\": model.char_encoder.parameters(), \"lr\": 2e-4},\n])\n\ncriterion = torch.nn.CrossEntropyLoss(ignore_index=etag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:48.228770Z","iopub.execute_input":"2025-04-06T13:13:48.229048Z","iopub.status.idle":"2025-04-06T13:13:49.148469Z","shell.execute_reply.started":"2025-04-06T13:13:48.229027Z","shell.execute_reply":"2025-04-06T13:13:49.147531Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torchmetrics.classification import Precision, Recall, F1Score, Accuracy\n\nprecision = Precision(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\nrecall    = Recall(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\nf1        = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\naccuracy  = Accuracy(task=\"multiclass\", num_classes=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:52.071422Z","iopub.execute_input":"2025-04-06T13:13:52.071758Z","iopub.status.idle":"2025-04-06T13:13:52.079822Z","shell.execute_reply.started":"2025-04-06T13:13:52.071726Z","shell.execute_reply":"2025-04-06T13:13:52.079029Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def compute_metrics(preds, labels, ignore_index=etag):\n    # Flatten\n    preds = preds.view(-1)\n    labels= labels.view(-1)\n\n    mask = (labels != ignore_index)\n    preds, labels = preds[mask], labels[mask]\n\n    p = precision(preds, labels)\n    r = recall(preds, labels)\n    f = f1(preds, labels)\n    a = accuracy(preds, labels)\n    return p.item(), r.item(), f.item(), a.item()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:52.465114Z","iopub.execute_input":"2025-04-06T13:13:52.465389Z","iopub.status.idle":"2025-04-06T13:13:52.470185Z","shell.execute_reply.started":"2025-04-06T13:13:52.465367Z","shell.execute_reply":"2025-04-06T13:13:52.469380Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def evaluate_model(model, data_loader):\n    model.eval()\n    total_loss = 0\n    all_preds, all_labels = [], []\n\n    for batch in data_loader:\n        input_ids      = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        token_type_ids = batch[\"token_type_ids\"].to(device)\n        char_input     = batch[\"char_input\"].to(device)\n        labels         = batch[\"labels\"].to(device)\n\n        logits = model(input_ids, attention_mask, token_type_ids, char_input)\n        loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n        total_loss += loss.item()\n\n        all_preds.append(torch.argmax(logits, dim=-1).cpu())\n        all_labels.append(labels.cpu())\n\n    avg_loss = total_loss / len(data_loader)\n    all_preds  = torch.cat(all_preds)\n    all_labels = torch.cat(all_labels)\n    precision_, recall_, f1_, accuracy_ = compute_metrics(all_preds, all_labels)\n\n    return float(avg_loss), precision_, recall_, f1_, accuracy_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:54.768250Z","iopub.execute_input":"2025-04-06T13:13:54.768532Z","iopub.status.idle":"2025-04-06T13:13:54.774370Z","shell.execute_reply.started":"2025-04-06T13:13:54.768510Z","shell.execute_reply":"2025-04-06T13:13:54.773626Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import torch\n\ndef train_model(model, train_loader, valid_loader, epochs=10, patience=3):\n    best_val_loss = float(\"inf\")\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        all_preds, all_labels = [], []\n\n        for batch in train_loader:\n            input_ids      = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)\n            char_input     = batch[\"char_input\"].to(device)\n            labels         = batch[\"labels\"].to(device)\n\n            optimizer.zero_grad()\n            logits = model(input_ids, attention_mask, token_type_ids, char_input)\n            loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            preds = torch.argmax(logits, dim=-1).cpu()\n            all_preds.append(preds)\n            all_labels.append(labels.cpu())\n\n        avg_train_loss = total_loss / len(train_loader)\n        all_preds  = torch.cat(all_preds)\n        all_labels = torch.cat(all_labels)\n        train_p, train_r, train_f1, train_a = compute_metrics(all_preds, all_labels)\n\n        # Validation\n        val_loss, val_p, val_r, val_f1, val_a = evaluate_model(model, valid_loader)\n\n        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n        print(f\"Train Loss: {avg_train_loss:.4f} | P: {train_p:.4f} | R: {train_r:.4f} | F1: {train_f1:.4f} | Acc: {train_a:.4f}\")\n        print(f\"Valid Loss: {val_loss:.4f} | P: {val_p:.4f} | R: {val_r:.4f} | F1: {val_f1:.4f} | Acc: {val_a:.4f}\")\n\n        # Early Stopping + Full Model Save\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n            torch.save(model, \"charbert_full_model.pt\")  # ✅ Save full model\n            print(\"✅ Full model saved!\")\n        else:\n            patience_counter += 1\n            print(f\"⏳ Patience Counter: {patience_counter}/{patience}\")\n\n        if patience_counter >= patience:\n            print(\"\\n🛑 Early stopping triggered. Restoring best full model...\")\n            model = torch.load(\"charbert_full_model.pt\")\n            model.to(device)\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:55.065900Z","iopub.execute_input":"2025-04-06T13:13:55.066217Z","iopub.status.idle":"2025-04-06T13:13:55.074566Z","shell.execute_reply.started":"2025-04-06T13:13:55.066192Z","shell.execute_reply":"2025-04-06T13:13:55.073633Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"train_model(model, train_loader, valid_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:13:57.721436Z","iopub.execute_input":"2025-04-06T13:13:57.721729Z","iopub.status.idle":"2025-04-06T13:37:47.632433Z","shell.execute_reply.started":"2025-04-06T13:13:57.721704Z","shell.execute_reply":"2025-04-06T13:37:47.631125Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10\nTrain Loss: 0.2206 | P: 0.9117 | R: 0.8656 | F1: 0.8863 | Acc: 0.9270\nValid Loss: 0.0835 | P: 0.9485 | R: 0.9623 | F1: 0.9552 | Acc: 0.9749\n✅ Full model saved!\n\nEpoch 2/10\nTrain Loss: 0.0724 | P: 0.9600 | R: 0.9614 | F1: 0.9606 | Acc: 0.9783\nValid Loss: 0.0706 | P: 0.9609 | R: 0.9636 | F1: 0.9622 | Acc: 0.9792\n✅ Full model saved!\n\nEpoch 3/10\nTrain Loss: 0.0576 | P: 0.9693 | R: 0.9709 | F1: 0.9701 | Acc: 0.9834\nValid Loss: 0.0728 | P: 0.9575 | R: 0.9652 | F1: 0.9612 | Acc: 0.9785\n⏳ Patience Counter: 1/3\n\nEpoch 4/10\nTrain Loss: 0.0516 | P: 0.9738 | R: 0.9756 | F1: 0.9747 | Acc: 0.9858\nValid Loss: 0.0762 | P: 0.9595 | R: 0.9671 | F1: 0.9632 | Acc: 0.9795\n⏳ Patience Counter: 2/3\n\nEpoch 5/10\nTrain Loss: 0.0443 | P: 0.9776 | R: 0.9795 | F1: 0.9786 | Acc: 0.9880\nValid Loss: 0.0747 | P: 0.9582 | R: 0.9673 | F1: 0.9627 | Acc: 0.9790\n⏳ Patience Counter: 3/3\n\n🛑 Early stopping triggered. Restoring best full model...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-36-a91cfad12c6d>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"charbert_full_model.pt\")\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"test_loss, test_precision, test_recall, test_f1, test_accuracy = evaluate_model(model, test_loader)\n\nprint(\"\\nTest Evaluation:\")\nprint(f\"  Loss: {test_loss:.4f}\")       # ✅ Now it won't crash                                                                                                                    \nprint(f\"  Precision: {test_precision:.4f}\")\nprint(f\"  Recall: {test_recall:.4f}\")\nprint(f\"  F1-Score: {test_f1:.4f}\")\nprint(f\"  Accuracy: {test_accuracy:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:54:41.185785Z","iopub.execute_input":"2025-04-06T13:54:41.186104Z","iopub.status.idle":"2025-04-06T13:54:49.519746Z","shell.execute_reply.started":"2025-04-06T13:54:41.186078Z","shell.execute_reply":"2025-04-06T13:54:49.518913Z"}},"outputs":[{"name":"stdout","text":"\nTest Evaluation:\n  Loss: 0.0425\n  Precision: 0.9721\n  Recall: 0.9755\n  F1-Score: 0.9738\n  Accuracy: 0.9893\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"import torch\n\n# Load the full model\nmodel = torch.load(\"charbert_full_model.pt\", map_location=device)\n\n# Move to GPU if available\nmodel.to(device)\n\n# Set to evaluation mode\nmodel.eval()\n\nprint(\"✅ Full model reloaded and ready for use!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:57:38.232353Z","iopub.execute_input":"2025-04-06T13:57:38.232716Z","iopub.status.idle":"2025-04-06T13:57:40.332577Z","shell.execute_reply.started":"2025-04-06T13:57:38.232676Z","shell.execute_reply":"2025-04-06T13:57:40.331745Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-40-5f986dd3ca57>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"charbert_full_model.pt\", map_location=device)\n","output_type":"stream"},{"name":"stdout","text":"✅ Full model reloaded and ready for use!\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"label_id_to_str = {\n    0: \"O\",\n    1: \"X\",\n    2: \"Y\",\n    # If you had more, add them here\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:57:51.143547Z","iopub.execute_input":"2025-04-06T13:57:51.143842Z","iopub.status.idle":"2025-04-06T13:57:51.147899Z","shell.execute_reply.started":"2025-04-06T13:57:51.143815Z","shell.execute_reply":"2025-04-06T13:57:51.146883Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"def predict_on_text_charbert(model, text, tokenizer, char_tokenizer, max_seq_len=128):\n    \"\"\"\n    Predict the NER tags for a single input text using CharBERT-style approach \n    (BERT subwords + grapheme-based character embeddings).\n    \n    Args:\n        model: Trained CharBERT_NER model (with .bert + .char_encoder + .lstm).\n        text: Raw Bengali text string.\n        tokenizer: HuggingFace tokenizer (AutoTokenizer) for BERT subwords.\n        char_tokenizer: Keras tokenizer trained on grapheme sequences.\n        max_seq_len: Max sequence length used in training/fine-tuning.\n    \n    Returns:\n        List of (word, predicted_label) tuples.\n    \"\"\"\n    model.eval()\n\n    # 1️⃣ Custom word-level splitting (punct, special chars)\n    original_tokens = custom_tokenize(text)\n\n    # 2️⃣ BERT subword tokenization + mapping\n    bert_tokens = []\n    token_mapping = []  # Maps subword indices back to original_tokens indices\n    for idx, tok in enumerate(original_tokens):\n        subwords = tokenizer.tokenize(tok)\n        bert_tokens.extend(subwords)\n        token_mapping.extend([idx] * len(subwords))\n\n    # 3️⃣ Convert to BERT token IDs (CLS + subwords + SEP)\n    input_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(bert_tokens) + [tokenizer.sep_token_id]\n    token_mapping = [-1] + token_mapping + [-1]  # -1 for CLS and SEP\n\n    # 4️⃣ Build Char/Grapheme input\n    #    We only do one grapheme sequence per *original token*, then replicate for subwords.\n    char_seq = []\n    for word in original_tokens:\n        # Split into graphemes\n        g_list = grapheme_split(word)\n        # Join with space so Keras char_tokenizer can parse them as separate tokens\n        grapheme_text = \" \".join(g_list)\n        grapheme_ids = char_tokenizer.texts_to_sequences([grapheme_text])[0] if grapheme_text else []\n        # Pad/truncate\n        grapheme_ids = grapheme_ids[:MAX_CHAR_LEN]\n        grapheme_ids += [0] * (MAX_CHAR_LEN - len(grapheme_ids))\n        # We'll store, replicate later\n        char_seq.append(grapheme_ids)\n\n    #    Replicate char_seq for subwords + add CLS/SEP placeholders\n    replicated_char_seq = [[0]*MAX_CHAR_LEN]  # CLS\n    idx_char = 0\n    for idx, tok in enumerate(original_tokens):\n        subword_count = sum([1 for m in token_mapping if m == idx]) \n        # replicate the same grapheme vector subword_count times\n        for _ in range(subword_count):\n            replicated_char_seq.append(char_seq[idx_char])\n        idx_char += 1\n    replicated_char_seq.append([0]*MAX_CHAR_LEN)  # SEP\n\n    # 5️⃣ Now we do sequence-level padding/truncation\n    attention_mask = [1] * len(input_ids)\n\n    if len(input_ids) > max_seq_len:\n        input_ids = input_ids[:max_seq_len]\n        attention_mask = attention_mask[:max_seq_len]\n        token_mapping = token_mapping[:max_seq_len]\n        replicated_char_seq = replicated_char_seq[:max_seq_len]\n    else:\n        pad_len = max_seq_len - len(input_ids)\n        input_ids += [0]*pad_len\n        attention_mask += [0]*pad_len\n        token_mapping += [-1]*pad_len\n        # Pad char_seq\n        for _ in range(pad_len):\n            replicated_char_seq.append([0]*MAX_CHAR_LEN)\n\n    # 6️⃣ Convert to tensors\n    input_ids      = torch.tensor([input_ids], dtype=torch.long).to(device)\n    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n    token_type_ids = torch.zeros_like(input_ids).to(device)  # typical for single-sentence inputs\n    char_input     = torch.tensor([replicated_char_seq], dtype=torch.long).to(device)\n\n    # 7️⃣ Forward pass with no grad\n    with torch.no_grad():\n        logits = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            char_input=char_input\n        )\n        preds = torch.argmax(logits, dim=-1).squeeze(0)  # shape: (seq_len,)\n\n    # 8️⃣ Map subword predictions back to original tokens\n    token_level_preds = {}\n    for i, label_id in enumerate(preds.cpu().numpy()):\n        orig_idx = token_mapping[i]\n        if orig_idx == -1:\n            continue  # skip CLS, SEP, padding\n        # first subword of a token can represent the entire token\n        # store only the first subword's predicted label\n        if orig_idx not in token_level_preds:\n            token_level_preds[orig_idx] = label_id\n\n    # 9️⃣ Generate final output\n    results = []\n    for i, word in enumerate(original_tokens):\n        label_id = token_level_preds.get(i, 0)  # default label if missing\n        label_str = label_id_to_str[label_id]   # map ID → \"O\", \"X\", \"Y\", etc.\n        results.append((word, label_str))\n\n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:57:55.772297Z","iopub.execute_input":"2025-04-06T13:57:55.772617Z","iopub.status.idle":"2025-04-06T13:57:55.785366Z","shell.execute_reply.started":"2025-04-06T13:57:55.772586Z","shell.execute_reply":"2025-04-06T13:57:55.784475Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = \"রিনা ও মিঞা যথাক্রমে গণিতে ৫০ ও ৬৫ নম্বর পেয়েছে. কিন্তু শিমা পেয়েছে ৪০ নম্বর\"\nnew_text=\"২০২০ সালে জিডিপি ছিল ৫.২%, কিন্তু মুদ্রাস্ফীতি ছিল ৩.১%। অন্যদিকে, ২০২১ সালে আমাদের জিডিপি বেড়ে ৬.৭% হলো, আর মুদ্রাস্ফীতি কমে দাঁড়ালো ২.১%।\"\n# Call predict\npredictions = predict_on_text_charbert(model, new_text, tokenizer, char_tokenizer, max_seq_len=128)\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:58:05.459221Z","iopub.execute_input":"2025-04-06T13:58:05.459561Z","iopub.status.idle":"2025-04-06T13:58:05.519070Z","shell.execute_reply.started":"2025-04-06T13:58:05.459532Z","shell.execute_reply":"2025-04-06T13:58:05.518153Z"}},"outputs":[{"name":"stdout","text":"২০২০\tX\nসালে\tO\nজিডিপি\tX\nছিল\tO\n৫.২\tY\n%\tY\n,\tO\nকিন্তু\tO\nমুদ্রাস্ফীতি\tX\nছিল\tO\n৩.১\tY\n%\tY\n।\tO\nঅন্যদিকে\tO\n,\tO\n২০২১\tX\nসালে\tO\nআমাদের\tO\nজিডিপি\tX\nবেড়ে\tO\n৬.৭\tY\n%\tY\nহলো\tO\n,\tO\nআর\tO\nমুদ্রাস্ফীতি\tX\nকমে\tO\nদাঁড়ালো\tO\n২.১\tY\n%\tY\n।\tO\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"pip install llama-cpp-python","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport subprocess\nimport os\nfrom llama_cpp import Llama\nfrom sentence_transformers import SentenceTransformer, util, CrossEncoder\nimport torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# ✅ Load the LLaMA 3 model from GGUF format\nMODEL_PATH = \"/kaggle/input/llma3_8b/pytorch/llama3_8b/1/Meta-Llama-3-8B-Instruct-Q6_K.gguf\"\nllm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_gpu_layers=-1)  # Use GPU acceleration\n\ndef simplify_text(text):\n    \"\"\"Uses LLaMA-3 GGUF model to simplify numerical text intelligently.\"\"\"\n    \n    prompt = f\"\"\"\n    তুমি একটি কৃত্রিম বুদ্ধিমত্তা মডেল, যাকে বাংলা ভাষায় সংখ্যা সমৃদ্ধ একটি পাঠ্য দেওয়া হবে।\n    তোমাকে ওই পাঠ্যের প্রাসঙ্গিক অর্থ বুঝতে হবে এবং তুলনামূলক বিবৃতিগুলোকে সুনির্দিষ্ট সংখ্যায় রূপান্তর করে পাঠ্যটি সরলীকরণ করতে হবে।\n    **নিয়মাবলী:**\n    ১. সংখ্যা পরিবর্তন কোরো না, শুধুমাত্র ব্যাখ্যা সহজ করো।\n    ২. কোনো অতিরিক্ত তথ্য যোগ কোরো না।\n    ৩. গাণিতিক হিসাব সঠিকভাবে সম্পন্ন করো।\n    Input: রহিমের আছে ৪০টাকা, শফিকের তার দ্বিগুন এবং করিমের আছে রহিমের অর্ধেক। \n    Output: রহিমের কাছে আছে ৪০ টাকা, সফিকের কাছে আছে ৮০ টাকা, করিমের কাছে আছে ২০ টাকা।\n    \n    Input: প্রাণ বেভারেজ এই মাসে পূর্ববর্তী মাসের চেয়ে ২.৫% বেশি আয় করেছে, পূর্ববর্তী মাসে তাদের আয় ছিল ২,৫০,০০০ টাকা। পরবর্তী মাসে অনুমান করা হচ্ছে যে বর্তমান মাসের চেয়েও দ্বিগুণ বেশি আয় হবে।\n    Output: প্রাণ বেভারেজ পূর্ববর্তী মাসে আয় করেছে ২,৫০,০০০ টাকা। এই মাসে আয় করেছে ২,৫৬,২৫০ টাকা। পরবর্তী মাসে অনুমান করা হচ্ছে যে আয় হবে ৫,০০,০০০ টাকা।\n    \n    Input: উত্তর আমেরিকায় এই বছর প্রচণ্ড গরম পড়েছে, যা গত দশকের তুলনায় ২.৫ ডিগ্রি সেলসিয়াস বেশি। গত দশকে তাপমাত্রা ছিল ৩৫ ডিগ্রি সেলসিয়াস।\n    Output: উত্তর আমেরিকায় এই বছর প্রচণ্ড গরম পড়েছে। গত দশকে তাপমাত্রা ছিল ৩৫ ডিগ্রি সেলসিয়াস। এই বছর তাপমাত্রা হয়েছে ৩৭.৫ ডিগ্রি সেলসিয়াস।\n\n    ---\n    **ইনপুট:** \"{text}\"  \n    **আউটপুট:**  \n    \"\"\"\n\n    # ✅ Generate response using LLaMA-3 GGUF\n    output = llm(\n        prompt,\n        max_tokens=512,  # More room for generated text\n        temperature=0.1,  # Less randomness for accuracy\n        top_p=0.9,  # Balanced output\n        stop=['\\n']  # Don't cut off early\n    )\n\n    # ✅ Extract and clean output\n    simplified_text = output[\"choices\"][0][\"text\"].strip().replace(\"\\n\", \" \")\n\n    # ✅ Ensure output is meaningful\n    if not simplified_text or len(simplified_text.split()) < 3:\n        return \"Error: Model did not generate a valid response.\"\n\n    return simplified_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_test_sentences(text, max_words=150):\n    \"\"\"\n    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n    Ensures words are not split in the middle.\n    \"\"\"\n    words = text.split()  # Split the text into words\n    chunks = []\n    \n    # ✅ Process in chunks of `max_words`\n    for i in range(0, len(words), max_words):\n        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n        chunk_text = \" \".join(chunk_words)  # Convert back to text\n        chunks.append(chunk_text.strip())\n    print([chunk for chunk in chunks])\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_test_sentences(text, max_words=150):\n    \"\"\"\n    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n    Ensures words are not split in the middle.\n    \"\"\"\n    words = text.split()  # Split the text into words\n    chunks = []\n    \n    # ✅ Process in chunks of `max_words`\n    for i in range(0, len(words), max_words):\n        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n        chunk_text = \" \".join(chunk_words)  # Convert back to text\n        chunks.append(chunk_text.strip())\n    print([chunk for chunk in chunks])\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def simplify_large_text(text):\n    \"\"\"\n    Breaks large text into token-based chunks, processes each with LLaMA, and merges results.\n    \"\"\"\n    chunks = convert_to_test_sentences(text)  # Token-based chunking\n    simplified_chunks = [simplify_text(chunk) for chunk in chunks]  # Process each chunk\n    \n    return \" \".join(simplified_chunks)  # Merge all processed chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text=\"\"\"সায়রা জুলাই মাসে ১০০০০ টাকা পায় তার ব্যবসা থেকে। আগস্ট মাসে তার লাভ হয় জুলাই মাস থেকে ৫০০০ টাকা বেশি। কিন্তু সেপ্টেম্বরে আবার জুলাই এর সমান পরিমাণ লাভ হয়। অক্টোবরে লাভের পরিমাণ সেপ্টেম্বর থেকে আরো ৩০০০ কমে ৭০০০ টাকা হয়ে যায়। নভেম্বরে আবার লাভের মুখ দেখে সায়রা। তার ব্যবসা থেকে বছরের শেষদিকে নভেম্বর মাসে লাভ হয় ২০০০০ টাকা।\"\"\"\n# ✅ Process and get final simplified output\nfinal_simplified_output = simplify_large_text(text)\n\n# ✅ Print the final merged output\nprint(\"Final Simplified Output:\", final_simplified_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = final_simplified_output\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = \"২০২০ সালে জিডিপি ছিল ৫.২%, কিন্তু মুদ্রাস্ফীতি ছিল ৩.১%। অন্যদিকে, ২০২১ সালে আমাদের জিডিপি বেড়ে ৬.৭% হলো, আর মুদ্রাস্ফীতি কমে দাঁড়ালো ২.১%।\"\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n# Run validation\nvalid_pairs = validate_xy_pairs(tokens, labels)\n\n# Output valid pairs\nprint(\"✅ Valid X-Y Pairs:\", valid_pairs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = \"২০২০ সালে জিডিপি ছিল ৫.২%, কিন্তু মুদ্রাস্ফীতি ছিল ৩.১%। অন্যদিকে, ২০২১ সালে আমাদের জিডিপি বেড়ে ৬.৭% হলো, আর মুদ্রাস্ফীতি কমে গেলো।\"\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = \"কোভিড মহামারির বছর ২০২০ সালে ছয় হাজার ১৩০ জন আক্রান্ত হয় ম্যালেরিয়ায়। পরের বছর ২০২১ সালে এ সংখ্যা ছিল সাত হাজার ২৯৪।\"\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Suppose you have a new text snippet\nnew_text = \"রিমা বার্ষিক পরীক্ষায় দ্বিতীয় হয়েছে কিন্তু সীমা হয়েছে তৃতীয়, ফজলি হয়েছে পঞ্চম\"\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom typing import List\nimport re\ntxt=\"আমার কাছে তিন কোটি দুই লক্ষ পঞ্চাশ হাজার চারশো ত্রিশ টাকা আছে, তোমার কাছে আছে ৫০০০ টাকা, কিন্তু মিনার কাছে টাকা আছে ৫৪৯০৮\"# Suppose you have a new text snippet\nnew_text = txt\n\n# Call predict\npredictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\n\n# Print token-label pairs\nfor token, label in predictions:\n    print(f\"{token}\\t{label}\")\n\ndef convert_bengali_text_to_number(tokens, labels):\n    \"\"\"\n    Converts Bengali number words to their numerical values based on the 'Y' labels.\n    \n    Args:\n    - tokens (list): List of words (tokens) from the sentence.\n    - labels (list): Corresponding NER labels (X, Y, O).\n\n    Returns:\n    - converted_tokens (list): Tokens with 'Y' values converted to numbers.\n    \"\"\"\n    # Bengali number words mapping\n    bengali_number_map = {\n        \"শূন্য\": 0, \"এক\": 1, \"দুই\": 2, \"তিন\": 3, \"চার\": 4,\n        \"পাঁচ\": 5, \"ছয়\": 6, \"সাত\": 7, \"আট\": 8, \"নয়\": 9,\n        \"দশ\": 10, \"এগারো\": 11, \"বারো\": 12, \"তেরো\": 13, \"চৌদ্দ\": 14,\n        \"পনেরো\": 15, \"ষোল\": 16, \"সতেরো\": 17, \"আঠারো\": 18, \"ঊনিশ\": 19,\n        \"বিশ\": 20, \"একুশ\": 21, \"বাইশ\": 22, \"তেইশ\": 23, \"চব্বিশ\": 24,\n        \"পঁচিশ\": 25, \"ছাব্বিশ\": 26, \"সাতাশ\": 27, \"আঠাশ\": 28, \"ঊনত্রিশ\": 29,\n        \"ত্রিশ\": 30, \"একত্রিশ\": 31, \"বত্রিশ\": 32, \"তেত্রিশ\": 33, \"চৌত্রিশ\": 34,\n        \"পঁইত্রিশ\": 35, \"ছত্রিশ\": 36, \"সাইত্রিশ\": 37, \"আটত্রিশ\": 38, \"ঊনচল্লিশ\": 39,\n        \"চল্লিশ\": 40, \"একচল্লিশ\": 41, \"বিয়াল্লিশ\": 42, \"তেতাল্লিশ\": 43, \"চুয়াল্লিশ\": 44,\n        \"পঁইচল্লিশ\": 45, \"ছিচল্লিশ\": 46, \"সাতচল্লিশ\": 47, \"আটচল্লিশ\": 48, \"ঊনপঞ্চাশ\": 49,\n        \"পঞ্চাশ\": 50, \"একান্ন\": 51, \"বায়ান্ন\": 52, \"তিপ্পান্ন\": 53, \"চুয়ান্ন\": 54,\n        \"পঞ্চান্ন\": 55, \"ছাপ্পান্ন\": 56, \"সাতান্ন\": 57, \"আটান্ন\": 58, \"ঊনষাট\": 59,\n        \"ষাট\": 60, \"একষট্টি\": 61, \"বাষট্টি\": 62, \"তেষট্টি\": 63, \"চৌষট্টি\": 64,\n        \"পঁইষট্টি\": 65, \"ছেষট্টি\": 66, \"সাতষট্টি\": 67, \"আটষট্টি\": 68, \"ঊনসত্তর\": 69,\n        \"সত্তর\": 70, \"একাত্তর\": 71, \"বাহাত্তর\": 72, \"তিয়াত্তর\": 73, \"চুয়াত্তর\": 74,\n        \"পঁচাত্তর\": 75, \"ছিয়াত্তর\": 76, \"সাতাত্তর\": 77, \"আটাত্তর\": 78, \"ঊনআশি\": 79,\n        \"আশি\": 80, \"একাশি\": 81, \"বিরাশি\": 82, \"তিরাশি\": 83, \"চুরাশি\": 84,\n        \"পঁচাশি\": 85, \"ছিয়াশি\": 86, \"সাতাশি\": 87, \"আটাশি\": 88, \"ঊননব্বই\": 89,\n        \"নব্বই\": 90, \"একানব্বই\": 91, \"বিরানব্বই\": 92, \"তিরানব্বই\": 93, \"চুরানব্বই\": 94,\n        \"পঁচানব্বই\": 95, \"ছিয়ানব্বই\": 96, \"সাতানব্বই\": 97, \"আটানব্বই\": 98, \"নিরানব্বই\": 99,\n        \"একশো\": 100, \"দুইশো\": 200, \"তিনশো\": 300, \"চারশো\": 400, \"পাঁচশো\": 500,\n        \"ছয়শো\": 600, \"সাতশো\": 700, \"আটশো\": 800, \"নয়শো\": 900\n    }\n\n    multipliers = {\n        \"হাজার\": 1000, \"লক্ষ\": 100000, \"লাখ\": 100000, \"কোটি\": 10000000,\n        \"মিলিয়ন\": 1000000, \"বিলিয়ন\": 1000000000\n    }\n\n    # Convert words to numbers\n    converted_tokens = []\n    current_number = 0\n    i = 0\n\n    while i < len(tokens):\n        word = tokens[i]\n        label = labels[i]\n\n        if label == \"Y\":\n            if word.isdigit():  # If it's already a number, store as integer\n                current_number += int(word)\n            elif word in bengali_number_map:  # Convert Bengali number words\n                temp_number = bengali_number_map[word]\n\n                # Look ahead for a multiplier\n                if (i + 1) < len(tokens) and tokens[i + 1] in multipliers:\n                    temp_number *= multipliers[tokens[i + 1]]\n                    i += 1  # Skip the multiplier\n\n                current_number += temp_number\n            else:\n                converted_tokens.append(word)  # Keep other words unchanged\n        else:\n            if current_number > 0:\n                converted_tokens.append(current_number)\n                current_number = 0  # Reset for next number\n            converted_tokens.append(word)\n\n        i += 1\n\n    # If there's an unprocessed number at the end\n    if current_number > 0:\n        converted_tokens.append(current_number)\n\n    return converted_tokens\n\n\n# Run the function\ntoken,labels=valid_pairs = validate_xy_pairs(tokens, labels)\n\n# Output valid pairs\nprint(\"✅ Valid X-Y Pairs:\", valid_pairs)\nconverted_tokens = convert_bengali_text_to_number(tokens, labels)\n\n\nprint(\"✅ Converted Tokens:\", converted_tokens)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom typing import List\n\ndef convert_bengali_text_to_number(tokens, labels):\n    \"\"\"Pairs `X` labels with `Y` words and prepares them for conversion.\"\"\"\n    \n    # Define Bengali Number Words and Multipliers\n    bengali_number_map = {\n        \"শূন্য\": 0, \"এক\": 1, \"দুই\": 2, \"তিন\": 3, \"চার\": 4, \"পাঁচ\": 5, \"ছয়\": 6, \"সাত\": 7, \"আট\": 8, \"নয়\": 9,\n        \"দশ\": 10, \"বিশ\": 20, \"ত্রিশ\": 30, \"চল্লিশ\": 40, \"পঞ্চাশ\": 50, \"ষাট\": 60, \"সত্তর\": 70, \"আশি\": 80, \"নব্বই\": 90,\n        \"একশো\": 100, \"দুইশো\": 200, \"তিনশো\": 300, \"চারশো\": 400, \"পাঁচশো\": 500, \"ছয়শো\": 600, \"সাতশো\": 700, \"আটশো\": 800, \"নয়শো\": 900\n    }\n\n    multipliers = {\n        \"হাজার\": 1000, \"লক্ষ\": 100000, \"লাখ\": 100000, \"কোটি\": 10000000, \"মিলিয়ন\": 1000000, \"বিলিয়ন\": 1000000000\n    }\n\n    paired_results = {}  # Store results\n    current_x = None\n    current_y_words = []  # Store all Y words\n    unit = None  # Store unit if found\n    last_multiplier = None  # Track last multiplier to prevent duplication\n    i = 0\n\n    while i < len(tokens):\n        word, label = tokens[i], labels[i]\n\n        if label == \"X\":\n            if current_x and current_y_words:  \n                # Store previous X-Y pair with its unit (before conversion)\n                paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n                current_y_words = []  \n                unit = None  \n                last_multiplier = None  # Reset last multiplier\n\n            current_x = word  # Assign new X entity\n\n        elif label == \"Y\":\n            current_y_words.append(word)  # Store Y word\n\n            # Check if it's a multiplier and store only once\n            if word in multipliers and last_multiplier != word:\n                unit = word  # Store the first occurrence as the unit\n                last_multiplier = word  # Prevent duplicate multipliers\n\n        i += 1  \n\n    # Store the last X-Y pair\n    if current_x and current_y_words:\n        paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n\n    return paired_results, bengali_number_map, multipliers\n\n\ndef convert_text_to_number(paired_results, bengali_number_map, multipliers):\n    \"\"\"\n    Converts Bengali number words into numeric values while keeping the unit.\n\n    Args:\n    - paired_results (dict): Mapping of `X` to `Y` words before conversion.\n    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n    - multipliers (dict): Mapping of Bengali multipliers (হাজার, কোটি, etc.).\n\n    Returns:\n    - converted_results (dict): Mapping of `X` to final numeric values with units.\n    \"\"\"\n    converted_results = {}\n\n    for x_entity, data in paired_results.items():\n        y_words = data[\"words\"]\n        unit = data[\"unit\"] if data[\"unit\"] else \"\"\n\n        # Convert Bengali number words to a single numerical value\n        number_text = \" \".join(y_words)\n        number_value = convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers)\n\n        # Store the final result\n        converted_results[x_entity] = f\"{number_value} {unit}\"\n\n    return converted_results\n\n\ndef convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers):\n    \"\"\"\n    Helper function to convert Bengali text numbers into numeric values.\n\n    Args:\n    - number_text (str): Bengali number words.\n    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n    - multipliers (dict): Mapping of Bengali multipliers (হাজার, কোটি, etc.).\n\n    Returns:\n    - int: Converted numeric value.\n    \"\"\"\n    words = number_text.split()\n    total = 0\n    temp_value = 0\n\n    for i, word in enumerate(words):\n        if word.isdigit():\n            temp_value += int(word)\n        elif word in bengali_number_map:\n            temp_value += bengali_number_map[word]\n        elif word in multipliers:\n            temp_value *= multipliers[word]\n            total += temp_value\n            temp_value = 0\n\n    return total + temp_value\n\n\n# Example Input\ntokens = [\"আমার\", \"কাছে\", \"তিন\", \"কোটি\", \"দুই\", \"লক্ষ\", \"পঞ্চাশ\", \"হাজার\", \"চারশো\", \"ত্রিশ\", \"টাকা\", \"আছে\",\n          \",\", \"তোমার\", \"কাছে\", \"আছে\", \"৫০০০\", \"টাকা\", \",\", \"কিন্তু\", \"মিনার\", \"কাছে\", \"টাকা\", \"আছে\", \"৫৪৯০৮\"]\n\nlabels = [\"X\", \"O\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"O\",\n          \"O\", \"X\", \"O\", \"O\", \"Y\", \"Y\", \"O\", \"O\", \"X\", \"O\", \"O\", \"O\", \"Y\"]\n\n# Step 1: Pair `X` with `Y` words before conversion\npaired_results, bengali_number_map, multipliers = convert_bengali_text_to_number(tokens, labels)\n\n# Step 2: Convert `Y` words into numeric values\nfinal_results = convert_text_to_number(paired_results, bengali_number_map, multipliers)\n\n# Output the result\nprint(\"🔹 **Before Conversion:**\")\nfor key, data in paired_results.items():\n    print(f\"{key} -> {' '.join(data['words'])} {data['unit'] if data['unit'] else ''}\")\n\nprint(\"\\n🔹 **After Conversion:**\")\nfor key, value in final_results.items():\n    print(f\"{key} -> {value}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom typing import List\nimport re\n\n# Example text\ntxt = \"আমার কাছে তিন কোটি দুই লক্ষ পঞ্চাশ হাজার চারশো ত্রিশ টাকা আছে, তোমার কাছে আছে ৫০০০ টাকা, কিন্তু মিনার কাছে টাকা আছে ৫৪৯০৮\"\n\n# Call predict function (ensure predict_on_text returns [(token, label), ...])\npredictions = predict_on_text(model, txt, tokenizer, max_seq_len=100)\n\n# Extract tokens and labels\ntokens = [token for token, label in predictions]\nlabels = [label for token, label in predictions]\n\ndef extract_xy_pairs(data):\n    pairs = {}\n    current_x = None\n    current_y = []\n    \n    for word, label in data:\n        if label == 'X':\n            if current_x and current_y:\n                pairs[current_x] = ' '.join(current_y)\n            current_x = word\n            current_y = []\n        elif label == 'Y':\n            if current_x:\n                current_y.append(word)\n    \n    if current_x and current_y:\n        pairs[current_x] = ' '.join(current_y)\n    \n    return pairs\n\n# Extract pairs\nxy_pairs = extract_xy_pairs(predictions)  # Directly pass predictions\n\n# Print the result\nfor x, y in xy_pairs.items():\n    print(f\"{x} -> {y}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport pandas as pd\nfrom typing import List\nimport plotly.graph_objects as go\n\n\ndef extract_xy_pairs(data):\n    pairs = {}\n    units = {}\n    current_x = None\n    current_y = []\n    current_unit = None\n    unit_list = []\n    \n    # Define Bengali Number Words and Multipliers\n    bengali_number_map = {\n        \"শূন্য\": 0, \"এক\": 1, \"দুই\": 2, \"তিন\": 3, \"চার\": 4, \"পাঁচ\": 5, \"ছয়\": 6, \"সাত\": 7, \"আট\": 8, \"নয়\": 9,\n        \"দশ\": 10, \"বিশ\": 20, \"ত্রিশ\": 30, \"চল্লিশ\": 40, \"পঞ্চাশ\": 50, \"ষাট\": 60, \"সত্তর\": 70, \"আশি\": 80, \"নব্বই\": 90,\n        \"একশো\": 100, \"দুইশো\": 200, \"তিনশো\": 300, \"চারশো\": 400, \"পাঁচশো\": 500, \"ছয়শো\": 600, \"সাতশো\": 700, \"আটশো\": 800, \"নয়শো\": 900\n    }\n\n    multipliers = {\n        \"হাজার\": 1000, \"লক্ষ\": 100000, \"লাখ\": 100000, \"কোটি\": 10000000, \"মিলিয়ন\": 1000000, \"বিলিয়ন\": 1000000000\n    }\n    \n    for i, (word, label) in enumerate(data):\n        if label == 'X':\n            if current_x and current_y:\n                pairs[current_x] = ' '.join(current_y)\n                units[current_x] = current_unit\n                unit_list.append(current_unit if current_unit else \"null\")\n            current_x = word\n            current_y = []\n            current_unit = None\n        elif label == 'Y':\n            if current_x:\n                current_y.append(word)\n            if word not in bengali_number_map and word not in multipliers:\n                if not word.isdigit():  # Ensure it's not a number before marking it as a unit\n                    current_unit = word\n    \n    if current_x and current_y:\n        pairs[current_x] = ' '.join(current_y)\n        units[current_x] = current_unit\n        unit_list.append(current_unit if current_unit else \"null\")\n    \n    return pairs, units, unit_list\n\ndef normalize_units(numeric_results, unit_list):\n    \"\"\"Converts all values to the smallest detected unit if necessary.\"\"\"\n    conversion_map = {\"বিলিয়ন\": 1000, \"মিলিয়ন\": 1, \"লক্ষ\": 0.1}  # Convert billions to millions\n    \n    # Detect the most frequently occurring unit (smallest denominator)\n    valid_units = [unit for unit in unit_list if unit in conversion_map]\n    if not valid_units:\n        return numeric_results, unit_list  # No unit conversions needed\n    \n    target_unit = min(valid_units, key=lambda x: conversion_map[x])  # Find lowest unit\n    normalized_results = {}\n    new_unit_list = []\n    \n    for (key, value), unit in zip(numeric_results.items(), unit_list):\n        if unit in conversion_map:\n            factor = conversion_map[unit] / conversion_map[target_unit]\n            normalized_results[key] = value * factor\n            new_unit_list.append(target_unit)\n        else:\n            normalized_results[key] = value\n            new_unit_list.append(unit)\n    \n    return normalized_results, new_unit_list\ndef parse_bengali_float(token: str):\n    \"\"\"\n    Translates Bengali digits in the token to ASCII digits,\n    then attempts to convert it to a float.\n    Returns None if unsuccessful.\n    \"\"\"\n    # Make a translation map for Bengali digits -> ASCII digits\n    translation_map = str.maketrans(\"০১২৩৪৫৬৭৮৯\", \"0123456789\")\n    \n    # Translate the token to ASCII form\n    ascii_token = token.translate(translation_map)\n    \n    try:\n        return float(ascii_token)  # If \"৫.২\" => \"5.2\" => 5.2 (float)\n    except ValueError:\n        return None\n\n\ndef convert_bengali_text_to_number(xy_pairs_dict, units):\n    \"\"\"Converts Bengali number words into numerical values and retains units.\"\"\"\n    bengali_number_map = {\n        'এক': 1, 'প্রথম': 1, 'পহেলা': 1,\n        'দুই': 2, 'দ্বিতীয়': 2, 'দোসরা': 2,\n        'তিন': 3, 'তৃতীয়': 3,\n        'চার': 4, 'চতুর্থ': 4,\n        'পাঁচ': 5, 'পঞ্চম': 5,\n        'ছয়': 6, 'ষষ্ঠ': 6,\n        'সাত': 7, 'সপ্তম': 7,\n        'আট': 8, 'অষ্টম': 8,\n        'নয়': 9, 'নবম': 9,\n        'দশ': 10, 'দশম': 10, \"এগারো\": 11, \"বারো\": 12, \"তেরো\": 13, \"চৌদ্দ\": 14,\n        \"পনেরো\": 15, \"ষোল\": 16, \"সতেরো\": 17, \"আঠারো\": 18, \"ঊনিশ\": 19,\n        \"বিশ\": 20, \"একুশ\": 21, \"বাইশ\": 22, \"তেইশ\": 23, \"চব্বিশ\": 24,\n        \"পঁচিশ\": 25, \"ছাব্বিশ\": 26, \"সাতাশ\": 27, \"আঠাশ\": 28, \"ঊনত্রিশ\": 29,\n        \"ত্রিশ\": 30, \"একত্রিশ\": 31, \"বত্রিশ\": 32, \"তেত্রিশ\": 33, \"চৌত্রিশ\": 34,\n        \"পঁইত্রিশ\": 35, \"ছত্রিশ\": 36, \"সাইত্রিশ\": 37, \"আটত্রিশ\": 38, \"ঊনচল্লিশ\": 39,\n        \"চল্লিশ\": 40, \"একচল্লিশ\": 41, \"বিয়াল্লিশ\": 42, \"তেতাল্লিশ\": 43, \"চুয়াল্লিশ\": 44,\n        \"পঁইচল্লিশ\": 45, \"ছিচল্লিশ\": 46, \"সাতচল্লিশ\": 47, \"আটচল্লিশ\": 48, \"ঊনপঞ্চাশ\": 49,\n        \"পঞ্চাশ\": 50, \"একান্ন\": 51, \"বায়ান্ন\": 52, \"তিপ্পান্ন\": 53, \"চুয়ান্ন\": 54,\n        \"পঞ্চান্ন\": 55, \"ছাপ্পান্ন\": 56, \"সাতান্ন\": 57, \"আটান্ন\": 58, \"ঊনষাট\": 59,\n        \"ষাট\": 60, \"একষট্টি\": 61, \"বাষট্টি\": 62, \"তেষট্টি\": 63, \"চৌষট্টি\": 64,\n        \"পঁইষট্টি\": 65, \"ছেষট্টি\": 66, \"সাতষট্টি\": 67, \"আটষট্টি\": 68, \"ঊনসত্তর\": 69,\n        \"সত্তর\": 70, \"একাত্তর\": 71, \"বাহাত্তর\": 72, \"তিয়াত্তর\": 73, \"চুয়াত্তর\": 74,\n        \"পঁচাত্তর\": 75, \"ছিয়াত্তর\": 76, \"সাতাত্তর\": 77, \"আটাত্তর\": 78, \"ঊনআশি\": 79,\n        \"আশি\": 80, \"একাশি\": 81, \"বিরাশি\": 82, \"তিরাশি\": 83, \"চুরাশি\": 84,\n        \"পঁচাশি\": 85, \"ছিয়াশি\": 86, \"সাতাশি\": 87, \"আটাশি\": 88, \"ঊননব্বই\": 89,\n        \"নব্বই\": 90, \"একানব্বই\": 91, \"বিরানব্বই\": 92, \"তিরানব্বই\": 93, \"চুরানব্বই\": 94,\n        \"পঁচানব্বই\": 95, \"ছিয়ানব্বই\": 96, \"সাতানব্বই\": 97, \"আটানব্বই\": 98, \"নিরানব্বই\": 99,\n        \"একশো\": 100, \"দুইশো\": 200, \"তিনশো\": 300, \"চারশো\": 400, \"পাঁচশো\": 500,\n        \"ছয়শো\": 600, \"সাতশো\": 700, \"আটশো\": 800, \"নয়শো\": 900\n    }\n\n    multipliers = {\n        \"হাজার\": 1000, \"লক্ষ\": 100000, \"লাখ\": 100000, \"কোটি\": 10000000,\n        \"মিলিয়ন\": 1000000, \"বিলিয়ন\": 1000000000\n    }\n    \n    numeric_dict = {}\n    for x_label, y_text in xy_pairs_dict.items():\n        tokens = y_text.split()\n        total_value = 0\n        part_sum = 0\n\n        for w in tokens:\n            # 1) Try integer\n            if w.isdigit():\n                part_sum += int(w)\n\n            # 2) Try dictionary\n            elif w in bengali_number_map:\n                part_sum += bengali_number_map[w]\n\n            # 3) Try multiplier\n            elif w in multipliers:\n                part_sum *= multipliers[w]\n                total_value += part_sum\n                part_sum = 0\n\n            # 4) Attempt to parse decimal (float)\n            else:\n                val = parse_bengali_float(w)\n                if val is not None:\n                    part_sum += val\n                else:\n                    # Possibly a unit or unknown word\n                    pass\n        \n        # Once we finish all tokens for this X, update total_value and the dictionary\n        total_value += part_sum\n        numeric_dict[x_label] = total_value\n\n    # Now we have processed **all** X-Y pairs\n    return numeric_dict\n\n    \ndef get_dynamic_ticks(min_val: float, max_val: float):\n    \"\"\"\n    Generates y-axis tick values divided into 10 equal steps based on min and max values.\n    \"\"\"\n    if max_val <= min_val:\n        return [min_val]  # Edge case\n\n    step = (max_val - min_val) // 10\n    ticks = [round(min_val + i * step, 2) for i in range(11)]  # 11 ticks = 10 steps\n    return ticks\n\ndef convert_to_bengali(num):\n    bengali_digits = '০১২৩৪৫৬৭৮৯'\n    return ''.join(bengali_digits[int(digit)] if digit.isdigit() else digit for digit in str(num))\n\n\n# # Extract pairs and units\n# xy_pairs, xy_units, unit_list = extract_xy_pairs(predictions)\n\n# # Convert Y values to numeric format\n# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n\n# # Normalize units\n# normalized_values, normalized_units = normalize_units(numeric_values, unit_list)\n\n# # Print results\n# for idx, (x, y) in enumerate(normalized_values.items(), 1):\n#     print(f\"{x} -> {y}\")\n\n# # Print unit mapping\n# for idx, unit in enumerate(normalized_units, 1):\n#     print(f\"unit{idx} -> {unit}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert Y values to numeric format\n# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n\n# Convert values to Bengali numerals for display\nx_values = ['গত দশকে', 'আগামী পাঁচ বছরে']\ny_values = [75,200]\nunit_list = ['মিলিয়ন', 'মিলিয়ন']\n\ny_values_bengali = [convert_to_bengali(value) + \" \" + unit for value, unit in zip(y_values, unit_list)]\n\nmax_val = max(y_values) if y_values else 0\nmin_val = min(y_values) if y_values else 0\nprint(min_val)\nprint(max_val)\n\ny_tick_vals = get_dynamic_ticks(min_val, max_val)\ny_tick_text_bengali = [convert_to_bengali(val) for val in y_tick_vals]\n\n# Create the Plotly bar chart\nfig = go.Figure([go.Bar(\n    x=x_values,\n    y=y_values,\n    width=0.6,\n    text=y_values_bengali,\n    textfont=dict(size=15),\n    textposition='auto',\n    marker=dict(color='blue')\n)])\n\n# Update layout with Bengali tick labels and fonts\nfig.update_layout(\n    xaxis=dict(\n        tickfont=dict(size=13),\n        title=dict(text=\"নাম\", font=dict(size=13))  # X-axis label in Bengali\n    ),\n    yaxis=dict(\n        tickvals=y_tick_vals,\n        ticktext=y_tick_text_bengali,\n        tickfont=dict(size=15),\n        title=dict(text=\"মান\", font=dict(size=13))  # Y-axis label in Bengali\n    ),\n    width=600,\n    height=400,\n    margin=dict(l=50, r=50, t=50, b=50)\n)\n\n# Show and save the plot\nfig.show()\nfig.write_html('generated_chart.html')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"(Moyna has 53 taka. Rohima has 159 taka. Mohona has 318 taka. Hena has 79.5 taka.)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}