{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-06T16:07:34.050750Z",
     "iopub.status.busy": "2025-04-06T16:07:34.050230Z",
     "iopub.status.idle": "2025-04-06T16:07:34.061263Z",
     "shell.execute_reply": "2025-04-06T16:07:34.060568Z",
     "shell.execute_reply.started": "2025-04-06T16:07:34.050719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llma3_8b/pytorch/llama3_8b/1/Meta-Llama-3-8B-Instruct-Q6_K.gguf\n",
      "/kaggle/input/thesisdata/output1(3000).json\n",
      "/kaggle/input/thesisdata/test2.json\n",
      "/kaggle/input/thesisdata1/final_train1.json\n",
      "/kaggle/input/thesisdata1/final_train.json\n",
      "/kaggle/input/thesisdata1/final_testing1.json\n",
      "/kaggle/input/thesisdata1/final_test.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:07:36.575815Z",
     "iopub.status.busy": "2025-04-06T16:07:36.575495Z",
     "iopub.status.idle": "2025-04-06T16:07:36.579939Z",
     "shell.execute_reply": "2025-04-06T16:07:36.579051Z",
     "shell.execute_reply.started": "2025-04-06T16:07:36.575792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex  # for grapheme cluster splitting\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:03.875097Z",
     "iopub.status.busy": "2025-04-06T16:08:03.874761Z",
     "iopub.status.idle": "2025-04-06T16:08:04.045813Z",
     "shell.execute_reply": "2025-04-06T16:08:04.045038Z",
     "shell.execute_reply.started": "2025-04-06T16:08:03.875072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶Ü‡¶≤‡¶Æ‡ßá‡¶∞ ‡¶ó‡ßç‡¶≤‡ßã‡¶ï‡ßã‡¶Æ‡¶æ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß‡ß® ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá‡¶∞ ‡¶≤‡ßá‡¶ú‡¶æ‡¶∞...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶¨‡¶ø‡¶è‡¶´‡¶è-‡¶è‡¶∞ ‡ß®‡ß©‡¶§‡¶Æ ‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá, ‡ßß,‡ßØ‡ß©‡ß®‡¶ü‡¶ø ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶æ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶ó‡¶§ ‡ß© ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ï‡ßç‡¶§‡¶ö‡¶æ‡¶™ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ ‡¶â‡¶®‡ßç‡¶®‡¶§‡¶ø ‡¶¶‡ßá‡¶ñ‡¶ø...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡ß´‡ß®‡ß¶‡ß¶ ‡¶ú‡¶®‡•§ ‡¶ï‡ßá‡ß® ‡¶∂‡ßÉ‡¶ô‡ßç‡¶ó ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá...</td>\n",
       "      <td>[1, 0, 0, 2, 2, 0, 1, 1, 0, 0, 2, 2, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶≤‡¶æ‡¶ö‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶≤‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡ß©‡ß´‡ß¶‡ß¶ ‡¶ï‡¶ø‡¶≤‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞...</td>\n",
       "      <td>[1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>‡¶¨‡¶æ‡¶®‡ßç‡¶¶‡¶∞‡¶¨‡¶æ‡¶®‡ßá ‡¶¨‡¶®‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡ßß‡ß®‡ß¶‡ß¶‡ß¶ ‡¶π‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶∞‡¶æ‡¶ô‡¶æ‡¶Æ‡¶æ...</td>\n",
       "      <td>[1, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>‡¶™‡¶ø‡¶∞‡¶æ‡¶Æ‡¶ø‡¶° ‡¶®‡¶ø‡¶∞‡ßç‡¶Æ‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡ß™‡ß´‡ß¶‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá‡•§ ‡¶∏‡ßç‡¶ü‡ßã‡¶®...</td>\n",
       "      <td>[1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶∏‡ßç‡¶™‡¶∞‡ßç‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡ßß‡ßØ‡ß´‡ß© ‡¶∏‡¶æ‡¶≤‡ßá‡•§...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ‡¶Ü‡¶≤‡¶Æ‡ßá‡¶∞ ‡¶ó‡ßç‡¶≤‡ßã‡¶ï‡ßã‡¶Æ‡¶æ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß‡ß® ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá‡¶∞ ‡¶≤‡ßá‡¶ú‡¶æ‡¶∞...   \n",
       "1    ‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...   \n",
       "2    ‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...   \n",
       "3    ‡¶¨‡¶ø‡¶è‡¶´‡¶è-‡¶è‡¶∞ ‡ß®‡ß©‡¶§‡¶Æ ‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá, ‡ßß,‡ßØ‡ß©‡ß®‡¶ü‡¶ø ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶æ...   \n",
       "4    ‡¶ó‡¶§ ‡ß© ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ï‡ßç‡¶§‡¶ö‡¶æ‡¶™ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ ‡¶â‡¶®‡ßç‡¶®‡¶§‡¶ø ‡¶¶‡ßá‡¶ñ‡¶ø...   \n",
       "..                                                 ...   \n",
       "295  ‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡ß´‡ß®‡ß¶‡ß¶ ‡¶ú‡¶®‡•§ ‡¶ï‡ßá‡ß® ‡¶∂‡ßÉ‡¶ô‡ßç‡¶ó ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá...   \n",
       "296  ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶≤‡¶æ‡¶ö‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶≤‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡ß©‡ß´‡ß¶‡ß¶ ‡¶ï‡¶ø‡¶≤‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞...   \n",
       "297  ‡¶¨‡¶æ‡¶®‡ßç‡¶¶‡¶∞‡¶¨‡¶æ‡¶®‡ßá ‡¶¨‡¶®‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡ßß‡ß®‡ß¶‡ß¶‡ß¶ ‡¶π‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶∞‡¶æ‡¶ô‡¶æ‡¶Æ‡¶æ...   \n",
       "298  ‡¶™‡¶ø‡¶∞‡¶æ‡¶Æ‡¶ø‡¶° ‡¶®‡¶ø‡¶∞‡ßç‡¶Æ‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡ß™‡ß´‡ß¶‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá‡•§ ‡¶∏‡ßç‡¶ü‡ßã‡¶®...   \n",
       "299  ‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶∏‡ßç‡¶™‡¶∞‡ßç‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡ßß‡ßØ‡ß´‡ß© ‡¶∏‡¶æ‡¶≤‡ßá‡•§...   \n",
       "\n",
       "                                                 label  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, ...  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "295  [1, 0, 0, 2, 2, 0, 1, 1, 0, 0, 2, 2, 0, 1, 2, ...  \n",
       "296  [1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, ...  \n",
       "297  [1, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, ...  \n",
       "298  [1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, ...  \n",
       "299  [1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, ...  \n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"/kaggle/input/thesisdata1/final_train1.json\")\n",
    "td = pd.read_json(\"/kaggle/input/thesisdata1/final_testing1.json\")\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:07.072065Z",
     "iopub.status.busy": "2025-04-06T16:08:07.071741Z",
     "iopub.status.idle": "2025-04-06T16:08:07.076852Z",
     "shell.execute_reply": "2025-04-06T16:08:07.075940Z",
     "shell.execute_reply.started": "2025-04-06T16:08:07.072042Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "raw_text       = df[\"text\"].tolist()\n",
    "raw_label      = df[\"label\"].tolist()\n",
    "raw_test_text  = td[\"text\"].tolist()\n",
    "raw_test_label = td[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:07.277406Z",
     "iopub.status.busy": "2025-04-06T16:08:07.277072Z",
     "iopub.status.idle": "2025-04-06T16:08:34.447082Z",
     "shell.execute_reply": "2025-04-06T16:08:34.445889Z",
     "shell.execute_reply.started": "2025-04-06T16:08:07.277367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4179df272d743b497ce971a19580d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda0efbf9ab041f5a435fa68630e934e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571b60efcf8e42899d58dcd5746f4e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b92a2a884f443729f1ff665b24bb1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e63c0efe0843b0a58ed9ec26778338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "bert_model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:34.449200Z",
     "iopub.status.busy": "2025-04-06T16:08:34.448547Z",
     "iopub.status.idle": "2025-04-06T16:08:34.453287Z",
     "shell.execute_reply": "2025-04-06T16:08:34.452365Z",
     "shell.execute_reply.started": "2025-04-06T16:08:34.449173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 256  # Max subword length\n",
    "num_labels = 4         # e.g. 0,1,2,3\n",
    "etag = 4               # Padding label\n",
    "ftag = 3               # Tag for sub-word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:34.455163Z",
     "iopub.status.busy": "2025-04-06T16:08:34.454859Z",
     "iopub.status.idle": "2025-04-06T16:08:34.478008Z",
     "shell.execute_reply": "2025-04-06T16:08:34.477012Z",
     "shell.execute_reply.started": "2025-04-06T16:08:34.455141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# For CharBERT\n",
    "MAX_CHAR_LEN = 15      # Max graphemes per token\n",
    "CHAR_VOCAB_SIZE = 800 # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:08:34.479599Z",
     "iopub.status.busy": "2025-04-06T16:08:34.479313Z",
     "iopub.status.idle": "2025-04-06T16:08:36.739263Z",
     "shell.execute_reply": "2025-04-06T16:08:36.738537Z",
     "shell.execute_reply.started": "2025-04-06T16:08:34.479576Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2) Grapheme-Level Tokenizer\n",
    "########################################\n",
    "import unicodedata\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def normalize_bengali_text(text):\n",
    "    \"\"\"Normalize text to NFKC for consistent vowel/conjunct representation.\"\"\"\n",
    "    return unicodedata.normalize('NFKC', text)\n",
    "\n",
    "def grapheme_split(word):\n",
    "    \"\"\"\n",
    "    Splits a single word into a list of grapheme clusters.\n",
    "    Using regex \\\\X to match any Unicode extended grapheme cluster.\n",
    "    \"\"\"\n",
    "    word = normalize_bengali_text(word)\n",
    "    return regex.findall(r'\\X', word)  # returns a list of graphemes\n",
    "\n",
    "# Fit a Keras tokenizer on all graphemes found in the dataset\n",
    "char_tokenizer = Tokenizer(num_words=CHAR_VOCAB_SIZE, char_level=False, oov_token=\"<OOV>\")\n",
    "\n",
    "# Collect all graphemes from the entire dataset\n",
    "all_words = []\n",
    "for txt in df[\"text\"].tolist():\n",
    "    # naive split by space\n",
    "    for w in txt.split():\n",
    "        # gather graphemes\n",
    "        g_list = grapheme_split(w)\n",
    "        all_words.append(\" \".join(g_list))  \n",
    "        # \" \".join(...) makes each grapheme appear as a 'token' for the Keras Tokenizer\n",
    "\n",
    "# Fit on this pseudo-labeled text\n",
    "char_tokenizer.fit_on_texts(all_words)\n",
    "\n",
    "########################################\n",
    "# 3) Custom Tokenizer for Words\n",
    "########################################\n",
    "# You mentioned special punctuation logic; let's keep it if it‚Äôs relevant.\n",
    "special_character = ['(', ')','%',':','-','$','‚Äî',\"'\",';','‚Äò','‚Äô']\n",
    "punctuation_marks = {',', '‡•§', '!', '?'}\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    \"\"\"\n",
    "    Splits punctuation from words, etc. \n",
    "    Adjust for your language-specific rules if needed.\n",
    "    \"\"\"\n",
    "    space_split_tokens = text.split()\n",
    "    tokens1 = []\n",
    "    # 1) separate punctuation at end\n",
    "    for token in space_split_tokens:\n",
    "        if token and token[-1] in punctuation_marks:\n",
    "            tokens1.append(token[:-1])  # word w/o punctuation\n",
    "            tokens1.append(token[-1])   # punctuation as separate token\n",
    "        else:\n",
    "            tokens1.append(token)\n",
    "\n",
    "    # 2) separate special chars\n",
    "    tokens = []\n",
    "    for token in tokens1:\n",
    "        tmp = \"\"\n",
    "        for ch in token:\n",
    "            if ch not in special_character:\n",
    "                tmp += ch\n",
    "            else:\n",
    "                if tmp:\n",
    "                    tokens.append(tmp)\n",
    "                    tmp = \"\"\n",
    "                tokens.append(ch)\n",
    "        if tmp:\n",
    "            tokens.append(tmp)\n",
    "    return tokens\n",
    "\n",
    "########################################\n",
    "# 4) Dataset + Collation\n",
    "########################################\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates PyTorch dataset with BERT subwords + CharBERT graphemes.\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.inputs, self.all_labels = self.build_inputs(texts, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return dictionary for a single sample\n",
    "        return {\n",
    "            \"input_ids\":      self.inputs[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
    "            \"token_type_ids\": self.inputs[\"token_type_ids\"][idx],\n",
    "            \"char_input\":     self.inputs[\"char_input\"][idx],  # graphemes\n",
    "            \"labels\":         self.all_labels[idx]\n",
    "        }\n",
    "\n",
    "    def build_inputs(self, text_list, label_list):\n",
    "        input_ids_list, attention_mask_list, token_type_list, tag_list = [], [], [], []\n",
    "        char_sequences = []\n",
    "\n",
    "        for sentence, labels in zip(text_list, label_list):\n",
    "            tokens = custom_tokenize(sentence)\n",
    "            new_labels = []\n",
    "            tokenized_ids = []\n",
    "            char_seq = []\n",
    "\n",
    "            # 1) BERT Subword Tokenization + Expand labels\n",
    "            for word, tag in zip(tokens, labels):\n",
    "                sub_tokens = tokenizer.tokenize(word)\n",
    "                sub_ids = tokenizer.convert_tokens_to_ids(sub_tokens)\n",
    "                # Expand labels to subwords\n",
    "                labels_expanded = [tag] + ([ftag] * (len(sub_tokens) - 1))\n",
    "\n",
    "                tokenized_ids.extend(sub_ids)\n",
    "                new_labels.extend(labels_expanded)\n",
    "\n",
    "                # 2) Build Char/Grapheme sequence for each sub-token\n",
    "                # We'll do only one set of graphemes per *original word*, \n",
    "                # then replicate for sub_tokens\n",
    "                g_list = grapheme_split(word)\n",
    "                # Convert to 'text' form so Keras Tokenizer can map to int\n",
    "                # We'll just join with space\n",
    "                grapheme_text = \" \".join(g_list)  \n",
    "                grapheme_ids = char_tokenizer.texts_to_sequences([grapheme_text])[0]\n",
    "                # Pad or truncate\n",
    "                grapheme_ids = grapheme_ids[:MAX_CHAR_LEN]\n",
    "                grapheme_ids += [0] * (MAX_CHAR_LEN - len(grapheme_ids))\n",
    "\n",
    "                # For each sub-token of the word, we replicate the same char seq\n",
    "                for _ in sub_tokens:\n",
    "                    char_seq.append(grapheme_ids)\n",
    "\n",
    "            # 3) Add [CLS] and [SEP]\n",
    "            tokenized_ids = [tokenizer.cls_token_id] + tokenized_ids + [tokenizer.sep_token_id]\n",
    "            new_labels = [0] + new_labels + [0]\n",
    "            attention_mask = [1] * len(tokenized_ids)\n",
    "            token_type_ids = [0] * len(tokenized_ids)\n",
    "\n",
    "            # For char_input, replicate [CLS]/[SEP] as zero-vector\n",
    "            cls_chars = [0] * MAX_CHAR_LEN\n",
    "            sep_chars = [0] * MAX_CHAR_LEN\n",
    "            char_seq = [cls_chars] + char_seq + [sep_chars]\n",
    "\n",
    "            # 4) Truncate & Pad to sequence_length\n",
    "            cur_len = len(tokenized_ids)\n",
    "            if cur_len > sequence_length:\n",
    "                tokenized_ids = tokenized_ids[:sequence_length]\n",
    "                new_labels   = new_labels[:sequence_length]\n",
    "                attention_mask = attention_mask[:sequence_length]\n",
    "                token_type_ids = token_type_ids[:sequence_length]\n",
    "                char_seq = char_seq[:sequence_length]\n",
    "            elif cur_len < sequence_length:\n",
    "                pad_len = sequence_length - cur_len\n",
    "                tokenized_ids   += [0] * pad_len\n",
    "                new_labels      += [etag] * pad_len\n",
    "                attention_mask  += [0] * pad_len\n",
    "                token_type_ids  += [0] * pad_len\n",
    "                for _ in range(pad_len):\n",
    "                    char_seq.append([0]*MAX_CHAR_LEN)\n",
    "\n",
    "            # Final shape checks\n",
    "            # assert len(tokenized_ids) == sequence_length\n",
    "            # assert len(new_labels)    == sequence_length\n",
    "            # assert len(attention_mask)== sequence_length\n",
    "            # assert len(token_type_ids)== sequence_length\n",
    "            # assert len(char_seq)      == sequence_length\n",
    "            # Final shape checks\n",
    "            if not (len(tokenized_ids) == sequence_length and \n",
    "                    len(new_labels) == sequence_length and \n",
    "                    len(attention_mask) == sequence_length and \n",
    "                    len(token_type_ids) == sequence_length and \n",
    "                    len(char_seq) == sequence_length):\n",
    "                \n",
    "                print(\"\\n‚ùó Assertion failed on one sample:\")\n",
    "                print(\"Original sentence:\", sentence)\n",
    "                print(\"Tokenized words:\", tokens)\n",
    "                print(\"Subword token IDs:\", tokenized_ids)\n",
    "                print(\"New labels:\", new_labels)\n",
    "                print(\"char_seq shape:\", len(char_seq))\n",
    "                print(\"Lengths => tokenized_ids:\", len(tokenized_ids), \n",
    "                      \"| new_labels:\", len(new_labels), \n",
    "                      \"| attention_mask:\", len(attention_mask), \n",
    "                      \"| token_type_ids:\", len(token_type_ids), \n",
    "                      \"| char_seq:\", len(char_seq))\n",
    "                \n",
    "                raise AssertionError(\"Mismatch in sequence lengths\")\n",
    "            \n",
    "            # If passes, continue with tensor conversion\n",
    "\n",
    "\n",
    "            # Convert to tensors\n",
    "            input_ids_list.append(torch.tensor(tokenized_ids, dtype=torch.long))\n",
    "            attention_mask_list.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "            token_type_list.append(torch.tensor(token_type_ids, dtype=torch.long))\n",
    "            tag_list.append(torch.tensor(new_labels, dtype=torch.long))\n",
    "            char_sequences.append(torch.tensor(char_seq, dtype=torch.long))\n",
    "\n",
    "        inputs_dict = {\n",
    "            \"input_ids\":      torch.stack(input_ids_list),\n",
    "            \"attention_mask\": torch.stack(attention_mask_list),\n",
    "            \"token_type_ids\": torch.stack(token_type_list),\n",
    "            \"char_input\":     torch.stack(char_sequences)\n",
    "        }\n",
    "        labels_tensor = torch.stack(tag_list)\n",
    "        return inputs_dict, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:12.390422Z",
     "iopub.status.busy": "2025-04-06T16:09:12.390112Z",
     "iopub.status.idle": "2025-04-06T16:09:27.534886Z",
     "shell.execute_reply": "2025-04-06T16:09:27.533795Z",
     "shell.execute_reply.started": "2025-04-06T16:09:12.390399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Build Datasets & DataLoaders\n",
    "########################################\n",
    "import random\n",
    "data = list(zip(raw_text, raw_label))\n",
    "tdata= list(zip(raw_test_text, raw_test_label))\n",
    "random.shuffle(data)\n",
    "random.shuffle(tdata)\n",
    "\n",
    "shuffled_text,  shuffled_label  = zip(*data)\n",
    "shuffled_ttext, shuffled_tlabel = zip(*tdata)\n",
    "\n",
    "split_1 = int(0.8 * len(shuffled_text))\n",
    "train_dataset = NERDataset(shuffled_text[:split_1],  shuffled_label[:split_1])\n",
    "valid_dataset = NERDataset(shuffled_text[split_1:], shuffled_label[split_1:])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "########################################\n",
    "# 5) Define the CharBERT Model\n",
    "########################################\n",
    "import torch.nn as nn\n",
    "\n",
    "class CharEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A GRU-based encoder for graphemes, akin to CharBERT's character encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, char_vocab_size, char_embed_dim=30, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=char_embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "    def forward(self, char_input):\n",
    "        \"\"\"\n",
    "        char_input: (batch, seq_len, MAX_CHAR_LEN)\n",
    "        Output: (batch, seq_len, hidden_dim*2)\n",
    "        \"\"\"\n",
    "        b, sl, cl = char_input.shape\n",
    "        # Flatten for GRU\n",
    "        char_input = char_input.view(b*sl, cl)\n",
    "\n",
    "        emb = self.char_embedding(char_input)  # (b*sl, cl, char_embed_dim)\n",
    "        # forward GRU\n",
    "        _, h_n = self.gru(emb)  # h_n: (2, b*sl, hidden_dim)\n",
    "        # concat final states from both directions\n",
    "        char_rep = torch.cat([h_n[0], h_n[1]], dim=-1)  # (b*sl, hidden_dim*2)\n",
    "\n",
    "        # reshape back\n",
    "        char_rep = char_rep.view(b, sl, -1)\n",
    "        return char_rep\n",
    "\n",
    "class CharBERT_NER(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels, char_vocab_size):\n",
    "        super().__init__()\n",
    "        self.bert = bert_model\n",
    "        hidden_dim = self.bert.config.hidden_size  # 1024 for xlm-roberta-large\n",
    "\n",
    "        # Char GRU\n",
    "        self.char_encoder = CharEncoder(char_vocab_size, char_embed_dim=30, hidden_dim=32)\n",
    "        # Output dimension from char_encoder = 128 (64*2)\n",
    "\n",
    "        # BiLSTM for final sequence modeling\n",
    "        combined_dim = hidden_dim + 64\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=combined_dim,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        self.fc = nn.Linear(128*2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, char_input):\n",
    "        # 1) BERT forward\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        sequence_output = outputs.last_hidden_state  # (batch, seq_len, 1024)\n",
    "\n",
    "        # 2) Char Encoder (GRU)\n",
    "        char_repr = self.char_encoder(char_input)    # (batch, seq_len, 128)\n",
    "\n",
    "        # 3) Fuse representations\n",
    "        fused = torch.cat([sequence_output, char_repr], dim=-1)  # (batch, seq_len, 1024+128)\n",
    "\n",
    "        # 4) BiLSTM\n",
    "        lstm_out, _ = self.lstm(fused)               # (batch, seq_len, 1024)\n",
    "        logits = self.fc(lstm_out)                   # (batch, seq_len, num_labels)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:27.536368Z",
     "iopub.status.busy": "2025-04-06T16:09:27.536054Z",
     "iopub.status.idle": "2025-04-06T16:09:29.561527Z",
     "shell.execute_reply": "2025-04-06T16:09:29.560127Z",
     "shell.execute_reply.started": "2025-04-06T16:09:27.536331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = NERDataset(shuffled_ttext, shuffled_tlabel)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:33.249672Z",
     "iopub.status.busy": "2025-04-06T16:09:33.249309Z",
     "iopub.status.idle": "2025-04-06T16:09:35.184596Z",
     "shell.execute_reply": "2025-04-06T16:09:35.183586Z",
     "shell.execute_reply.started": "2025-04-06T16:09:33.249646Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchmetrics import Precision, Recall, F1Score, Accuracy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharBERT_NER(bert_model, num_labels, CHAR_VOCAB_SIZE).to(device)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": model.bert.parameters(), \"lr\": 5e-6},  \n",
    "    {\"params\": model.lstm.parameters(), \"lr\": 2e-4},  \n",
    "    {\"params\": model.fc.parameters(),   \"lr\": 2e-4},\n",
    "    {\"params\": model.char_encoder.parameters(), \"lr\": 2e-4},\n",
    "])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=etag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:36.034137Z",
     "iopub.status.busy": "2025-04-06T16:09:36.033532Z",
     "iopub.status.idle": "2025-04-06T16:09:36.043827Z",
     "shell.execute_reply": "2025-04-06T16:09:36.043009Z",
     "shell.execute_reply.started": "2025-04-06T16:09:36.034112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import Precision, Recall, F1Score, Accuracy\n",
    "\n",
    "precision = Precision(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\n",
    "recall    = Recall(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\n",
    "f1        = F1Score(task=\"multiclass\", average=\"macro\", num_classes=num_labels)\n",
    "accuracy  = Accuracy(task=\"multiclass\", num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:38.426816Z",
     "iopub.status.busy": "2025-04-06T16:09:38.426512Z",
     "iopub.status.idle": "2025-04-06T16:09:38.431806Z",
     "shell.execute_reply": "2025-04-06T16:09:38.430726Z",
     "shell.execute_reply.started": "2025-04-06T16:09:38.426794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_metrics(preds, labels, ignore_index=etag):\n",
    "    # Flatten\n",
    "    preds = preds.view(-1)\n",
    "    labels= labels.view(-1)\n",
    "\n",
    "    mask = (labels != ignore_index)\n",
    "    preds, labels = preds[mask], labels[mask]\n",
    "\n",
    "    p = precision(preds, labels)\n",
    "    r = recall(preds, labels)\n",
    "    f = f1(preds, labels)\n",
    "    a = accuracy(preds, labels)\n",
    "    return p.item(), r.item(), f.item(), a.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:38.779390Z",
     "iopub.status.busy": "2025-04-06T16:09:38.778959Z",
     "iopub.status.idle": "2025-04-06T16:09:38.788888Z",
     "shell.execute_reply": "2025-04-06T16:09:38.787796Z",
     "shell.execute_reply.started": "2025-04-06T16:09:38.779357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids      = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "        char_input     = batch[\"char_input\"].to(device)\n",
    "        labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids, attention_mask, token_type_ids, char_input)\n",
    "        loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_preds.append(torch.argmax(logits, dim=-1).cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_preds  = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    precision_, recall_, f1_, accuracy_ = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "    return float(avg_loss), precision_, recall_, f1_, accuracy_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:53.502541Z",
     "iopub.status.busy": "2025-04-06T16:09:53.502171Z",
     "iopub.status.idle": "2025-04-06T16:09:53.511578Z",
     "shell.execute_reply": "2025-04-06T16:09:53.510556Z",
     "shell.execute_reply.started": "2025-04-06T16:09:53.502510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, epochs=10, patience=3):\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids      = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            token_type_ids = batch[\"token_type_ids\"].to(device)\n",
    "            char_input     = batch[\"char_input\"].to(device)\n",
    "            labels         = batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask, token_type_ids, char_input)\n",
    "            loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1).cpu()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        all_preds  = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "        train_p, train_r, train_f1, train_a = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_p, val_r, val_f1, val_a = evaluate_model(model, valid_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | P: {train_p:.4f} | R: {train_r:.4f} | F1: {train_f1:.4f} | Acc: {train_a:.4f}\")\n",
    "        print(f\"Valid Loss: {val_loss:.4f} | P: {val_p:.4f} | R: {val_r:.4f} | F1: {val_f1:.4f} | Acc: {val_a:.4f}\")\n",
    "\n",
    "        # Early Stopping + Full Model Save\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model, \"charbert_full_model1.pt\")  # ‚úÖ Save full model\n",
    "            print(\"‚úÖ Full model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚è≥ Patience Counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"\\nüõë Early stopping triggered. Restoring best full model...\")\n",
    "            model = torch.load(\"charbert_full_model1.pt\")\n",
    "            model.to(device)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:09:57.097302Z",
     "iopub.status.busy": "2025-04-06T16:09:57.097017Z",
     "iopub.status.idle": "2025-04-06T16:38:42.513783Z",
     "shell.execute_reply": "2025-04-06T16:38:42.512873Z",
     "shell.execute_reply.started": "2025-04-06T16:09:57.097280Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "Train Loss: 0.2428 | P: 0.9028 | R: 0.8512 | F1: 0.8741 | Acc: 0.9135\n",
      "Valid Loss: 0.0759 | P: 0.9529 | R: 0.9630 | F1: 0.9579 | Acc: 0.9768\n",
      "‚úÖ Full model saved!\n",
      "\n",
      "Epoch 2/10\n",
      "Train Loss: 0.0721 | P: 0.9602 | R: 0.9614 | F1: 0.9608 | Acc: 0.9786\n",
      "Valid Loss: 0.0685 | P: 0.9628 | R: 0.9661 | F1: 0.9643 | Acc: 0.9805\n",
      "‚úÖ Full model saved!\n",
      "\n",
      "Epoch 3/10\n",
      "Train Loss: 0.0583 | P: 0.9688 | R: 0.9709 | F1: 0.9698 | Acc: 0.9835\n",
      "Valid Loss: 0.0659 | P: 0.9605 | R: 0.9717 | F1: 0.9659 | Acc: 0.9812\n",
      "‚úÖ Full model saved!\n",
      "\n",
      "Epoch 4/10\n",
      "Train Loss: 0.0505 | P: 0.9739 | R: 0.9753 | F1: 0.9746 | Acc: 0.9861\n",
      "Valid Loss: 0.0701 | P: 0.9644 | R: 0.9696 | F1: 0.9669 | Acc: 0.9818\n",
      "‚è≥ Patience Counter: 1/3\n",
      "\n",
      "Epoch 5/10\n",
      "Train Loss: 0.0440 | P: 0.9771 | R: 0.9791 | F1: 0.9781 | Acc: 0.9880\n",
      "Valid Loss: 0.0700 | P: 0.9610 | R: 0.9682 | F1: 0.9645 | Acc: 0.9804\n",
      "‚è≥ Patience Counter: 2/3\n",
      "\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0395 | P: 0.9798 | R: 0.9819 | F1: 0.9809 | Acc: 0.9896\n",
      "Valid Loss: 0.0700 | P: 0.9628 | R: 0.9698 | F1: 0.9662 | Acc: 0.9814\n",
      "‚è≥ Patience Counter: 3/3\n",
      "\n",
      "üõë Early stopping triggered. Restoring best full model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-d9db2a25ba7a>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"charbert_full_model1.pt\")\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T16:38:56.887763Z",
     "iopub.status.busy": "2025-04-06T16:38:56.887247Z",
     "iopub.status.idle": "2025-04-06T16:39:05.204891Z",
     "shell.execute_reply": "2025-04-06T16:39:05.203871Z",
     "shell.execute_reply.started": "2025-04-06T16:38:56.887711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Evaluation:\n",
      "  Loss: 0.0470\n",
      "  Precision: 0.9744\n",
      "  Recall: 0.9708\n",
      "  F1-Score: 0.9725\n",
      "  Accuracy: 0.9882\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1, test_accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "print(\"\\nTest Evaluation:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")       # ‚úÖ Now it won't crash                                                                                                                    \n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:57:38.232716Z",
     "iopub.status.busy": "2025-04-06T13:57:38.232353Z",
     "iopub.status.idle": "2025-04-06T13:57:40.332577Z",
     "shell.execute_reply": "2025-04-06T13:57:40.331745Z",
     "shell.execute_reply.started": "2025-04-06T13:57:38.232676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-5f986dd3ca57>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"charbert_full_model.pt\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full model reloaded and ready for use!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the full model\n",
    "model = torch.load(\"charbert_full_model.pt\", map_location=device)\n",
    "\n",
    "# Move to GPU if available\n",
    "model.to(device)\n",
    "\n",
    "# Set to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ Full model reloaded and ready for use!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:57:51.143842Z",
     "iopub.status.busy": "2025-04-06T13:57:51.143547Z",
     "iopub.status.idle": "2025-04-06T13:57:51.147899Z",
     "shell.execute_reply": "2025-04-06T13:57:51.146883Z",
     "shell.execute_reply.started": "2025-04-06T13:57:51.143815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_id_to_str = {\n",
    "    0: \"O\",\n",
    "    1: \"X\",\n",
    "    2: \"Y\",\n",
    "    # If you had more, add them here\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:57:55.772617Z",
     "iopub.status.busy": "2025-04-06T13:57:55.772297Z",
     "iopub.status.idle": "2025-04-06T13:57:55.785366Z",
     "shell.execute_reply": "2025-04-06T13:57:55.784475Z",
     "shell.execute_reply.started": "2025-04-06T13:57:55.772586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_on_text_charbert(model, text, tokenizer, char_tokenizer, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Predict the NER tags for a single input text using CharBERT-style approach \n",
    "    (BERT subwords + grapheme-based character embeddings).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CharBERT_NER model (with .bert + .char_encoder + .lstm).\n",
    "        text: Raw Bengali text string.\n",
    "        tokenizer: HuggingFace tokenizer (AutoTokenizer) for BERT subwords.\n",
    "        char_tokenizer: Keras tokenizer trained on grapheme sequences.\n",
    "        max_seq_len: Max sequence length used in training/fine-tuning.\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, predicted_label) tuples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1Ô∏è‚É£ Custom word-level splitting (punct, special chars)\n",
    "    original_tokens = custom_tokenize(text)\n",
    "\n",
    "    # 2Ô∏è‚É£ BERT subword tokenization + mapping\n",
    "    bert_tokens = []\n",
    "    token_mapping = []  # Maps subword indices back to original_tokens indices\n",
    "    for idx, tok in enumerate(original_tokens):\n",
    "        subwords = tokenizer.tokenize(tok)\n",
    "        bert_tokens.extend(subwords)\n",
    "        token_mapping.extend([idx] * len(subwords))\n",
    "\n",
    "    # 3Ô∏è‚É£ Convert to BERT token IDs (CLS + subwords + SEP)\n",
    "    input_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(bert_tokens) + [tokenizer.sep_token_id]\n",
    "    token_mapping = [-1] + token_mapping + [-1]  # -1 for CLS and SEP\n",
    "\n",
    "    # 4Ô∏è‚É£ Build Char/Grapheme input\n",
    "    #    We only do one grapheme sequence per *original token*, then replicate for subwords.\n",
    "    char_seq = []\n",
    "    for word in original_tokens:\n",
    "        # Split into graphemes\n",
    "        g_list = grapheme_split(word)\n",
    "        # Join with space so Keras char_tokenizer can parse them as separate tokens\n",
    "        grapheme_text = \" \".join(g_list)\n",
    "        grapheme_ids = char_tokenizer.texts_to_sequences([grapheme_text])[0] if grapheme_text else []\n",
    "        # Pad/truncate\n",
    "        grapheme_ids = grapheme_ids[:MAX_CHAR_LEN]\n",
    "        grapheme_ids += [0] * (MAX_CHAR_LEN - len(grapheme_ids))\n",
    "        # We'll store, replicate later\n",
    "        char_seq.append(grapheme_ids)\n",
    "\n",
    "    #    Replicate char_seq for subwords + add CLS/SEP placeholders\n",
    "    replicated_char_seq = [[0]*MAX_CHAR_LEN]  # CLS\n",
    "    idx_char = 0\n",
    "    for idx, tok in enumerate(original_tokens):\n",
    "        subword_count = sum([1 for m in token_mapping if m == idx]) \n",
    "        # replicate the same grapheme vector subword_count times\n",
    "        for _ in range(subword_count):\n",
    "            replicated_char_seq.append(char_seq[idx_char])\n",
    "        idx_char += 1\n",
    "    replicated_char_seq.append([0]*MAX_CHAR_LEN)  # SEP\n",
    "\n",
    "    # 5Ô∏è‚É£ Now we do sequence-level padding/truncation\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "        attention_mask = attention_mask[:max_seq_len]\n",
    "        token_mapping = token_mapping[:max_seq_len]\n",
    "        replicated_char_seq = replicated_char_seq[:max_seq_len]\n",
    "    else:\n",
    "        pad_len = max_seq_len - len(input_ids)\n",
    "        input_ids += [0]*pad_len\n",
    "        attention_mask += [0]*pad_len\n",
    "        token_mapping += [-1]*pad_len\n",
    "        # Pad char_seq\n",
    "        for _ in range(pad_len):\n",
    "            replicated_char_seq.append([0]*MAX_CHAR_LEN)\n",
    "\n",
    "    # 6Ô∏è‚É£ Convert to tensors\n",
    "    input_ids      = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    token_type_ids = torch.zeros_like(input_ids).to(device)  # typical for single-sentence inputs\n",
    "    char_input     = torch.tensor([replicated_char_seq], dtype=torch.long).to(device)\n",
    "\n",
    "    # 7Ô∏è‚É£ Forward pass with no grad\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            char_input=char_input\n",
    "        )\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze(0)  # shape: (seq_len,)\n",
    "\n",
    "    # 8Ô∏è‚É£ Map subword predictions back to original tokens\n",
    "    token_level_preds = {}\n",
    "    for i, label_id in enumerate(preds.cpu().numpy()):\n",
    "        orig_idx = token_mapping[i]\n",
    "        if orig_idx == -1:\n",
    "            continue  # skip CLS, SEP, padding\n",
    "        # first subword of a token can represent the entire token\n",
    "        # store only the first subword's predicted label\n",
    "        if orig_idx not in token_level_preds:\n",
    "            token_level_preds[orig_idx] = label_id\n",
    "\n",
    "    # 9Ô∏è‚É£ Generate final output\n",
    "    results = []\n",
    "    for i, word in enumerate(original_tokens):\n",
    "        label_id = token_level_preds.get(i, 0)  # default label if missing\n",
    "        label_str = label_id_to_str[label_id]   # map ID ‚Üí \"O\", \"X\", \"Y\", etc.\n",
    "        results.append((word, label_str))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:58:05.459561Z",
     "iopub.status.busy": "2025-04-06T13:58:05.459221Z",
     "iopub.status.idle": "2025-04-06T13:58:05.519070Z",
     "shell.execute_reply": "2025-04-06T13:58:05.518153Z",
     "shell.execute_reply.started": "2025-04-06T13:58:05.459532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡ß®‡ß¶‡ß®‡ß¶\tX\n",
      "‡¶∏‡¶æ‡¶≤‡ßá\tO\n",
      "‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø\tX\n",
      "‡¶õ‡¶ø‡¶≤\tO\n",
      "‡ß´.‡ß®\tY\n",
      "%\tY\n",
      ",\tO\n",
      "‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ\tO\n",
      "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø\tX\n",
      "‡¶õ‡¶ø‡¶≤\tO\n",
      "‡ß©.‡ßß\tY\n",
      "%\tY\n",
      "‡•§\tO\n",
      "‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá\tO\n",
      ",\tO\n",
      "‡ß®‡ß¶‡ß®‡ßß\tX\n",
      "‡¶∏‡¶æ‡¶≤‡ßá\tO\n",
      "‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞\tO\n",
      "‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø\tX\n",
      "‡¶¨‡ßá‡ßú‡ßá\tO\n",
      "‡ß¨.‡ß≠\tY\n",
      "%\tY\n",
      "‡¶π‡¶≤‡ßã\tO\n",
      ",\tO\n",
      "‡¶Ü‡¶∞\tO\n",
      "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø\tX\n",
      "‡¶ï‡¶Æ‡ßá\tO\n",
      "‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã\tO\n",
      "‡ß®.‡ßß\tY\n",
      "%\tY\n",
      "‡•§\tO\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶∞‡¶ø‡¶®‡¶æ ‡¶ì ‡¶Æ‡¶ø‡¶û‡¶æ ‡¶Ø‡¶•‡¶æ‡¶ï‡ßç‡¶∞‡¶Æ‡ßá ‡¶ó‡¶£‡¶ø‡¶§‡ßá ‡ß´‡ß¶ ‡¶ì ‡ß¨‡ß´ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶™‡ßá‡ßü‡ßá‡¶õ‡ßá. ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∂‡¶ø‡¶Æ‡¶æ ‡¶™‡ßá‡ßü‡ßá‡¶õ‡ßá ‡ß™‡ß¶ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞\"\n",
    "new_text=\"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã ‡ß®.‡ßß%‡•§\"\n",
    "# Call predict\n",
    "predictions = predict_on_text_charbert(model, new_text, tokenizer, char_tokenizer, max_seq_len=128)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ‚úÖ Load the LLaMA 3 model from GGUF format\n",
    "MODEL_PATH = \"/kaggle/input/llma3_8b/pytorch/llama3_8b/1/Meta-Llama-3-8B-Instruct-Q6_K.gguf\"\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_gpu_layers=-1)  # Use GPU acceleration\n",
    "\n",
    "def simplify_text(text):\n",
    "    \"\"\"Uses LLaMA-3 GGUF model to simplify numerical text intelligently.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡ßÉ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶§‡ßç‡¶§‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤, ‡¶Ø‡¶æ‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶∏‡¶Æ‡ßÉ‡¶¶‡ßç‡¶ß ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶æ‡¶†‡ßç‡¶Ø ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶¨‡ßá‡•§\n",
    "    ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ì‡¶á ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶• ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶è‡¶¨‡¶Ç ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶¨‡¶ø‡¶¨‡ßÉ‡¶§‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶∏‡ßÅ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡ßÄ‡¶ï‡¶∞‡¶£ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§\n",
    "    **‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:**\n",
    "    ‡ßß. ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡ßã‡¶∞‡ßã ‡¶®‡¶æ, ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶∏‡¶π‡¶ú ‡¶ï‡¶∞‡ßã‡•§\n",
    "    ‡ß®. ‡¶ï‡ßã‡¶®‡ßã ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡ßã‡¶∞‡ßã ‡¶®‡¶æ‡•§\n",
    "    ‡ß©. ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶∏‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡ßã‡•§\n",
    "    Input: ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶õ‡ßá ‡ß™‡ß¶‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶∂‡¶´‡¶ø‡¶ï‡ßá‡¶∞ ‡¶§‡¶æ‡¶∞ ‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶∞‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶õ‡ßá ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶ß‡ßá‡¶ï‡•§ \n",
    "    Output: ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß™‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶∏‡¶´‡¶ø‡¶ï‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ßÆ‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶∞‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß®‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\n",
    "    \n",
    "    Input: ‡¶™‡ßç‡¶∞‡¶æ‡¶£ ‡¶¨‡ßá‡¶≠‡¶æ‡¶∞‡ßá‡¶ú ‡¶è‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡ß®.‡ß´% ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá, ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ü‡¶Ø‡¶º ‡¶õ‡¶ø‡¶≤ ‡ß®,‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶ì ‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶£ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶Ø‡¶º ‡¶π‡¶¨‡ßá‡•§\n",
    "    Output: ‡¶™‡ßç‡¶∞‡¶æ‡¶£ ‡¶¨‡ßá‡¶≠‡¶æ‡¶∞‡ßá‡¶ú ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ü‡ßü ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡ß®,‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶è‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ü‡ßü ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡ß®,‡ß´‡ß¨,‡ß®‡ß´‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡ßá ‡¶Ü‡ßü ‡¶π‡¶¨‡ßá ‡ß´,‡ß¶‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\n",
    "    \n",
    "    Input: ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ‡¶Ø‡¶º ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶£‡ßç‡¶° ‡¶ó‡¶∞‡¶Æ ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Ø‡¶º ‡ß®.‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏ ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶õ‡¶ø‡¶≤ ‡ß©‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§\n",
    "    Output: ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ‡¶Ø‡¶º ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶£‡ßç‡¶° ‡¶ó‡¶∞‡¶Æ ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶õ‡¶ø‡¶≤ ‡ß©‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§ ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡ß©‡ß≠.‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§\n",
    "\n",
    "    ---\n",
    "    **‡¶á‡¶®‡¶™‡ßÅ‡¶ü:** \"{text}\"  \n",
    "    **‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü:**  \n",
    "    \"\"\"\n",
    "\n",
    "    # ‚úÖ Generate response using LLaMA-3 GGUF\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=512,  # More room for generated text\n",
    "        temperature=0.1,  # Less randomness for accuracy\n",
    "        top_p=0.9,  # Balanced output\n",
    "        stop=['\\n']  # Don't cut off early\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Extract and clean output\n",
    "    simplified_text = output[\"choices\"][0][\"text\"].strip().replace(\"\\n\", \" \")\n",
    "\n",
    "    # ‚úÖ Ensure output is meaningful\n",
    "    if not simplified_text or len(simplified_text.split()) < 3:\n",
    "        return \"Error: Model did not generate a valid response.\"\n",
    "\n",
    "    return simplified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_test_sentences(text, max_words=150):\n",
    "    \"\"\"\n",
    "    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n",
    "    Ensures words are not split in the middle.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = []\n",
    "    \n",
    "    # ‚úÖ Process in chunks of `max_words`\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n",
    "        chunk_text = \" \".join(chunk_words)  # Convert back to text\n",
    "        chunks.append(chunk_text.strip())\n",
    "    print([chunk for chunk in chunks])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_test_sentences(text, max_words=150):\n",
    "    \"\"\"\n",
    "    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n",
    "    Ensures words are not split in the middle.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = []\n",
    "    \n",
    "    # ‚úÖ Process in chunks of `max_words`\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n",
    "        chunk_text = \" \".join(chunk_words)  # Convert back to text\n",
    "        chunks.append(chunk_text.strip())\n",
    "    print([chunk for chunk in chunks])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def simplify_large_text(text):\n",
    "    \"\"\"\n",
    "    Breaks large text into token-based chunks, processes each with LLaMA, and merges results.\n",
    "    \"\"\"\n",
    "    chunks = convert_to_test_sentences(text)  # Token-based chunking\n",
    "    simplified_chunks = [simplify_text(chunk) for chunk in chunks]  # Process each chunk\n",
    "    \n",
    "    return \" \".join(simplified_chunks)  # Merge all processed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"‡¶∏‡¶æ‡ßü‡¶∞‡¶æ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡ßß‡ß¶‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶™‡¶æ‡ßü ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ ‡¶•‡ßá‡¶ï‡ßá‡•§ ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶§‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶Æ‡¶æ‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶è‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü‡•§ ‡¶Ö‡¶ï‡ßç‡¶ü‡ßã‡¶¨‡¶∞‡ßá ‡¶≤‡¶æ‡¶≠‡ßá‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶∞‡ßã ‡ß©‡ß¶‡ß¶‡ß¶ ‡¶ï‡¶Æ‡ßá ‡ß≠‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü‡•§ ‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶≠‡ßá‡¶∞ ‡¶Æ‡ßÅ‡¶ñ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶∏‡¶æ‡ßü‡¶∞‡¶æ‡•§ ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶∂‡ßá‡¶∑‡¶¶‡¶ø‡¶ï‡ßá ‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü ‡ß®‡ß¶‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\"\"\"\n",
    "# ‚úÖ Process and get final simplified output\n",
    "final_simplified_output = simplify_large_text(text)\n",
    "\n",
    "# ‚úÖ Print the final merged output\n",
    "print(\"Final Simplified Output:\", final_simplified_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = final_simplified_output\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã ‡ß®.‡ßß%‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n",
    "# Run validation\n",
    "valid_pairs = validate_xy_pairs(tokens, labels)\n",
    "\n",
    "# Output valid pairs\n",
    "print(\"‚úÖ Valid X-Y Pairs:\", valid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶ó‡ßá‡¶≤‡ßã‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶ï‡ßã‡¶≠‡¶ø‡¶° ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶¨‡¶õ‡¶∞ ‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶õ‡ßü ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡ßß‡ß©‡ß¶ ‡¶ú‡¶® ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡ßü ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶∞‡¶ø‡ßü‡¶æ‡ßü‡•§ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶¨‡¶õ‡¶∞ ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶è ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶õ‡¶ø‡¶≤¬†‡¶∏‡¶æ‡¶§¬†‡¶π‡¶æ‡¶ú‡¶æ‡¶∞¬†‡ß®‡ßØ‡ß™‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶∞‡¶ø‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶∞‡ßç‡¶∑‡¶ø‡¶ï ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü ‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡ßü ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡ßÄ‡¶Æ‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶§‡ßÉ‡¶§‡ßÄ‡ßü, ‡¶´‡¶ú‡¶≤‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶™‡¶û‡ßç‡¶ö‡¶Æ\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import re\n",
    "txt=\"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶§‡¶ø‡¶® ‡¶ï‡ßã‡¶ü‡¶ø ‡¶¶‡ßÅ‡¶á ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡¶∂‡ßã ‡¶§‡ßç‡¶∞‡¶ø‡¶∂ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá ‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"# Suppose you have a new text snippet\n",
    "new_text = txt\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n",
    "\n",
    "def convert_bengali_text_to_number(tokens, labels):\n",
    "    \"\"\"\n",
    "    Converts Bengali number words to their numerical values based on the 'Y' labels.\n",
    "    \n",
    "    Args:\n",
    "    - tokens (list): List of words (tokens) from the sentence.\n",
    "    - labels (list): Corresponding NER labels (X, Y, O).\n",
    "\n",
    "    Returns:\n",
    "    - converted_tokens (list): Tokens with 'Y' values converted to numbers.\n",
    "    \"\"\"\n",
    "    # Bengali number words mapping\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4,\n",
    "        \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶è‡¶ó‡¶æ‡¶∞‡ßã\": 11, \"‡¶¨‡¶æ‡¶∞‡ßã\": 12, \"‡¶§‡ßá‡¶∞‡ßã\": 13, \"‡¶ö‡ßå‡¶¶‡ßç‡¶¶\": 14,\n",
    "        \"‡¶™‡¶®‡ßá‡¶∞‡ßã\": 15, \"‡¶∑‡ßã‡¶≤\": 16, \"‡¶∏‡¶§‡ßá‡¶∞‡ßã\": 17, \"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã\": 18, \"‡¶ä‡¶®‡¶ø‡¶∂\": 19,\n",
    "        \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶è‡¶ï‡ßÅ‡¶∂\": 21, \"‡¶¨‡¶æ‡¶á‡¶∂\": 22, \"‡¶§‡ßá‡¶á‡¶∂\": 23, \"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 24,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶ø‡¶∂\": 25, \"‡¶õ‡¶æ‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 26, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂\": 27, \"‡¶Ü‡¶†‡¶æ‡¶∂\": 28, \"‡¶ä‡¶®‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 29,\n",
    "        \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 31, \"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 32, \"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 33, \"‡¶ö‡ßå‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 34,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 35, \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 36, \"‡¶∏‡¶æ‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 37, \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 38, \"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 39,\n",
    "        \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶è‡¶ï‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 41, \"‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 42, \"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 43, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 44,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 45, \"‡¶õ‡¶ø‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 46, \"‡¶∏‡¶æ‡¶§‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 47, \"‡¶Ü‡¶ü‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 48, \"‡¶ä‡¶®‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 49,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®\": 51, \"‡¶¨‡¶æ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 52, \"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 53, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 54,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶®‡ßç‡¶®\": 55, \"‡¶õ‡¶æ‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 56, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡ßç‡¶®\": 57, \"‡¶Ü‡¶ü‡¶æ‡¶®‡ßç‡¶®\": 58, \"‡¶ä‡¶®‡¶∑‡¶æ‡¶ü\": 59,\n",
    "        \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 61, \"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 62, \"‡¶§‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 63, \"‡¶ö‡ßå‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 64,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 65, \"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 66, \"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 67, \"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 68, \"‡¶ä‡¶®‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 69,\n",
    "        \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶è‡¶ï‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 71, \"‡¶¨‡¶æ‡¶π‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 72, \"‡¶§‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 73, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 74,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 75, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 76, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 77, \"‡¶Ü‡¶ü‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 78, \"‡¶ä‡¶®‡¶Ü‡¶∂‡¶ø\": 79,\n",
    "        \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶è‡¶ï‡¶æ‡¶∂‡¶ø\": 81, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 82, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 83, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø\": 84,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶∂‡¶ø\": 85, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶∂‡¶ø\": 86, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂‡¶ø\": 87, \"‡¶Ü‡¶ü‡¶æ‡¶∂‡¶ø\": 88, \"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 89,\n",
    "        \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90, \"‡¶è‡¶ï‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 91, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 92, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 93, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 94,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 95, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 96, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 97, \"‡¶Ü‡¶ü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 98, \"‡¶®‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 99,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500,\n",
    "        \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000,\n",
    "        \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "\n",
    "    # Convert words to numbers\n",
    "    converted_tokens = []\n",
    "    current_number = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        word = tokens[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        if label == \"Y\":\n",
    "            if word.isdigit():  # If it's already a number, store as integer\n",
    "                current_number += int(word)\n",
    "            elif word in bengali_number_map:  # Convert Bengali number words\n",
    "                temp_number = bengali_number_map[word]\n",
    "\n",
    "                # Look ahead for a multiplier\n",
    "                if (i + 1) < len(tokens) and tokens[i + 1] in multipliers:\n",
    "                    temp_number *= multipliers[tokens[i + 1]]\n",
    "                    i += 1  # Skip the multiplier\n",
    "\n",
    "                current_number += temp_number\n",
    "            else:\n",
    "                converted_tokens.append(word)  # Keep other words unchanged\n",
    "        else:\n",
    "            if current_number > 0:\n",
    "                converted_tokens.append(current_number)\n",
    "                current_number = 0  # Reset for next number\n",
    "            converted_tokens.append(word)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # If there's an unprocessed number at the end\n",
    "    if current_number > 0:\n",
    "        converted_tokens.append(current_number)\n",
    "\n",
    "    return converted_tokens\n",
    "\n",
    "\n",
    "# Run the function\n",
    "token,labels=valid_pairs = validate_xy_pairs(tokens, labels)\n",
    "\n",
    "# Output valid pairs\n",
    "print(\"‚úÖ Valid X-Y Pairs:\", valid_pairs)\n",
    "converted_tokens = convert_bengali_text_to_number(tokens, labels)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Converted Tokens:\", converted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def convert_bengali_text_to_number(tokens, labels):\n",
    "    \"\"\"Pairs `X` labels with `Y` words and prepares them for conversion.\"\"\"\n",
    "    \n",
    "    # Define Bengali Number Words and Multipliers\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4, \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500, \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "\n",
    "    paired_results = {}  # Store results\n",
    "    current_x = None\n",
    "    current_y_words = []  # Store all Y words\n",
    "    unit = None  # Store unit if found\n",
    "    last_multiplier = None  # Track last multiplier to prevent duplication\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        word, label = tokens[i], labels[i]\n",
    "\n",
    "        if label == \"X\":\n",
    "            if current_x and current_y_words:  \n",
    "                # Store previous X-Y pair with its unit (before conversion)\n",
    "                paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n",
    "                current_y_words = []  \n",
    "                unit = None  \n",
    "                last_multiplier = None  # Reset last multiplier\n",
    "\n",
    "            current_x = word  # Assign new X entity\n",
    "\n",
    "        elif label == \"Y\":\n",
    "            current_y_words.append(word)  # Store Y word\n",
    "\n",
    "            # Check if it's a multiplier and store only once\n",
    "            if word in multipliers and last_multiplier != word:\n",
    "                unit = word  # Store the first occurrence as the unit\n",
    "                last_multiplier = word  # Prevent duplicate multipliers\n",
    "\n",
    "        i += 1  \n",
    "\n",
    "    # Store the last X-Y pair\n",
    "    if current_x and current_y_words:\n",
    "        paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n",
    "\n",
    "    return paired_results, bengali_number_map, multipliers\n",
    "\n",
    "\n",
    "def convert_text_to_number(paired_results, bengali_number_map, multipliers):\n",
    "    \"\"\"\n",
    "    Converts Bengali number words into numeric values while keeping the unit.\n",
    "\n",
    "    Args:\n",
    "    - paired_results (dict): Mapping of `X` to `Y` words before conversion.\n",
    "    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n",
    "    - multipliers (dict): Mapping of Bengali multipliers (‡¶π‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶ï‡ßã‡¶ü‡¶ø, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - converted_results (dict): Mapping of `X` to final numeric values with units.\n",
    "    \"\"\"\n",
    "    converted_results = {}\n",
    "\n",
    "    for x_entity, data in paired_results.items():\n",
    "        y_words = data[\"words\"]\n",
    "        unit = data[\"unit\"] if data[\"unit\"] else \"\"\n",
    "\n",
    "        # Convert Bengali number words to a single numerical value\n",
    "        number_text = \" \".join(y_words)\n",
    "        number_value = convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers)\n",
    "\n",
    "        # Store the final result\n",
    "        converted_results[x_entity] = f\"{number_value} {unit}\"\n",
    "\n",
    "    return converted_results\n",
    "\n",
    "\n",
    "def convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers):\n",
    "    \"\"\"\n",
    "    Helper function to convert Bengali text numbers into numeric values.\n",
    "\n",
    "    Args:\n",
    "    - number_text (str): Bengali number words.\n",
    "    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n",
    "    - multipliers (dict): Mapping of Bengali multipliers (‡¶π‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶ï‡ßã‡¶ü‡¶ø, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - int: Converted numeric value.\n",
    "    \"\"\"\n",
    "    words = number_text.split()\n",
    "    total = 0\n",
    "    temp_value = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.isdigit():\n",
    "            temp_value += int(word)\n",
    "        elif word in bengali_number_map:\n",
    "            temp_value += bengali_number_map[word]\n",
    "        elif word in multipliers:\n",
    "            temp_value *= multipliers[word]\n",
    "            total += temp_value\n",
    "            temp_value = 0\n",
    "\n",
    "    return total + temp_value\n",
    "\n",
    "\n",
    "# Example Input\n",
    "tokens = [\"‡¶Ü‡¶Æ‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶§‡¶ø‡¶®\", \"‡¶ï‡ßã‡¶ü‡¶ø\", \"‡¶¶‡ßÅ‡¶á\", \"‡¶≤‡¶ï‡ßç‡¶∑\", \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\", \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\", \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\", \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \"‡¶Ü‡¶õ‡ßá\",\n",
    "          \",\", \"‡¶§‡ßã‡¶Æ‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶Ü‡¶õ‡ßá\", \"‡ß´‡ß¶‡ß¶‡ß¶\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \",\", \"‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ\", \"‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \"‡¶Ü‡¶õ‡ßá\", \"‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"]\n",
    "\n",
    "labels = [\"X\", \"O\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"O\",\n",
    "          \"O\", \"X\", \"O\", \"O\", \"Y\", \"Y\", \"O\", \"O\", \"X\", \"O\", \"O\", \"O\", \"Y\"]\n",
    "\n",
    "# Step 1: Pair `X` with `Y` words before conversion\n",
    "paired_results, bengali_number_map, multipliers = convert_bengali_text_to_number(tokens, labels)\n",
    "\n",
    "# Step 2: Convert `Y` words into numeric values\n",
    "final_results = convert_text_to_number(paired_results, bengali_number_map, multipliers)\n",
    "\n",
    "# Output the result\n",
    "print(\"üîπ **Before Conversion:**\")\n",
    "for key, data in paired_results.items():\n",
    "    print(f\"{key} -> {' '.join(data['words'])} {data['unit'] if data['unit'] else ''}\")\n",
    "\n",
    "print(\"\\nüîπ **After Conversion:**\")\n",
    "for key, value in final_results.items():\n",
    "    print(f\"{key} -> {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# Example text\n",
    "txt = \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶§‡¶ø‡¶® ‡¶ï‡ßã‡¶ü‡¶ø ‡¶¶‡ßÅ‡¶á ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡¶∂‡ßã ‡¶§‡ßç‡¶∞‡¶ø‡¶∂ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá ‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"\n",
    "\n",
    "# Call predict function (ensure predict_on_text returns [(token, label), ...])\n",
    "predictions = predict_on_text(model, txt, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Extract tokens and labels\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "def extract_xy_pairs(data):\n",
    "    pairs = {}\n",
    "    current_x = None\n",
    "    current_y = []\n",
    "    \n",
    "    for word, label in data:\n",
    "        if label == 'X':\n",
    "            if current_x and current_y:\n",
    "                pairs[current_x] = ' '.join(current_y)\n",
    "            current_x = word\n",
    "            current_y = []\n",
    "        elif label == 'Y':\n",
    "            if current_x:\n",
    "                current_y.append(word)\n",
    "    \n",
    "    if current_x and current_y:\n",
    "        pairs[current_x] = ' '.join(current_y)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Extract pairs\n",
    "xy_pairs = extract_xy_pairs(predictions)  # Directly pass predictions\n",
    "\n",
    "# Print the result\n",
    "for x, y in xy_pairs.items():\n",
    "    print(f\"{x} -> {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def extract_xy_pairs(data):\n",
    "    pairs = {}\n",
    "    units = {}\n",
    "    current_x = None\n",
    "    current_y = []\n",
    "    current_unit = None\n",
    "    unit_list = []\n",
    "    \n",
    "    # Define Bengali Number Words and Multipliers\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4, \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500, \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "    \n",
    "    for i, (word, label) in enumerate(data):\n",
    "        if label == 'X':\n",
    "            if current_x and current_y:\n",
    "                pairs[current_x] = ' '.join(current_y)\n",
    "                units[current_x] = current_unit\n",
    "                unit_list.append(current_unit if current_unit else \"null\")\n",
    "            current_x = word\n",
    "            current_y = []\n",
    "            current_unit = None\n",
    "        elif label == 'Y':\n",
    "            if current_x:\n",
    "                current_y.append(word)\n",
    "            if word not in bengali_number_map and word not in multipliers:\n",
    "                if not word.isdigit():  # Ensure it's not a number before marking it as a unit\n",
    "                    current_unit = word\n",
    "    \n",
    "    if current_x and current_y:\n",
    "        pairs[current_x] = ' '.join(current_y)\n",
    "        units[current_x] = current_unit\n",
    "        unit_list.append(current_unit if current_unit else \"null\")\n",
    "    \n",
    "    return pairs, units, unit_list\n",
    "\n",
    "def normalize_units(numeric_results, unit_list):\n",
    "    \"\"\"Converts all values to the smallest detected unit if necessary.\"\"\"\n",
    "    conversion_map = {\"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1, \"‡¶≤‡¶ï‡ßç‡¶∑\": 0.1}  # Convert billions to millions\n",
    "    \n",
    "    # Detect the most frequently occurring unit (smallest denominator)\n",
    "    valid_units = [unit for unit in unit_list if unit in conversion_map]\n",
    "    if not valid_units:\n",
    "        return numeric_results, unit_list  # No unit conversions needed\n",
    "    \n",
    "    target_unit = min(valid_units, key=lambda x: conversion_map[x])  # Find lowest unit\n",
    "    normalized_results = {}\n",
    "    new_unit_list = []\n",
    "    \n",
    "    for (key, value), unit in zip(numeric_results.items(), unit_list):\n",
    "        if unit in conversion_map:\n",
    "            factor = conversion_map[unit] / conversion_map[target_unit]\n",
    "            normalized_results[key] = value * factor\n",
    "            new_unit_list.append(target_unit)\n",
    "        else:\n",
    "            normalized_results[key] = value\n",
    "            new_unit_list.append(unit)\n",
    "    \n",
    "    return normalized_results, new_unit_list\n",
    "def parse_bengali_float(token: str):\n",
    "    \"\"\"\n",
    "    Translates Bengali digits in the token to ASCII digits,\n",
    "    then attempts to convert it to a float.\n",
    "    Returns None if unsuccessful.\n",
    "    \"\"\"\n",
    "    # Make a translation map for Bengali digits -> ASCII digits\n",
    "    translation_map = str.maketrans(\"‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ\", \"0123456789\")\n",
    "    \n",
    "    # Translate the token to ASCII form\n",
    "    ascii_token = token.translate(translation_map)\n",
    "    \n",
    "    try:\n",
    "        return float(ascii_token)  # If \"‡ß´.‡ß®\" => \"5.2\" => 5.2 (float)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_bengali_text_to_number(xy_pairs_dict, units):\n",
    "    \"\"\"Converts Bengali number words into numerical values and retains units.\"\"\"\n",
    "    bengali_number_map = {\n",
    "        '‡¶è‡¶ï': 1, '‡¶™‡ßç‡¶∞‡¶•‡¶Æ': 1, '‡¶™‡¶π‡ßá‡¶≤‡¶æ': 1,\n",
    "        '‡¶¶‡ßÅ‡¶á': 2, '‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º': 2, '‡¶¶‡ßã‡¶∏‡¶∞‡¶æ': 2,\n",
    "        '‡¶§‡¶ø‡¶®': 3, '‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º': 3,\n",
    "        '‡¶ö‡¶æ‡¶∞': 4, '‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶•': 4,\n",
    "        '‡¶™‡¶æ‡¶Å‡¶ö': 5, '‡¶™‡¶û‡ßç‡¶ö‡¶Æ': 5,\n",
    "        '‡¶õ‡¶Ø‡¶º': 6, '‡¶∑‡¶∑‡ßç‡¶†': 6,\n",
    "        '‡¶∏‡¶æ‡¶§': 7, '‡¶∏‡¶™‡ßç‡¶§‡¶Æ': 7,\n",
    "        '‡¶Ü‡¶ü': 8, '‡¶Ö‡¶∑‡ßç‡¶ü‡¶Æ': 8,\n",
    "        '‡¶®‡¶Ø‡¶º': 9, '‡¶®‡¶¨‡¶Æ': 9,\n",
    "        '‡¶¶‡¶∂': 10, '‡¶¶‡¶∂‡¶Æ': 10, \"‡¶è‡¶ó‡¶æ‡¶∞‡ßã\": 11, \"‡¶¨‡¶æ‡¶∞‡ßã\": 12, \"‡¶§‡ßá‡¶∞‡ßã\": 13, \"‡¶ö‡ßå‡¶¶‡ßç‡¶¶\": 14,\n",
    "        \"‡¶™‡¶®‡ßá‡¶∞‡ßã\": 15, \"‡¶∑‡ßã‡¶≤\": 16, \"‡¶∏‡¶§‡ßá‡¶∞‡ßã\": 17, \"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã\": 18, \"‡¶ä‡¶®‡¶ø‡¶∂\": 19,\n",
    "        \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶è‡¶ï‡ßÅ‡¶∂\": 21, \"‡¶¨‡¶æ‡¶á‡¶∂\": 22, \"‡¶§‡ßá‡¶á‡¶∂\": 23, \"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 24,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶ø‡¶∂\": 25, \"‡¶õ‡¶æ‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 26, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂\": 27, \"‡¶Ü‡¶†‡¶æ‡¶∂\": 28, \"‡¶ä‡¶®‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 29,\n",
    "        \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 31, \"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 32, \"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 33, \"‡¶ö‡ßå‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 34,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 35, \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 36, \"‡¶∏‡¶æ‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 37, \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 38, \"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 39,\n",
    "        \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶è‡¶ï‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 41, \"‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 42, \"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 43, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 44,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 45, \"‡¶õ‡¶ø‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 46, \"‡¶∏‡¶æ‡¶§‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 47, \"‡¶Ü‡¶ü‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 48, \"‡¶ä‡¶®‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 49,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®\": 51, \"‡¶¨‡¶æ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 52, \"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 53, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 54,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶®‡ßç‡¶®\": 55, \"‡¶õ‡¶æ‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 56, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡ßç‡¶®\": 57, \"‡¶Ü‡¶ü‡¶æ‡¶®‡ßç‡¶®\": 58, \"‡¶ä‡¶®‡¶∑‡¶æ‡¶ü\": 59,\n",
    "        \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 61, \"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 62, \"‡¶§‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 63, \"‡¶ö‡ßå‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 64,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 65, \"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 66, \"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 67, \"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 68, \"‡¶ä‡¶®‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 69,\n",
    "        \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶è‡¶ï‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 71, \"‡¶¨‡¶æ‡¶π‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 72, \"‡¶§‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 73, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 74,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 75, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 76, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 77, \"‡¶Ü‡¶ü‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 78, \"‡¶ä‡¶®‡¶Ü‡¶∂‡¶ø\": 79,\n",
    "        \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶è‡¶ï‡¶æ‡¶∂‡¶ø\": 81, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 82, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 83, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø\": 84,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶∂‡¶ø\": 85, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶∂‡¶ø\": 86, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂‡¶ø\": 87, \"‡¶Ü‡¶ü‡¶æ‡¶∂‡¶ø\": 88, \"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 89,\n",
    "        \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90, \"‡¶è‡¶ï‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 91, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 92, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 93, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 94,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 95, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 96, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 97, \"‡¶Ü‡¶ü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 98, \"‡¶®‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 99,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500,\n",
    "        \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000,\n",
    "        \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "    \n",
    "    numeric_dict = {}\n",
    "    for x_label, y_text in xy_pairs_dict.items():\n",
    "        tokens = y_text.split()\n",
    "        total_value = 0\n",
    "        part_sum = 0\n",
    "\n",
    "        for w in tokens:\n",
    "            # 1) Try integer\n",
    "            if w.isdigit():\n",
    "                part_sum += int(w)\n",
    "\n",
    "            # 2) Try dictionary\n",
    "            elif w in bengali_number_map:\n",
    "                part_sum += bengali_number_map[w]\n",
    "\n",
    "            # 3) Try multiplier\n",
    "            elif w in multipliers:\n",
    "                part_sum *= multipliers[w]\n",
    "                total_value += part_sum\n",
    "                part_sum = 0\n",
    "\n",
    "            # 4) Attempt to parse decimal (float)\n",
    "            else:\n",
    "                val = parse_bengali_float(w)\n",
    "                if val is not None:\n",
    "                    part_sum += val\n",
    "                else:\n",
    "                    # Possibly a unit or unknown word\n",
    "                    pass\n",
    "        \n",
    "        # Once we finish all tokens for this X, update total_value and the dictionary\n",
    "        total_value += part_sum\n",
    "        numeric_dict[x_label] = total_value\n",
    "\n",
    "    # Now we have processed **all** X-Y pairs\n",
    "    return numeric_dict\n",
    "\n",
    "    \n",
    "def get_dynamic_ticks(min_val: float, max_val: float):\n",
    "    \"\"\"\n",
    "    Generates y-axis tick values divided into 10 equal steps based on min and max values.\n",
    "    \"\"\"\n",
    "    if max_val <= min_val:\n",
    "        return [min_val]  # Edge case\n",
    "\n",
    "    step = (max_val - min_val) // 10\n",
    "    ticks = [round(min_val + i * step, 2) for i in range(11)]  # 11 ticks = 10 steps\n",
    "    return ticks\n",
    "\n",
    "def convert_to_bengali(num):\n",
    "    bengali_digits = '‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ'\n",
    "    return ''.join(bengali_digits[int(digit)] if digit.isdigit() else digit for digit in str(num))\n",
    "\n",
    "\n",
    "# # Extract pairs and units\n",
    "# xy_pairs, xy_units, unit_list = extract_xy_pairs(predictions)\n",
    "\n",
    "# # Convert Y values to numeric format\n",
    "# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n",
    "\n",
    "# # Normalize units\n",
    "# normalized_values, normalized_units = normalize_units(numeric_values, unit_list)\n",
    "\n",
    "# # Print results\n",
    "# for idx, (x, y) in enumerate(normalized_values.items(), 1):\n",
    "#     print(f\"{x} -> {y}\")\n",
    "\n",
    "# # Print unit mapping\n",
    "# for idx, unit in enumerate(normalized_units, 1):\n",
    "#     print(f\"unit{idx} -> {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert Y values to numeric format\n",
    "# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n",
    "\n",
    "# Convert values to Bengali numerals for display\n",
    "x_values = ['‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá', '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶™‡¶æ‡¶Å‡¶ö ‡¶¨‡¶õ‡¶∞‡ßá']\n",
    "y_values = [75,200]\n",
    "unit_list = ['‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®', '‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®']\n",
    "\n",
    "y_values_bengali = [convert_to_bengali(value) + \" \" + unit for value, unit in zip(y_values, unit_list)]\n",
    "\n",
    "max_val = max(y_values) if y_values else 0\n",
    "min_val = min(y_values) if y_values else 0\n",
    "print(min_val)\n",
    "print(max_val)\n",
    "\n",
    "y_tick_vals = get_dynamic_ticks(min_val, max_val)\n",
    "y_tick_text_bengali = [convert_to_bengali(val) for val in y_tick_vals]\n",
    "\n",
    "# Create the Plotly bar chart\n",
    "fig = go.Figure([go.Bar(\n",
    "    x=x_values,\n",
    "    y=y_values,\n",
    "    width=0.6,\n",
    "    text=y_values_bengali,\n",
    "    textfont=dict(size=15),\n",
    "    textposition='auto',\n",
    "    marker=dict(color='blue')\n",
    ")])\n",
    "\n",
    "# Update layout with Bengali tick labels and fonts\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickfont=dict(size=13),\n",
    "        title=dict(text=\"‡¶®‡¶æ‡¶Æ\", font=dict(size=13))  # X-axis label in Bengali\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickvals=y_tick_vals,\n",
    "        ticktext=y_tick_text_bengali,\n",
    "        tickfont=dict(size=15),\n",
    "        title=dict(text=\"‡¶Æ‡¶æ‡¶®\", font=dict(size=13))  # Y-axis label in Bengali\n",
    "    ),\n",
    "    width=600,\n",
    "    height=400,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)\n",
    ")\n",
    "\n",
    "# Show and save the plot\n",
    "fig.show()\n",
    "fig.write_html('generated_chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "(Moyna has 53 taka. Rohima has 159 taka. Mohona has 318 taka. Hena has 79.5 taka.)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6710212,
     "sourceId": 11124909,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7063017,
     "sourceId": 11299613,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 65478,
     "modelInstanceId": 47680,
     "sourceId": 56830,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
