{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:54:51.970081Z",
     "iopub.status.busy": "2025-04-06T18:54:51.969779Z",
     "iopub.status.idle": "2025-04-06T18:55:00.470209Z",
     "shell.execute_reply": "2025-04-06T18:55:00.469284Z",
     "shell.execute_reply.started": "2025-04-06T18:54:51.970048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex  # for grapheme cluster splitting\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:00.474234Z",
     "iopub.status.busy": "2025-04-06T18:55:00.473989Z",
     "iopub.status.idle": "2025-04-06T18:55:00.605939Z",
     "shell.execute_reply": "2025-04-06T18:55:00.604928Z",
     "shell.execute_reply.started": "2025-04-06T18:55:00.474215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>‡¶Ü‡¶≤‡¶Æ‡ßá‡¶∞ ‡¶ó‡ßç‡¶≤‡ßã‡¶ï‡ßã‡¶Æ‡¶æ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß‡ß® ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá‡¶∞ ‡¶≤‡ßá‡¶ú‡¶æ‡¶∞...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‡¶¨‡¶ø‡¶è‡¶´‡¶è-‡¶è‡¶∞ ‡ß®‡ß©‡¶§‡¶Æ ‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá, ‡ßß,‡ßØ‡ß©‡ß®‡¶ü‡¶ø ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶æ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‡¶ó‡¶§ ‡ß© ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ï‡ßç‡¶§‡¶ö‡¶æ‡¶™ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ ‡¶â‡¶®‡ßç‡¶®‡¶§‡¶ø ‡¶¶‡ßá‡¶ñ‡¶ø...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡ß´‡ß®‡ß¶‡ß¶ ‡¶ú‡¶®‡•§ ‡¶ï‡ßá‡ß® ‡¶∂‡ßÉ‡¶ô‡ßç‡¶ó ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá...</td>\n",
       "      <td>[1, 0, 0, 2, 2, 0, 1, 1, 0, 0, 2, 2, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶≤‡¶æ‡¶ö‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶≤‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡ß©‡ß´‡ß¶‡ß¶ ‡¶ï‡¶ø‡¶≤‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞...</td>\n",
       "      <td>[1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>‡¶¨‡¶æ‡¶®‡ßç‡¶¶‡¶∞‡¶¨‡¶æ‡¶®‡ßá ‡¶¨‡¶®‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡ßß‡ß®‡ß¶‡ß¶‡ß¶ ‡¶π‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶∞‡¶æ‡¶ô‡¶æ‡¶Æ‡¶æ...</td>\n",
       "      <td>[1, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>‡¶™‡¶ø‡¶∞‡¶æ‡¶Æ‡¶ø‡¶° ‡¶®‡¶ø‡¶∞‡ßç‡¶Æ‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡ß™‡ß´‡ß¶‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá‡•§ ‡¶∏‡ßç‡¶ü‡ßã‡¶®...</td>\n",
       "      <td>[1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶∏‡ßç‡¶™‡¶∞‡ßç‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡ßß‡ßØ‡ß´‡ß© ‡¶∏‡¶æ‡¶≤‡ßá‡•§...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    ‡¶Ü‡¶≤‡¶Æ‡ßá‡¶∞ ‡¶ó‡ßç‡¶≤‡ßã‡¶ï‡ßã‡¶Æ‡¶æ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ßß‡ß® ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá‡¶∞ ‡¶≤‡ßá‡¶ú‡¶æ‡¶∞...   \n",
       "1    ‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...   \n",
       "2    ‡¶∞‡¶æ‡¶ü‡¶æ‡¶ü‡ßá‡¶∞ ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ï‡ßÅ‡¶≤‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ö‡¶ø‡¶ï‡¶ø‡ßé‡¶∏‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß©...   \n",
       "3    ‡¶¨‡¶ø‡¶è‡¶´‡¶è-‡¶è‡¶∞ ‡ß®‡ß©‡¶§‡¶Æ ‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá, ‡ßß,‡ßØ‡ß©‡ß®‡¶ü‡¶ø ‡¶™‡¶∞‡¶ø‡¶¨‡¶æ‡¶∞‡¶ï‡ßá ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶æ...   \n",
       "4    ‡¶ó‡¶§ ‡ß© ‡¶∏‡¶™‡ßç‡¶§‡¶æ‡¶π‡ßá ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶∞‡¶ï‡ßç‡¶§‡¶ö‡¶æ‡¶™ ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ ‡¶â‡¶®‡ßç‡¶®‡¶§‡¶ø ‡¶¶‡ßá‡¶ñ‡¶ø...   \n",
       "..                                                 ...   \n",
       "295  ‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá‡¶® ‡ß´‡ß®‡ß¶‡ß¶ ‡¶ú‡¶®‡•§ ‡¶ï‡ßá‡ß® ‡¶∂‡ßÉ‡¶ô‡ßç‡¶ó ‡¶ú‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá...   \n",
       "296  ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡¶æ‡¶≤‡¶æ‡¶ö‡¶ø‡¶Ø‡¶º‡¶æ‡¶® ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶≤‡ßá‡¶∞ ‡¶¶‡ßà‡¶∞‡ßç‡¶ò‡ßç‡¶Ø ‡ß©‡ß´‡ß¶‡ß¶ ‡¶ï‡¶ø‡¶≤‡ßã‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞...   \n",
       "297  ‡¶¨‡¶æ‡¶®‡ßç‡¶¶‡¶∞‡¶¨‡¶æ‡¶®‡ßá ‡¶¨‡¶®‡¶≠‡ßÇ‡¶Æ‡¶ø‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡ßß‡ß®‡ß¶‡ß¶‡ß¶ ‡¶π‡ßá‡¶ï‡ßç‡¶ü‡¶∞‡•§ ‡¶∞‡¶æ‡¶ô‡¶æ‡¶Æ‡¶æ...   \n",
       "298  ‡¶™‡¶ø‡¶∞‡¶æ‡¶Æ‡¶ø‡¶° ‡¶®‡¶ø‡¶∞‡ßç‡¶Æ‡¶ø‡¶§ ‡¶π‡¶Ø‡¶º ‡¶™‡ßç‡¶∞‡¶æ‡¶Ø‡¶º ‡ß™‡ß´‡ß¶‡ß¶ ‡¶¨‡¶õ‡¶∞ ‡¶Ü‡¶ó‡ßá‡•§ ‡¶∏‡ßç‡¶ü‡ßã‡¶®...   \n",
       "299  ‡¶è‡¶≠‡¶æ‡¶∞‡ßá‡¶∏‡ßç‡¶ü ‡¶™‡ßç‡¶∞‡¶•‡¶Æ ‡¶∂‡ßÄ‡¶∞‡ßç‡¶∑ ‡¶∏‡ßç‡¶™‡¶∞‡ßç‡¶∂ ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º ‡ßß‡ßØ‡ß´‡ß© ‡¶∏‡¶æ‡¶≤‡ßá‡•§...   \n",
       "\n",
       "                                                 label  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...  \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3    [0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0, 2, 0, 0, ...  \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "295  [1, 0, 0, 2, 2, 0, 1, 1, 0, 0, 2, 2, 0, 1, 2, ...  \n",
       "296  [1, 1, 0, 2, 2, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, ...  \n",
       "297  [1, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, 2, 0, 1, ...  \n",
       "298  [1, 0, 0, 0, 2, 2, 0, 0, 1, 2, 2, 0, 0, 1, 2, ...  \n",
       "299  [1, 0, 0, 0, 0, 0, 2, 2, 0, 1, 2, 2, 0, 1, 2, ...  \n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(\"/kaggle/input/thesisdata1/final_train1.json\")\n",
    "td = pd.read_json(\"/kaggle/input/thesisdata1/final_testing1.json\")\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:00.607060Z",
     "iopub.status.busy": "2025-04-06T18:55:00.606756Z",
     "iopub.status.idle": "2025-04-06T18:55:00.611318Z",
     "shell.execute_reply": "2025-04-06T18:55:00.610391Z",
     "shell.execute_reply.started": "2025-04-06T18:55:00.607036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "raw_text       = df[\"text\"].tolist()\n",
    "raw_label      = df[\"label\"].tolist()\n",
    "raw_test_text  = td[\"text\"].tolist()\n",
    "raw_test_label = td[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:01.014212Z",
     "iopub.status.busy": "2025-04-06T18:55:01.013748Z",
     "iopub.status.idle": "2025-04-06T18:55:25.105832Z",
     "shell.execute_reply": "2025-04-06T18:55:25.105127Z",
     "shell.execute_reply.started": "2025-04-06T18:55:01.014181Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6589b12e30402b8ba092f5deaab722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b2f736479e4d609bf287f564a96ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/852 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3f906a7a624be38edb2f0c98ec3f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1838e2fc958b4c49aed570886361804f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb7d8c0cc2b4f9497ad3e31fa210471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_checkpoint = \"FacebookAI/xlm-roberta-large-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "bert_model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:25.107645Z",
     "iopub.status.busy": "2025-04-06T18:55:25.107163Z",
     "iopub.status.idle": "2025-04-06T18:55:25.111197Z",
     "shell.execute_reply": "2025-04-06T18:55:25.110554Z",
     "shell.execute_reply.started": "2025-04-06T18:55:25.107622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 256  # Max subword length\n",
    "num_labels = 4         # e.g. 0,1,2,3\n",
    "etag = 4               # Padding label\n",
    "ftag = 3               # Tag for sub-word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:50.798385Z",
     "iopub.status.busy": "2025-04-06T18:55:50.798057Z",
     "iopub.status.idle": "2025-04-06T18:55:50.805864Z",
     "shell.execute_reply": "2025-04-06T18:55:50.804877Z",
     "shell.execute_reply.started": "2025-04-06T18:55:50.798358Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Tokenization & BERT Input Formatting\n",
    "def bert_input(text_list, label_list):\n",
    "    input_ids_list, attention_mask_list, token_type_list, tag_list = [], [], [], []\n",
    "\n",
    "    for sentence, labels in zip(text_list, label_list):\n",
    "        tokens = custom_tokenize(sentence)  # ‚úÖ Use custom tokenizer\n",
    "        new_labels = []\n",
    "\n",
    "        tokenized_ids = []\n",
    "        for word, tag in zip(tokens, labels):\n",
    "            tokenized_word = tokenizer.tokenize(word)\n",
    "            token_ids = tokenizer.convert_tokens_to_ids(tokenized_word)\n",
    "\n",
    "            labels_expanded = [tag] + ([ftag] * (len(tokenized_word) - 1))  # Extend label for subwords\n",
    "\n",
    "            tokenized_ids.extend(token_ids)\n",
    "            new_labels.extend(labels_expanded)\n",
    "\n",
    "        # Add CLS and SEP tokens\n",
    "        tokenized_ids = [tokenizer.cls_token_id] + tokenized_ids + [tokenizer.sep_token_id]\n",
    "        new_labels = [0] + new_labels + [0]\n",
    "\n",
    "        # Create attention mask & token type ids\n",
    "        attention_mask = [1] * len(tokenized_ids)\n",
    "        token_type_ids = [0] * len(tokenized_ids)\n",
    "\n",
    "        # Padding\n",
    "        if len(tokenized_ids) < sequence_length:\n",
    "            pad_length = sequence_length - len(tokenized_ids)\n",
    "            tokenized_ids.extend([0] * pad_length)\n",
    "            new_labels.extend([etag] * pad_length)\n",
    "            attention_mask.extend([0] * pad_length)\n",
    "            token_type_ids.extend([0] * pad_length)\n",
    "\n",
    "        elif len(tokenized_ids) > sequence_length:\n",
    "            tokenized_ids = tokenized_ids[:sequence_length - 1] + [tokenizer.sep_token_id]\n",
    "            new_labels = new_labels[:sequence_length - 1] + [0]\n",
    "            attention_mask = attention_mask[:sequence_length]\n",
    "            token_type_ids = token_type_ids[:sequence_length]\n",
    "\n",
    "        input_ids_list.append(torch.tensor(tokenized_ids))\n",
    "        attention_mask_list.append(torch.tensor(attention_mask))\n",
    "        token_type_list.append(torch.tensor(token_type_ids))\n",
    "        tag_list.append(torch.tensor(new_labels))\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(input_ids_list),\n",
    "        \"attention_mask\": torch.stack(attention_mask_list),\n",
    "        \"token_type_ids\": torch.stack(token_type_list)\n",
    "    }, torch.stack(tag_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:54.095438Z",
     "iopub.status.busy": "2025-04-06T18:55:54.095052Z",
     "iopub.status.idle": "2025-04-06T18:55:54.101974Z",
     "shell.execute_reply": "2025-04-06T18:55:54.101116Z",
     "shell.execute_reply.started": "2025-04-06T18:55:54.095406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ‚úÖ Custom Dataset Class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.inputs, self.labels = bert_input(texts, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
    "            \"token_type_ids\": self.inputs[\"token_type_ids\"][idx],\n",
    "            \"labels\": self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:55:54.437354Z",
     "iopub.status.busy": "2025-04-06T18:55:54.437103Z",
     "iopub.status.idle": "2025-04-06T18:56:01.601180Z",
     "shell.execute_reply": "2025-04-06T18:56:01.600237Z",
     "shell.execute_reply.started": "2025-04-06T18:55:54.437334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "data = list(zip(raw_text, raw_label))\n",
    "tdata= list(zip(raw_test_text, raw_test_label))\n",
    "random.shuffle(data)\n",
    "random.shuffle(tdata)\n",
    "\n",
    "shuffled_text,  shuffled_label  = zip(*data)\n",
    "shuffled_ttext, shuffled_tlabel = zip(*tdata)\n",
    "\n",
    "split_1 = int(0.8 * len(shuffled_text))\n",
    "train_dataset = NERDataset(shuffled_text[:split_1],  shuffled_label[:split_1])\n",
    "valid_dataset = NERDataset(shuffled_text[split_1:], shuffled_label[split_1:])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BERT_LSTM_NER(nn.Module):\n",
    "    def __init__(self, bert_model, num_labels):\n",
    "        super(BERT_LSTM_NER, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        \n",
    "        # 1) Dynamically get the hidden size from the model config\n",
    "        hidden_dim = self.bert.config.hidden_size  # 1024 for xlm-roberta-large\n",
    "\n",
    "        # 2) Initialize LSTM with the correct input size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=512,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.3  \n",
    "        )\n",
    "\n",
    "        # If it's bidirectional with hidden_size=512 => output dim = 1024\n",
    "        self.fc = nn.Linear(512 * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # with torch.no_grad():\n",
    "        bert_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        lstm_output, _ = self.lstm(bert_output.last_hidden_state)\n",
    "        # lstm_output is [batch_size, seq_len, 1024] because hidden_size=512 + bidirectional\n",
    "        \n",
    "        logits = self.fc(lstm_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:56:01.602614Z",
     "iopub.status.busy": "2025-04-06T18:56:01.602328Z",
     "iopub.status.idle": "2025-04-06T18:56:02.531891Z",
     "shell.execute_reply": "2025-04-06T18:56:02.530970Z",
     "shell.execute_reply.started": "2025-04-06T18:56:01.602592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset = NERDataset(shuffled_ttext, shuffled_tlabel)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:56:02.533682Z",
     "iopub.status.busy": "2025-04-06T18:56:02.533370Z",
     "iopub.status.idle": "2025-04-06T18:56:03.713647Z",
     "shell.execute_reply": "2025-04-06T18:56:03.712734Z",
     "shell.execute_reply.started": "2025-04-06T18:56:02.533649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT_LSTM_NER(bert_model, num_labels).to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "optimizer = optim.AdamW([\n",
    "    {\"params\": model.bert.parameters(), \"lr\": 5e-6},  # Slower fine-tuning for BERT\n",
    "    {\"params\": model.lstm.parameters(), \"lr\": 2e-4},  # Faster training for LSTM\n",
    "    {\"params\": model.fc.parameters(), \"lr\": 2e-4}     # Fully connected layer also gets higher LR\n",
    "])\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=etag)  # Ignore padding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:56:04.007174Z",
     "iopub.status.busy": "2025-04-06T18:56:04.006890Z",
     "iopub.status.idle": "2025-04-06T18:56:04.689364Z",
     "shell.execute_reply": "2025-04-06T18:56:04.688694Z",
     "shell.execute_reply.started": "2025-04-06T18:56:04.007152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics import Precision, Recall, F1Score, Accuracy\n",
    "\n",
    "def compute_metrics(preds, labels, ignore_index=etag):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, F1-score, and accuracy.\n",
    "    Ignores padding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape to 1D\n",
    "    preds = preds.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # Ignore padding labels\n",
    "    mask = labels != ignore_index  \n",
    "    preds, labels = preds[mask], labels[mask]  # Apply mask\n",
    "\n",
    "    p = precision(preds, labels)\n",
    "    r = recall(preds, labels)\n",
    "    f = f1(preds, labels)\n",
    "    a = accuracy(preds, labels)\n",
    "\n",
    "    return p.item(), r.item(), f.item(), a.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:56:04.690919Z",
     "iopub.status.busy": "2025-04-06T18:56:04.690276Z",
     "iopub.status.idle": "2025-04-06T18:56:04.696761Z",
     "shell.execute_reply": "2025-04-06T18:56:04.695984Z",
     "shell.execute_reply.started": "2025-04-06T18:56:04.690891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model on validation/test set and returns loss & metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, token_type_ids, labels = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"token_type_ids\"].to(device),\n",
    "                batch[\"labels\"].to(device),\n",
    "            )\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions & labels\n",
    "            all_preds.append(torch.argmax(logits, dim=-1).cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    all_preds, all_labels = torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "    precision, recall, f1, accuracy = compute_metrics(all_preds, all_labels)\n",
    "    return avg_loss, precision, recall, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T18:56:12.272262Z",
     "iopub.status.busy": "2025-04-06T18:56:12.271892Z",
     "iopub.status.idle": "2025-04-06T18:56:12.280602Z",
     "shell.execute_reply": "2025-04-06T18:56:12.279594Z",
     "shell.execute_reply.started": "2025-04-06T18:56:12.272238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, epochs=10, patience=3):\n",
    "    \"\"\"\n",
    "    Trains the model with early stopping and prints training & validation metrics per epoch.\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")  # Track lowest validation loss\n",
    "    patience_counter = 0  # Counter for early stopping\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, token_type_ids, labels = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"token_type_ids\"].to(device),\n",
    "                batch[\"labels\"].to(device),\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask, token_type_ids)\n",
    "            loss = criterion(logits.view(-1, num_labels), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions & labels for computing metrics\n",
    "            all_preds.append(torch.argmax(logits, dim=-1).cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "        # Compute training loss & metrics\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        all_preds, all_labels = torch.cat(all_preds), torch.cat(all_labels)\n",
    "        train_precision, train_recall, train_f1, train_acc = compute_metrics(all_preds, all_labels)\n",
    "\n",
    "        # Compute validation loss & metrics\n",
    "        val_loss, val_precision, val_recall, val_f1, val_acc = evaluate_model(model, valid_loader)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f} | Precision: {train_precision:.4f} | Recall: {train_recall:.4f} | F1: {train_f1:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Valid Loss: {val_loss:.4f} | Precision: {val_precision:.4f} | Recall: {val_recall:.4f} | F1: {val_f1:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # ‚úÖ Early Stopping Check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss  # Update best loss\n",
    "            patience_counter = 0  # Reset patience\n",
    "            torch.save(model.state_dict(), \"best_model2.pt\")  # Save best model\n",
    "            print(\"‚úÖ Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience\n",
    "            print(f\"‚è≥ Patience Counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:  # Stop training if patience exceeded\n",
    "            print(\"\\nüõë Early stopping triggered. Restoring best model...\")\n",
    "            model.load_state_dict(torch.load(\"best_model2.pt\"))  # Load best model\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T19:03:11.133892Z",
     "iopub.status.busy": "2025-04-06T19:03:11.133521Z",
     "iopub.status.idle": "2025-04-06T19:36:03.484553Z",
     "shell.execute_reply": "2025-04-06T19:36:03.483603Z",
     "shell.execute_reply.started": "2025-04-06T19:03:11.133791Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10:\n",
      "Train Loss: 0.0707 | Precision: 0.9595 | Recall: 0.9606 | F1: 0.9600 | Accuracy: 0.9782\n",
      "Valid Loss: 0.0909 | Precision: 0.9424 | Recall: 0.9655 | F1: 0.9533 | Accuracy: 0.9739\n",
      "‚úÖ Model saved!\n",
      "\n",
      "Epoch 2/10:\n",
      "Train Loss: 0.0590 | Precision: 0.9685 | Recall: 0.9700 | F1: 0.9692 | Accuracy: 0.9831\n",
      "Valid Loss: 0.0767 | Precision: 0.9586 | Recall: 0.9638 | F1: 0.9611 | Accuracy: 0.9788\n",
      "‚úÖ Model saved!\n",
      "\n",
      "Epoch 3/10:\n",
      "Train Loss: 0.0494 | Precision: 0.9736 | Recall: 0.9754 | F1: 0.9745 | Accuracy: 0.9860\n",
      "Valid Loss: 0.0746 | Precision: 0.9601 | Recall: 0.9623 | F1: 0.9611 | Accuracy: 0.9789\n",
      "‚úÖ Model saved!\n",
      "\n",
      "Epoch 4/10:\n",
      "Train Loss: 0.0434 | Precision: 0.9771 | Recall: 0.9791 | F1: 0.9781 | Accuracy: 0.9879\n",
      "Valid Loss: 0.0756 | Precision: 0.9613 | Recall: 0.9630 | F1: 0.9621 | Accuracy: 0.9796\n",
      "‚è≥ Patience Counter: 1/3\n",
      "\n",
      "Epoch 5/10:\n",
      "Train Loss: 0.0365 | Precision: 0.9809 | Recall: 0.9825 | F1: 0.9817 | Accuracy: 0.9899\n",
      "Valid Loss: 0.0930 | Precision: 0.9471 | Recall: 0.9695 | F1: 0.9576 | Accuracy: 0.9764\n",
      "‚è≥ Patience Counter: 2/3\n",
      "\n",
      "Epoch 6/10:\n",
      "Train Loss: 0.0337 | Precision: 0.9820 | Recall: 0.9841 | F1: 0.9830 | Accuracy: 0.9907\n",
      "Valid Loss: 0.0804 | Precision: 0.9567 | Recall: 0.9656 | F1: 0.9610 | Accuracy: 0.9788\n",
      "‚è≥ Patience Counter: 3/3\n",
      "\n",
      "üõë Early stopping triggered. Restoring best model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-44c7f01ea323>:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model2.pt\"))  # Load best model\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T19:36:36.182615Z",
     "iopub.status.busy": "2025-04-06T19:36:36.182270Z",
     "iopub.status.idle": "2025-04-06T19:36:45.876514Z",
     "shell.execute_reply": "2025-04-06T19:36:45.875618Z",
     "shell.execute_reply.started": "2025-04-06T19:36:36.182586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Evaluation:\n",
      "  Loss: 0.0491\n",
      "  Precision: 0.9724\n",
      "  Recall: 0.9675\n",
      "  F1-Score: 0.9698\n",
      "  Accuracy: 0.9869\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_precision, test_recall, test_f1, test_accuracy = evaluate_model(model, test_loader)\n",
    "\n",
    "print(\"\\nTest Evaluation:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")       # ‚úÖ Now it won't crash                                                                                                                    \n",
    "print(f\"  Precision: {test_precision:.4f}\")\n",
    "print(f\"  Recall: {test_recall:.4f}\")\n",
    "print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "print(f\"  Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:57:51.143842Z",
     "iopub.status.busy": "2025-04-06T13:57:51.143547Z",
     "iopub.status.idle": "2025-04-06T13:57:51.147899Z",
     "shell.execute_reply": "2025-04-06T13:57:51.146883Z",
     "shell.execute_reply.started": "2025-04-06T13:57:51.143815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_id_to_str = {\n",
    "    0: \"O\",\n",
    "    1: \"X\",\n",
    "    2: \"Y\",\n",
    "    # If you had more, add them here\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:57:55.772617Z",
     "iopub.status.busy": "2025-04-06T13:57:55.772297Z",
     "iopub.status.idle": "2025-04-06T13:57:55.785366Z",
     "shell.execute_reply": "2025-04-06T13:57:55.784475Z",
     "shell.execute_reply.started": "2025-04-06T13:57:55.772586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_on_text_charbert(model, text, tokenizer, char_tokenizer, max_seq_len=128):\n",
    "    \"\"\"\n",
    "    Predict the NER tags for a single input text using CharBERT-style approach \n",
    "    (BERT subwords + grapheme-based character embeddings).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained CharBERT_NER model (with .bert + .char_encoder + .lstm).\n",
    "        text: Raw Bengali text string.\n",
    "        tokenizer: HuggingFace tokenizer (AutoTokenizer) for BERT subwords.\n",
    "        char_tokenizer: Keras tokenizer trained on grapheme sequences.\n",
    "        max_seq_len: Max sequence length used in training/fine-tuning.\n",
    "    \n",
    "    Returns:\n",
    "        List of (word, predicted_label) tuples.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # 1Ô∏è‚É£ Custom word-level splitting (punct, special chars)\n",
    "    original_tokens = custom_tokenize(text)\n",
    "\n",
    "    # 2Ô∏è‚É£ BERT subword tokenization + mapping\n",
    "    bert_tokens = []\n",
    "    token_mapping = []  # Maps subword indices back to original_tokens indices\n",
    "    for idx, tok in enumerate(original_tokens):\n",
    "        subwords = tokenizer.tokenize(tok)\n",
    "        bert_tokens.extend(subwords)\n",
    "        token_mapping.extend([idx] * len(subwords))\n",
    "\n",
    "    # 3Ô∏è‚É£ Convert to BERT token IDs (CLS + subwords + SEP)\n",
    "    input_ids = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(bert_tokens) + [tokenizer.sep_token_id]\n",
    "    token_mapping = [-1] + token_mapping + [-1]  # -1 for CLS and SEP\n",
    "\n",
    "    # 4Ô∏è‚É£ Build Char/Grapheme input\n",
    "    #    We only do one grapheme sequence per *original token*, then replicate for subwords.\n",
    "    char_seq = []\n",
    "    for word in original_tokens:\n",
    "        # Split into graphemes\n",
    "        g_list = grapheme_split(word)\n",
    "        # Join with space so Keras char_tokenizer can parse them as separate tokens\n",
    "        grapheme_text = \" \".join(g_list)\n",
    "        grapheme_ids = char_tokenizer.texts_to_sequences([grapheme_text])[0] if grapheme_text else []\n",
    "        # Pad/truncate\n",
    "        grapheme_ids = grapheme_ids[:MAX_CHAR_LEN]\n",
    "        grapheme_ids += [0] * (MAX_CHAR_LEN - len(grapheme_ids))\n",
    "        # We'll store, replicate later\n",
    "        char_seq.append(grapheme_ids)\n",
    "\n",
    "    #    Replicate char_seq for subwords + add CLS/SEP placeholders\n",
    "    replicated_char_seq = [[0]*MAX_CHAR_LEN]  # CLS\n",
    "    idx_char = 0\n",
    "    for idx, tok in enumerate(original_tokens):\n",
    "        subword_count = sum([1 for m in token_mapping if m == idx]) \n",
    "        # replicate the same grapheme vector subword_count times\n",
    "        for _ in range(subword_count):\n",
    "            replicated_char_seq.append(char_seq[idx_char])\n",
    "        idx_char += 1\n",
    "    replicated_char_seq.append([0]*MAX_CHAR_LEN)  # SEP\n",
    "\n",
    "    # 5Ô∏è‚É£ Now we do sequence-level padding/truncation\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    if len(input_ids) > max_seq_len:\n",
    "        input_ids = input_ids[:max_seq_len]\n",
    "        attention_mask = attention_mask[:max_seq_len]\n",
    "        token_mapping = token_mapping[:max_seq_len]\n",
    "        replicated_char_seq = replicated_char_seq[:max_seq_len]\n",
    "    else:\n",
    "        pad_len = max_seq_len - len(input_ids)\n",
    "        input_ids += [0]*pad_len\n",
    "        attention_mask += [0]*pad_len\n",
    "        token_mapping += [-1]*pad_len\n",
    "        # Pad char_seq\n",
    "        for _ in range(pad_len):\n",
    "            replicated_char_seq.append([0]*MAX_CHAR_LEN)\n",
    "\n",
    "    # 6Ô∏è‚É£ Convert to tensors\n",
    "    input_ids      = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)\n",
    "    token_type_ids = torch.zeros_like(input_ids).to(device)  # typical for single-sentence inputs\n",
    "    char_input     = torch.tensor([replicated_char_seq], dtype=torch.long).to(device)\n",
    "\n",
    "    # 7Ô∏è‚É£ Forward pass with no grad\n",
    "    with torch.no_grad():\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            char_input=char_input\n",
    "        )\n",
    "        preds = torch.argmax(logits, dim=-1).squeeze(0)  # shape: (seq_len,)\n",
    "\n",
    "    # 8Ô∏è‚É£ Map subword predictions back to original tokens\n",
    "    token_level_preds = {}\n",
    "    for i, label_id in enumerate(preds.cpu().numpy()):\n",
    "        orig_idx = token_mapping[i]\n",
    "        if orig_idx == -1:\n",
    "            continue  # skip CLS, SEP, padding\n",
    "        # first subword of a token can represent the entire token\n",
    "        # store only the first subword's predicted label\n",
    "        if orig_idx not in token_level_preds:\n",
    "            token_level_preds[orig_idx] = label_id\n",
    "\n",
    "    # 9Ô∏è‚É£ Generate final output\n",
    "    results = []\n",
    "    for i, word in enumerate(original_tokens):\n",
    "        label_id = token_level_preds.get(i, 0)  # default label if missing\n",
    "        label_str = label_id_to_str[label_id]   # map ID ‚Üí \"O\", \"X\", \"Y\", etc.\n",
    "        results.append((word, label_str))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T13:58:05.459561Z",
     "iopub.status.busy": "2025-04-06T13:58:05.459221Z",
     "iopub.status.idle": "2025-04-06T13:58:05.519070Z",
     "shell.execute_reply": "2025-04-06T13:58:05.518153Z",
     "shell.execute_reply.started": "2025-04-06T13:58:05.459532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡ß®‡ß¶‡ß®‡ß¶\tX\n",
      "‡¶∏‡¶æ‡¶≤‡ßá\tO\n",
      "‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø\tX\n",
      "‡¶õ‡¶ø‡¶≤\tO\n",
      "‡ß´.‡ß®\tY\n",
      "%\tY\n",
      ",\tO\n",
      "‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ\tO\n",
      "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø\tX\n",
      "‡¶õ‡¶ø‡¶≤\tO\n",
      "‡ß©.‡ßß\tY\n",
      "%\tY\n",
      "‡•§\tO\n",
      "‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá\tO\n",
      ",\tO\n",
      "‡ß®‡ß¶‡ß®‡ßß\tX\n",
      "‡¶∏‡¶æ‡¶≤‡ßá\tO\n",
      "‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞\tO\n",
      "‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø\tX\n",
      "‡¶¨‡ßá‡ßú‡ßá\tO\n",
      "‡ß¨.‡ß≠\tY\n",
      "%\tY\n",
      "‡¶π‡¶≤‡ßã\tO\n",
      ",\tO\n",
      "‡¶Ü‡¶∞\tO\n",
      "‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø\tX\n",
      "‡¶ï‡¶Æ‡ßá\tO\n",
      "‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã\tO\n",
      "‡ß®.‡ßß\tY\n",
      "%\tY\n",
      "‡•§\tO\n"
     ]
    }
   ],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶∞‡¶ø‡¶®‡¶æ ‡¶ì ‡¶Æ‡¶ø‡¶û‡¶æ ‡¶Ø‡¶•‡¶æ‡¶ï‡ßç‡¶∞‡¶Æ‡ßá ‡¶ó‡¶£‡¶ø‡¶§‡ßá ‡ß´‡ß¶ ‡¶ì ‡ß¨‡ß´ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶™‡ßá‡ßü‡ßá‡¶õ‡ßá. ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∂‡¶ø‡¶Æ‡¶æ ‡¶™‡ßá‡ßü‡ßá‡¶õ‡ßá ‡ß™‡ß¶ ‡¶®‡¶Æ‡ßç‡¶¨‡¶∞\"\n",
    "new_text=\"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã ‡ß®.‡ßß%‡•§\"\n",
    "# Call predict\n",
    "predictions = predict_on_text_charbert(model, new_text, tokenizer, char_tokenizer, max_seq_len=128)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "from llama_cpp import Llama\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# ‚úÖ Load the LLaMA 3 model from GGUF format\n",
    "MODEL_PATH = \"/kaggle/input/llma3_8b/pytorch/llama3_8b/1/Meta-Llama-3-8B-Instruct-Q6_K.gguf\"\n",
    "llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_gpu_layers=-1)  # Use GPU acceleration\n",
    "\n",
    "def simplify_text(text):\n",
    "    \"\"\"Uses LLaMA-3 GGUF model to simplify numerical text intelligently.\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡ßÉ‡¶§‡ßç‡¶∞‡¶ø‡¶Æ ‡¶¨‡ßÅ‡¶¶‡ßç‡¶ß‡¶ø‡¶Æ‡¶§‡ßç‡¶§‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤, ‡¶Ø‡¶æ‡¶ï‡ßá ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶≠‡¶æ‡¶∑‡¶æ‡¶Ø‡¶º ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶∏‡¶Æ‡ßÉ‡¶¶‡ßç‡¶ß ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶æ‡¶†‡ßç‡¶Ø ‡¶¶‡ßá‡¶ì‡¶Ø‡¶º‡¶æ ‡¶π‡¶¨‡ßá‡•§\n",
    "    ‡¶§‡ßã‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ì‡¶á ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶æ‡¶∏‡¶ô‡ßç‡¶ó‡¶ø‡¶ï ‡¶Ö‡¶∞‡ßç‡¶• ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶è‡¶¨‡¶Ç ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶¨‡¶ø‡¶¨‡ßÉ‡¶§‡¶ø‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶∏‡ßÅ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ‡¶Ø‡¶º ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶™‡¶æ‡¶†‡ßç‡¶Ø‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡ßÄ‡¶ï‡¶∞‡¶£ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§\n",
    "    **‡¶®‡¶ø‡¶Ø‡¶º‡¶Æ‡¶æ‡¶¨‡¶≤‡ßÄ:**\n",
    "    ‡ßß. ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡ßã‡¶∞‡ßã ‡¶®‡¶æ, ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶∏‡¶π‡¶ú ‡¶ï‡¶∞‡ßã‡•§\n",
    "    ‡ß®. ‡¶ï‡ßã‡¶®‡ßã ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶§‡¶•‡ßç‡¶Ø ‡¶Ø‡ßã‡¶ó ‡¶ï‡ßã‡¶∞‡ßã ‡¶®‡¶æ‡•§\n",
    "    ‡ß©. ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶∏‡¶†‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶®‡ßç‡¶® ‡¶ï‡¶∞‡ßã‡•§\n",
    "    Input: ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶õ‡ßá ‡ß™‡ß¶‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶∂‡¶´‡¶ø‡¶ï‡ßá‡¶∞ ‡¶§‡¶æ‡¶∞ ‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶® ‡¶è‡¶¨‡¶Ç ‡¶ï‡¶∞‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ü‡¶õ‡ßá ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶ß‡ßá‡¶ï‡•§ \n",
    "    Output: ‡¶∞‡¶π‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß™‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶∏‡¶´‡¶ø‡¶ï‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ßÆ‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶∞‡¶ø‡¶Æ‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß®‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\n",
    "    \n",
    "    Input: ‡¶™‡ßç‡¶∞‡¶æ‡¶£ ‡¶¨‡ßá‡¶≠‡¶æ‡¶∞‡ßá‡¶ú ‡¶è‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá ‡ß®.‡ß´% ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶Ø‡¶º ‡¶ï‡¶∞‡ßá‡¶õ‡ßá, ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ü‡¶Ø‡¶º ‡¶õ‡¶ø‡¶≤ ‡ß®,‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡ßá ‡¶¨‡¶∞‡ßç‡¶§‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶æ‡¶∏‡ßá‡¶∞ ‡¶ö‡ßá‡¶Ø‡¶º‡ßá‡¶ì ‡¶¶‡ßç‡¶¨‡¶ø‡¶ó‡ßÅ‡¶£ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶Ø‡¶º ‡¶π‡¶¨‡ßá‡•§\n",
    "    Output: ‡¶™‡ßç‡¶∞‡¶æ‡¶£ ‡¶¨‡ßá‡¶≠‡¶æ‡¶∞‡ßá‡¶ú ‡¶™‡ßÇ‡¶∞‡ßç‡¶¨‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ü‡ßü ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡ß®,‡ß´‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶è‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ü‡ßü ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡ß®,‡ß´‡ß¨,‡ß®‡ß´‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡ßÄ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶Ö‡¶®‡ßÅ‡¶Æ‡¶æ‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá ‡¶Ø‡ßá ‡¶Ü‡ßü ‡¶π‡¶¨‡ßá ‡ß´,‡ß¶‡ß¶,‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\n",
    "    \n",
    "    Input: ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ‡¶Ø‡¶º ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶£‡ßç‡¶° ‡¶ó‡¶∞‡¶Æ ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá, ‡¶Ø‡¶æ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá‡¶∞ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Ø‡¶º ‡ß®.‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏ ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶õ‡¶ø‡¶≤ ‡ß©‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§\n",
    "    Output: ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶Ü‡¶Æ‡ßá‡¶∞‡¶ø‡¶ï‡¶æ‡¶Ø‡¶º ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ö‡¶£‡ßç‡¶° ‡¶ó‡¶∞‡¶Æ ‡¶™‡¶°‡¶º‡ßá‡¶õ‡ßá‡•§ ‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶õ‡¶ø‡¶≤ ‡ß©‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§ ‡¶è‡¶á ‡¶¨‡¶õ‡¶∞ ‡¶§‡¶æ‡¶™‡¶Æ‡¶æ‡¶§‡ßç‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡ß©‡ß≠.‡ß´ ‡¶°‡¶ø‡¶ó‡ßç‡¶∞‡¶ø ‡¶∏‡ßá‡¶≤‡¶∏‡¶ø‡¶Ø‡¶º‡¶æ‡¶∏‡•§\n",
    "\n",
    "    ---\n",
    "    **‡¶á‡¶®‡¶™‡ßÅ‡¶ü:** \"{text}\"  \n",
    "    **‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü:**  \n",
    "    \"\"\"\n",
    "\n",
    "    # ‚úÖ Generate response using LLaMA-3 GGUF\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=512,  # More room for generated text\n",
    "        temperature=0.1,  # Less randomness for accuracy\n",
    "        top_p=0.9,  # Balanced output\n",
    "        stop=['\\n']  # Don't cut off early\n",
    "    )\n",
    "\n",
    "    # ‚úÖ Extract and clean output\n",
    "    simplified_text = output[\"choices\"][0][\"text\"].strip().replace(\"\\n\", \" \")\n",
    "\n",
    "    # ‚úÖ Ensure output is meaningful\n",
    "    if not simplified_text or len(simplified_text.split()) < 3:\n",
    "        return \"Error: Model did not generate a valid response.\"\n",
    "\n",
    "    return simplified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_test_sentences(text, max_words=150):\n",
    "    \"\"\"\n",
    "    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n",
    "    Ensures words are not split in the middle.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = []\n",
    "    \n",
    "    # ‚úÖ Process in chunks of `max_words`\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n",
    "        chunk_text = \" \".join(chunk_words)  # Convert back to text\n",
    "        chunks.append(chunk_text.strip())\n",
    "    print([chunk for chunk in chunks])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def convert_to_test_sentences(text, max_words=150):\n",
    "    \"\"\"\n",
    "    Splits a Bengali paragraph into chunks, each with at most `max_words`.\n",
    "    Ensures words are not split in the middle.\n",
    "    \"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = []\n",
    "    \n",
    "    # ‚úÖ Process in chunks of `max_words`\n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk_words = words[i:i+max_words]  # Extract a chunk of max_words\n",
    "        chunk_text = \" \".join(chunk_words)  # Convert back to text\n",
    "        chunks.append(chunk_text.strip())\n",
    "    print([chunk for chunk in chunks])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def simplify_large_text(text):\n",
    "    \"\"\"\n",
    "    Breaks large text into token-based chunks, processes each with LLaMA, and merges results.\n",
    "    \"\"\"\n",
    "    chunks = convert_to_test_sentences(text)  # Token-based chunking\n",
    "    simplified_chunks = [simplify_text(chunk) for chunk in chunks]  # Process each chunk\n",
    "    \n",
    "    return \" \".join(simplified_chunks)  # Merge all processed chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text=\"\"\"‡¶∏‡¶æ‡ßü‡¶∞‡¶æ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶Æ‡¶æ‡¶∏‡ßá ‡ßß‡ß¶‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶™‡¶æ‡ßü ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ ‡¶•‡ßá‡¶ï‡ßá‡•§ ‡¶Ü‡¶ó‡¶∏‡ßç‡¶ü ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶§‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶Æ‡¶æ‡¶∏ ‡¶•‡ßá‡¶ï‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶ú‡ßÅ‡¶≤‡¶æ‡¶á ‡¶è‡¶∞ ‡¶∏‡¶Æ‡¶æ‡¶® ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü‡•§ ‡¶Ö‡¶ï‡ßç‡¶ü‡ßã‡¶¨‡¶∞‡ßá ‡¶≤‡¶æ‡¶≠‡ßá‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶∏‡ßá‡¶™‡ßç‡¶ü‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶∞‡ßã ‡ß©‡ß¶‡ß¶‡ß¶ ‡¶ï‡¶Æ‡ßá ‡ß≠‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡ßü‡•§ ‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶≠‡ßá‡¶∞ ‡¶Æ‡ßÅ‡¶ñ ‡¶¶‡ßá‡¶ñ‡ßá ‡¶∏‡¶æ‡ßü‡¶∞‡¶æ‡•§ ‡¶§‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶∂‡ßá‡¶∑‡¶¶‡¶ø‡¶ï‡ßá ‡¶®‡¶≠‡ßá‡¶Æ‡ßç‡¶¨‡¶∞ ‡¶Æ‡¶æ‡¶∏‡ßá ‡¶≤‡¶æ‡¶≠ ‡¶π‡ßü ‡ß®‡ß¶‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ‡•§\"\"\"\n",
    "# ‚úÖ Process and get final simplified output\n",
    "final_simplified_output = simplify_large_text(text)\n",
    "\n",
    "# ‚úÖ Print the final merged output\n",
    "print(\"Final Simplified Output:\", final_simplified_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = final_simplified_output\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶¶‡¶æ‡¶Å‡ßú‡¶æ‡¶≤‡ßã ‡ß®.‡ßß%‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n",
    "# Run validation\n",
    "valid_pairs = validate_xy_pairs(tokens, labels)\n",
    "\n",
    "# Output valid pairs\n",
    "print(\"‚úÖ Valid X-Y Pairs:\", valid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß´.‡ß®%, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶õ‡¶ø‡¶≤ ‡ß©.‡ßß%‡•§ ‡¶Ö‡¶®‡ßç‡¶Ø‡¶¶‡¶ø‡¶ï‡ßá, ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ú‡¶ø‡¶°‡¶ø‡¶™‡¶ø ‡¶¨‡ßá‡ßú‡ßá ‡ß¨.‡ß≠% ‡¶π‡¶≤‡ßã, ‡¶Ü‡¶∞ ‡¶Æ‡ßÅ‡¶¶‡ßç‡¶∞‡¶æ‡¶∏‡ßç‡¶´‡ßÄ‡¶§‡¶ø ‡¶ï‡¶Æ‡ßá ‡¶ó‡ßá‡¶≤‡ßã‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶ï‡ßã‡¶≠‡¶ø‡¶° ‡¶Æ‡¶π‡¶æ‡¶Æ‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶¨‡¶õ‡¶∞ ‡ß®‡ß¶‡ß®‡ß¶ ‡¶∏‡¶æ‡¶≤‡ßá ‡¶õ‡ßü ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡ßß‡ß©‡ß¶ ‡¶ú‡¶® ‡¶Ü‡¶ï‡ßç‡¶∞‡¶æ‡¶®‡ßç‡¶§ ‡¶π‡ßü ‡¶Æ‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶∞‡¶ø‡ßü‡¶æ‡ßü‡•§ ‡¶™‡¶∞‡ßá‡¶∞ ‡¶¨‡¶õ‡¶∞ ‡ß®‡ß¶‡ß®‡ßß ‡¶∏‡¶æ‡¶≤‡ßá ‡¶è ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ ‡¶õ‡¶ø‡¶≤¬†‡¶∏‡¶æ‡¶§¬†‡¶π‡¶æ‡¶ú‡¶æ‡¶∞¬†‡ß®‡ßØ‡ß™‡•§\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppose you have a new text snippet\n",
    "new_text = \"‡¶∞‡¶ø‡¶Æ‡¶æ ‡¶¨‡¶æ‡¶∞‡ßç‡¶∑‡¶ø‡¶ï ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡ßü ‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡ßü ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶∏‡ßÄ‡¶Æ‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶§‡ßÉ‡¶§‡ßÄ‡ßü, ‡¶´‡¶ú‡¶≤‡¶ø ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶™‡¶û‡ßç‡¶ö‡¶Æ\"\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import re\n",
    "txt=\"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶§‡¶ø‡¶® ‡¶ï‡ßã‡¶ü‡¶ø ‡¶¶‡ßÅ‡¶á ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡¶∂‡ßã ‡¶§‡ßç‡¶∞‡¶ø‡¶∂ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá ‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"# Suppose you have a new text snippet\n",
    "new_text = txt\n",
    "\n",
    "# Call predict\n",
    "predictions = predict_on_text(model, new_text, tokenizer, max_seq_len=100)\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "\n",
    "# Print token-label pairs\n",
    "for token, label in predictions:\n",
    "    print(f\"{token}\\t{label}\")\n",
    "\n",
    "def convert_bengali_text_to_number(tokens, labels):\n",
    "    \"\"\"\n",
    "    Converts Bengali number words to their numerical values based on the 'Y' labels.\n",
    "    \n",
    "    Args:\n",
    "    - tokens (list): List of words (tokens) from the sentence.\n",
    "    - labels (list): Corresponding NER labels (X, Y, O).\n",
    "\n",
    "    Returns:\n",
    "    - converted_tokens (list): Tokens with 'Y' values converted to numbers.\n",
    "    \"\"\"\n",
    "    # Bengali number words mapping\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4,\n",
    "        \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶è‡¶ó‡¶æ‡¶∞‡ßã\": 11, \"‡¶¨‡¶æ‡¶∞‡ßã\": 12, \"‡¶§‡ßá‡¶∞‡ßã\": 13, \"‡¶ö‡ßå‡¶¶‡ßç‡¶¶\": 14,\n",
    "        \"‡¶™‡¶®‡ßá‡¶∞‡ßã\": 15, \"‡¶∑‡ßã‡¶≤\": 16, \"‡¶∏‡¶§‡ßá‡¶∞‡ßã\": 17, \"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã\": 18, \"‡¶ä‡¶®‡¶ø‡¶∂\": 19,\n",
    "        \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶è‡¶ï‡ßÅ‡¶∂\": 21, \"‡¶¨‡¶æ‡¶á‡¶∂\": 22, \"‡¶§‡ßá‡¶á‡¶∂\": 23, \"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 24,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶ø‡¶∂\": 25, \"‡¶õ‡¶æ‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 26, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂\": 27, \"‡¶Ü‡¶†‡¶æ‡¶∂\": 28, \"‡¶ä‡¶®‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 29,\n",
    "        \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 31, \"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 32, \"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 33, \"‡¶ö‡ßå‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 34,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 35, \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 36, \"‡¶∏‡¶æ‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 37, \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 38, \"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 39,\n",
    "        \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶è‡¶ï‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 41, \"‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 42, \"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 43, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 44,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 45, \"‡¶õ‡¶ø‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 46, \"‡¶∏‡¶æ‡¶§‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 47, \"‡¶Ü‡¶ü‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 48, \"‡¶ä‡¶®‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 49,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®\": 51, \"‡¶¨‡¶æ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 52, \"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 53, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 54,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶®‡ßç‡¶®\": 55, \"‡¶õ‡¶æ‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 56, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡ßç‡¶®\": 57, \"‡¶Ü‡¶ü‡¶æ‡¶®‡ßç‡¶®\": 58, \"‡¶ä‡¶®‡¶∑‡¶æ‡¶ü\": 59,\n",
    "        \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 61, \"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 62, \"‡¶§‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 63, \"‡¶ö‡ßå‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 64,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 65, \"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 66, \"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 67, \"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 68, \"‡¶ä‡¶®‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 69,\n",
    "        \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶è‡¶ï‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 71, \"‡¶¨‡¶æ‡¶π‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 72, \"‡¶§‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 73, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 74,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 75, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 76, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 77, \"‡¶Ü‡¶ü‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 78, \"‡¶ä‡¶®‡¶Ü‡¶∂‡¶ø\": 79,\n",
    "        \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶è‡¶ï‡¶æ‡¶∂‡¶ø\": 81, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 82, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 83, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø\": 84,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶∂‡¶ø\": 85, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶∂‡¶ø\": 86, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂‡¶ø\": 87, \"‡¶Ü‡¶ü‡¶æ‡¶∂‡¶ø\": 88, \"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 89,\n",
    "        \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90, \"‡¶è‡¶ï‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 91, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 92, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 93, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 94,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 95, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 96, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 97, \"‡¶Ü‡¶ü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 98, \"‡¶®‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 99,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500,\n",
    "        \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000,\n",
    "        \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "\n",
    "    # Convert words to numbers\n",
    "    converted_tokens = []\n",
    "    current_number = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        word = tokens[i]\n",
    "        label = labels[i]\n",
    "\n",
    "        if label == \"Y\":\n",
    "            if word.isdigit():  # If it's already a number, store as integer\n",
    "                current_number += int(word)\n",
    "            elif word in bengali_number_map:  # Convert Bengali number words\n",
    "                temp_number = bengali_number_map[word]\n",
    "\n",
    "                # Look ahead for a multiplier\n",
    "                if (i + 1) < len(tokens) and tokens[i + 1] in multipliers:\n",
    "                    temp_number *= multipliers[tokens[i + 1]]\n",
    "                    i += 1  # Skip the multiplier\n",
    "\n",
    "                current_number += temp_number\n",
    "            else:\n",
    "                converted_tokens.append(word)  # Keep other words unchanged\n",
    "        else:\n",
    "            if current_number > 0:\n",
    "                converted_tokens.append(current_number)\n",
    "                current_number = 0  # Reset for next number\n",
    "            converted_tokens.append(word)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # If there's an unprocessed number at the end\n",
    "    if current_number > 0:\n",
    "        converted_tokens.append(current_number)\n",
    "\n",
    "    return converted_tokens\n",
    "\n",
    "\n",
    "# Run the function\n",
    "token,labels=valid_pairs = validate_xy_pairs(tokens, labels)\n",
    "\n",
    "# Output valid pairs\n",
    "print(\"‚úÖ Valid X-Y Pairs:\", valid_pairs)\n",
    "converted_tokens = convert_bengali_text_to_number(tokens, labels)\n",
    "\n",
    "\n",
    "print(\"‚úÖ Converted Tokens:\", converted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "def convert_bengali_text_to_number(tokens, labels):\n",
    "    \"\"\"Pairs `X` labels with `Y` words and prepares them for conversion.\"\"\"\n",
    "    \n",
    "    # Define Bengali Number Words and Multipliers\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4, \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500, \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "\n",
    "    paired_results = {}  # Store results\n",
    "    current_x = None\n",
    "    current_y_words = []  # Store all Y words\n",
    "    unit = None  # Store unit if found\n",
    "    last_multiplier = None  # Track last multiplier to prevent duplication\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        word, label = tokens[i], labels[i]\n",
    "\n",
    "        if label == \"X\":\n",
    "            if current_x and current_y_words:  \n",
    "                # Store previous X-Y pair with its unit (before conversion)\n",
    "                paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n",
    "                current_y_words = []  \n",
    "                unit = None  \n",
    "                last_multiplier = None  # Reset last multiplier\n",
    "\n",
    "            current_x = word  # Assign new X entity\n",
    "\n",
    "        elif label == \"Y\":\n",
    "            current_y_words.append(word)  # Store Y word\n",
    "\n",
    "            # Check if it's a multiplier and store only once\n",
    "            if word in multipliers and last_multiplier != word:\n",
    "                unit = word  # Store the first occurrence as the unit\n",
    "                last_multiplier = word  # Prevent duplicate multipliers\n",
    "\n",
    "        i += 1  \n",
    "\n",
    "    # Store the last X-Y pair\n",
    "    if current_x and current_y_words:\n",
    "        paired_results[current_x] = {\"words\": current_y_words, \"unit\": unit}\n",
    "\n",
    "    return paired_results, bengali_number_map, multipliers\n",
    "\n",
    "\n",
    "def convert_text_to_number(paired_results, bengali_number_map, multipliers):\n",
    "    \"\"\"\n",
    "    Converts Bengali number words into numeric values while keeping the unit.\n",
    "\n",
    "    Args:\n",
    "    - paired_results (dict): Mapping of `X` to `Y` words before conversion.\n",
    "    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n",
    "    - multipliers (dict): Mapping of Bengali multipliers (‡¶π‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶ï‡ßã‡¶ü‡¶ø, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - converted_results (dict): Mapping of `X` to final numeric values with units.\n",
    "    \"\"\"\n",
    "    converted_results = {}\n",
    "\n",
    "    for x_entity, data in paired_results.items():\n",
    "        y_words = data[\"words\"]\n",
    "        unit = data[\"unit\"] if data[\"unit\"] else \"\"\n",
    "\n",
    "        # Convert Bengali number words to a single numerical value\n",
    "        number_text = \" \".join(y_words)\n",
    "        number_value = convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers)\n",
    "\n",
    "        # Store the final result\n",
    "        converted_results[x_entity] = f\"{number_value} {unit}\"\n",
    "\n",
    "    return converted_results\n",
    "\n",
    "\n",
    "def convert_bengali_text_to_number_helper(number_text, bengali_number_map, multipliers):\n",
    "    \"\"\"\n",
    "    Helper function to convert Bengali text numbers into numeric values.\n",
    "\n",
    "    Args:\n",
    "    - number_text (str): Bengali number words.\n",
    "    - bengali_number_map (dict): Mapping of Bengali words to numbers.\n",
    "    - multipliers (dict): Mapping of Bengali multipliers (‡¶π‡¶æ‡¶ú‡¶æ‡¶∞, ‡¶ï‡ßã‡¶ü‡¶ø, etc.).\n",
    "\n",
    "    Returns:\n",
    "    - int: Converted numeric value.\n",
    "    \"\"\"\n",
    "    words = number_text.split()\n",
    "    total = 0\n",
    "    temp_value = 0\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word.isdigit():\n",
    "            temp_value += int(word)\n",
    "        elif word in bengali_number_map:\n",
    "            temp_value += bengali_number_map[word]\n",
    "        elif word in multipliers:\n",
    "            temp_value *= multipliers[word]\n",
    "            total += temp_value\n",
    "            temp_value = 0\n",
    "\n",
    "    return total + temp_value\n",
    "\n",
    "\n",
    "# Example Input\n",
    "tokens = [\"‡¶Ü‡¶Æ‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶§‡¶ø‡¶®\", \"‡¶ï‡ßã‡¶ü‡¶ø\", \"‡¶¶‡ßÅ‡¶á\", \"‡¶≤‡¶ï‡ßç‡¶∑\", \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\", \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\", \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\", \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \"‡¶Ü‡¶õ‡ßá\",\n",
    "          \",\", \"‡¶§‡ßã‡¶Æ‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶Ü‡¶õ‡ßá\", \"‡ß´‡ß¶‡ß¶‡ß¶\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \",\", \"‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ\", \"‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞\", \"‡¶ï‡¶æ‡¶õ‡ßá\", \"‡¶ü‡¶æ‡¶ï‡¶æ\", \"‡¶Ü‡¶õ‡ßá\", \"‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"]\n",
    "\n",
    "labels = [\"X\", \"O\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"O\",\n",
    "          \"O\", \"X\", \"O\", \"O\", \"Y\", \"Y\", \"O\", \"O\", \"X\", \"O\", \"O\", \"O\", \"Y\"]\n",
    "\n",
    "# Step 1: Pair `X` with `Y` words before conversion\n",
    "paired_results, bengali_number_map, multipliers = convert_bengali_text_to_number(tokens, labels)\n",
    "\n",
    "# Step 2: Convert `Y` words into numeric values\n",
    "final_results = convert_text_to_number(paired_results, bengali_number_map, multipliers)\n",
    "\n",
    "# Output the result\n",
    "print(\"üîπ **Before Conversion:**\")\n",
    "for key, data in paired_results.items():\n",
    "    print(f\"{key} -> {' '.join(data['words'])} {data['unit'] if data['unit'] else ''}\")\n",
    "\n",
    "print(\"\\nüîπ **After Conversion:**\")\n",
    "for key, value in final_results.items():\n",
    "    print(f\"{key} -> {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# Example text\n",
    "txt = \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶§‡¶ø‡¶® ‡¶ï‡ßã‡¶ü‡¶ø ‡¶¶‡ßÅ‡¶á ‡¶≤‡¶ï‡ßç‡¶∑ ‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂ ‡¶π‡¶æ‡¶ú‡¶æ‡¶∞ ‡¶ö‡¶æ‡¶∞‡¶∂‡ßã ‡¶§‡ßç‡¶∞‡¶ø‡¶∂ ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá, ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶Ü‡¶õ‡ßá ‡ß´‡ß¶‡ß¶‡ß¶ ‡¶ü‡¶æ‡¶ï‡¶æ, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Æ‡¶ø‡¶®‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶ü‡¶æ‡¶ï‡¶æ ‡¶Ü‡¶õ‡ßá ‡ß´‡ß™‡ßØ‡ß¶‡ßÆ\"\n",
    "\n",
    "# Call predict function (ensure predict_on_text returns [(token, label), ...])\n",
    "predictions = predict_on_text(model, txt, tokenizer, max_seq_len=100)\n",
    "\n",
    "# Extract tokens and labels\n",
    "tokens = [token for token, label in predictions]\n",
    "labels = [label for token, label in predictions]\n",
    "\n",
    "def extract_xy_pairs(data):\n",
    "    pairs = {}\n",
    "    current_x = None\n",
    "    current_y = []\n",
    "    \n",
    "    for word, label in data:\n",
    "        if label == 'X':\n",
    "            if current_x and current_y:\n",
    "                pairs[current_x] = ' '.join(current_y)\n",
    "            current_x = word\n",
    "            current_y = []\n",
    "        elif label == 'Y':\n",
    "            if current_x:\n",
    "                current_y.append(word)\n",
    "    \n",
    "    if current_x and current_y:\n",
    "        pairs[current_x] = ' '.join(current_y)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Extract pairs\n",
    "xy_pairs = extract_xy_pairs(predictions)  # Directly pass predictions\n",
    "\n",
    "# Print the result\n",
    "for x, y in xy_pairs.items():\n",
    "    print(f\"{x} -> {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def extract_xy_pairs(data):\n",
    "    pairs = {}\n",
    "    units = {}\n",
    "    current_x = None\n",
    "    current_y = []\n",
    "    current_unit = None\n",
    "    unit_list = []\n",
    "    \n",
    "    # Define Bengali Number Words and Multipliers\n",
    "    bengali_number_map = {\n",
    "        \"‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø\": 0, \"‡¶è‡¶ï\": 1, \"‡¶¶‡ßÅ‡¶á\": 2, \"‡¶§‡¶ø‡¶®\": 3, \"‡¶ö‡¶æ‡¶∞\": 4, \"‡¶™‡¶æ‡¶Å‡¶ö\": 5, \"‡¶õ‡¶Ø‡¶º\": 6, \"‡¶∏‡¶æ‡¶§\": 7, \"‡¶Ü‡¶ü\": 8, \"‡¶®‡¶Ø‡¶º\": 9,\n",
    "        \"‡¶¶‡¶∂\": 10, \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500, \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "    \n",
    "    for i, (word, label) in enumerate(data):\n",
    "        if label == 'X':\n",
    "            if current_x and current_y:\n",
    "                pairs[current_x] = ' '.join(current_y)\n",
    "                units[current_x] = current_unit\n",
    "                unit_list.append(current_unit if current_unit else \"null\")\n",
    "            current_x = word\n",
    "            current_y = []\n",
    "            current_unit = None\n",
    "        elif label == 'Y':\n",
    "            if current_x:\n",
    "                current_y.append(word)\n",
    "            if word not in bengali_number_map and word not in multipliers:\n",
    "                if not word.isdigit():  # Ensure it's not a number before marking it as a unit\n",
    "                    current_unit = word\n",
    "    \n",
    "    if current_x and current_y:\n",
    "        pairs[current_x] = ' '.join(current_y)\n",
    "        units[current_x] = current_unit\n",
    "        unit_list.append(current_unit if current_unit else \"null\")\n",
    "    \n",
    "    return pairs, units, unit_list\n",
    "\n",
    "def normalize_units(numeric_results, unit_list):\n",
    "    \"\"\"Converts all values to the smallest detected unit if necessary.\"\"\"\n",
    "    conversion_map = {\"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000, \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1, \"‡¶≤‡¶ï‡ßç‡¶∑\": 0.1}  # Convert billions to millions\n",
    "    \n",
    "    # Detect the most frequently occurring unit (smallest denominator)\n",
    "    valid_units = [unit for unit in unit_list if unit in conversion_map]\n",
    "    if not valid_units:\n",
    "        return numeric_results, unit_list  # No unit conversions needed\n",
    "    \n",
    "    target_unit = min(valid_units, key=lambda x: conversion_map[x])  # Find lowest unit\n",
    "    normalized_results = {}\n",
    "    new_unit_list = []\n",
    "    \n",
    "    for (key, value), unit in zip(numeric_results.items(), unit_list):\n",
    "        if unit in conversion_map:\n",
    "            factor = conversion_map[unit] / conversion_map[target_unit]\n",
    "            normalized_results[key] = value * factor\n",
    "            new_unit_list.append(target_unit)\n",
    "        else:\n",
    "            normalized_results[key] = value\n",
    "            new_unit_list.append(unit)\n",
    "    \n",
    "    return normalized_results, new_unit_list\n",
    "def parse_bengali_float(token: str):\n",
    "    \"\"\"\n",
    "    Translates Bengali digits in the token to ASCII digits,\n",
    "    then attempts to convert it to a float.\n",
    "    Returns None if unsuccessful.\n",
    "    \"\"\"\n",
    "    # Make a translation map for Bengali digits -> ASCII digits\n",
    "    translation_map = str.maketrans(\"‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ\", \"0123456789\")\n",
    "    \n",
    "    # Translate the token to ASCII form\n",
    "    ascii_token = token.translate(translation_map)\n",
    "    \n",
    "    try:\n",
    "        return float(ascii_token)  # If \"‡ß´.‡ß®\" => \"5.2\" => 5.2 (float)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_bengali_text_to_number(xy_pairs_dict, units):\n",
    "    \"\"\"Converts Bengali number words into numerical values and retains units.\"\"\"\n",
    "    bengali_number_map = {\n",
    "        '‡¶è‡¶ï': 1, '‡¶™‡ßç‡¶∞‡¶•‡¶Æ': 1, '‡¶™‡¶π‡ßá‡¶≤‡¶æ': 1,\n",
    "        '‡¶¶‡ßÅ‡¶á': 2, '‡¶¶‡ßç‡¶¨‡¶ø‡¶§‡ßÄ‡¶Ø‡¶º': 2, '‡¶¶‡ßã‡¶∏‡¶∞‡¶æ': 2,\n",
    "        '‡¶§‡¶ø‡¶®': 3, '‡¶§‡ßÉ‡¶§‡ßÄ‡¶Ø‡¶º': 3,\n",
    "        '‡¶ö‡¶æ‡¶∞': 4, '‡¶ö‡¶§‡ßÅ‡¶∞‡ßç‡¶•': 4,\n",
    "        '‡¶™‡¶æ‡¶Å‡¶ö': 5, '‡¶™‡¶û‡ßç‡¶ö‡¶Æ': 5,\n",
    "        '‡¶õ‡¶Ø‡¶º': 6, '‡¶∑‡¶∑‡ßç‡¶†': 6,\n",
    "        '‡¶∏‡¶æ‡¶§': 7, '‡¶∏‡¶™‡ßç‡¶§‡¶Æ': 7,\n",
    "        '‡¶Ü‡¶ü': 8, '‡¶Ö‡¶∑‡ßç‡¶ü‡¶Æ': 8,\n",
    "        '‡¶®‡¶Ø‡¶º': 9, '‡¶®‡¶¨‡¶Æ': 9,\n",
    "        '‡¶¶‡¶∂': 10, '‡¶¶‡¶∂‡¶Æ': 10, \"‡¶è‡¶ó‡¶æ‡¶∞‡ßã\": 11, \"‡¶¨‡¶æ‡¶∞‡ßã\": 12, \"‡¶§‡ßá‡¶∞‡ßã\": 13, \"‡¶ö‡ßå‡¶¶‡ßç‡¶¶\": 14,\n",
    "        \"‡¶™‡¶®‡ßá‡¶∞‡ßã\": 15, \"‡¶∑‡ßã‡¶≤\": 16, \"‡¶∏‡¶§‡ßá‡¶∞‡ßã\": 17, \"‡¶Ü‡¶†‡¶æ‡¶∞‡ßã\": 18, \"‡¶ä‡¶®‡¶ø‡¶∂\": 19,\n",
    "        \"‡¶¨‡¶ø‡¶∂\": 20, \"‡¶è‡¶ï‡ßÅ‡¶∂\": 21, \"‡¶¨‡¶æ‡¶á‡¶∂\": 22, \"‡¶§‡ßá‡¶á‡¶∂\": 23, \"‡¶ö‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 24,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶ø‡¶∂\": 25, \"‡¶õ‡¶æ‡¶¨‡ßç‡¶¨‡¶ø‡¶∂\": 26, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂\": 27, \"‡¶Ü‡¶†‡¶æ‡¶∂\": 28, \"‡¶ä‡¶®‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 29,\n",
    "        \"‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 30, \"‡¶è‡¶ï‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 31, \"‡¶¨‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 32, \"‡¶§‡ßá‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 33, \"‡¶ö‡ßå‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 34,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 35, \"‡¶õ‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 36, \"‡¶∏‡¶æ‡¶á‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 37, \"‡¶Ü‡¶ü‡¶§‡ßç‡¶∞‡¶ø‡¶∂\": 38, \"‡¶ä‡¶®‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 39,\n",
    "        \"‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 40, \"‡¶è‡¶ï‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 41, \"‡¶¨‡¶ø‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 42, \"‡¶§‡ßá‡¶§‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 43, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 44,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 45, \"‡¶õ‡¶ø‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 46, \"‡¶∏‡¶æ‡¶§‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 47, \"‡¶Ü‡¶ü‡¶ö‡¶≤‡ßç‡¶≤‡¶ø‡¶∂\": 48, \"‡¶ä‡¶®‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 49,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶∂\": 50, \"‡¶è‡¶ï‡¶æ‡¶®‡ßç‡¶®\": 51, \"‡¶¨‡¶æ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 52, \"‡¶§‡¶ø‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 53, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®\": 54,\n",
    "        \"‡¶™‡¶û‡ßç‡¶ö‡¶æ‡¶®‡ßç‡¶®\": 55, \"‡¶õ‡¶æ‡¶™‡ßç‡¶™‡¶æ‡¶®‡ßç‡¶®\": 56, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡ßç‡¶®\": 57, \"‡¶Ü‡¶ü‡¶æ‡¶®‡ßç‡¶®\": 58, \"‡¶ä‡¶®‡¶∑‡¶æ‡¶ü\": 59,\n",
    "        \"‡¶∑‡¶æ‡¶ü\": 60, \"‡¶è‡¶ï‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 61, \"‡¶¨‡¶æ‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 62, \"‡¶§‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 63, \"‡¶ö‡ßå‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 64,\n",
    "        \"‡¶™‡¶Å‡¶á‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 65, \"‡¶õ‡ßá‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 66, \"‡¶∏‡¶æ‡¶§‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 67, \"‡¶Ü‡¶ü‡¶∑‡¶ü‡ßç‡¶ü‡¶ø\": 68, \"‡¶ä‡¶®‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 69,\n",
    "        \"‡¶∏‡¶§‡ßç‡¶§‡¶∞\": 70, \"‡¶è‡¶ï‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 71, \"‡¶¨‡¶æ‡¶π‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 72, \"‡¶§‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 73, \"‡¶ö‡ßÅ‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 74,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 75, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 76, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 77, \"‡¶Ü‡¶ü‡¶æ‡¶§‡ßç‡¶§‡¶∞\": 78, \"‡¶ä‡¶®‡¶Ü‡¶∂‡¶ø\": 79,\n",
    "        \"‡¶Ü‡¶∂‡¶ø\": 80, \"‡¶è‡¶ï‡¶æ‡¶∂‡¶ø\": 81, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 82, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶∂‡¶ø\": 83, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶∂‡¶ø\": 84,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶∂‡¶ø\": 85, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶∂‡¶ø\": 86, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶∂‡¶ø\": 87, \"‡¶Ü‡¶ü‡¶æ‡¶∂‡¶ø\": 88, \"‡¶ä‡¶®‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 89,\n",
    "        \"‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 90, \"‡¶è‡¶ï‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 91, \"‡¶¨‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 92, \"‡¶§‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 93, \"‡¶ö‡ßÅ‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 94,\n",
    "        \"‡¶™‡¶Å‡¶ö‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 95, \"‡¶õ‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 96, \"‡¶∏‡¶æ‡¶§‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 97, \"‡¶Ü‡¶ü‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 98, \"‡¶®‡¶ø‡¶∞‡¶æ‡¶®‡¶¨‡ßç‡¶¨‡¶á\": 99,\n",
    "        \"‡¶è‡¶ï‡¶∂‡ßã\": 100, \"‡¶¶‡ßÅ‡¶á‡¶∂‡ßã\": 200, \"‡¶§‡¶ø‡¶®‡¶∂‡ßã\": 300, \"‡¶ö‡¶æ‡¶∞‡¶∂‡ßã\": 400, \"‡¶™‡¶æ‡¶Å‡¶ö‡¶∂‡ßã\": 500,\n",
    "        \"‡¶õ‡¶Ø‡¶º‡¶∂‡ßã\": 600, \"‡¶∏‡¶æ‡¶§‡¶∂‡ßã\": 700, \"‡¶Ü‡¶ü‡¶∂‡ßã\": 800, \"‡¶®‡¶Ø‡¶º‡¶∂‡ßã\": 900\n",
    "    }\n",
    "\n",
    "    multipliers = {\n",
    "        \"‡¶π‡¶æ‡¶ú‡¶æ‡¶∞\": 1000, \"‡¶≤‡¶ï‡ßç‡¶∑\": 100000, \"‡¶≤‡¶æ‡¶ñ\": 100000, \"‡¶ï‡ßã‡¶ü‡¶ø\": 10000000,\n",
    "        \"‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000, \"‡¶¨‡¶ø‡¶≤‡¶ø‡ßü‡¶®\": 1000000000\n",
    "    }\n",
    "    \n",
    "    numeric_dict = {}\n",
    "    for x_label, y_text in xy_pairs_dict.items():\n",
    "        tokens = y_text.split()\n",
    "        total_value = 0\n",
    "        part_sum = 0\n",
    "\n",
    "        for w in tokens:\n",
    "            # 1) Try integer\n",
    "            if w.isdigit():\n",
    "                part_sum += int(w)\n",
    "\n",
    "            # 2) Try dictionary\n",
    "            elif w in bengali_number_map:\n",
    "                part_sum += bengali_number_map[w]\n",
    "\n",
    "            # 3) Try multiplier\n",
    "            elif w in multipliers:\n",
    "                part_sum *= multipliers[w]\n",
    "                total_value += part_sum\n",
    "                part_sum = 0\n",
    "\n",
    "            # 4) Attempt to parse decimal (float)\n",
    "            else:\n",
    "                val = parse_bengali_float(w)\n",
    "                if val is not None:\n",
    "                    part_sum += val\n",
    "                else:\n",
    "                    # Possibly a unit or unknown word\n",
    "                    pass\n",
    "        \n",
    "        # Once we finish all tokens for this X, update total_value and the dictionary\n",
    "        total_value += part_sum\n",
    "        numeric_dict[x_label] = total_value\n",
    "\n",
    "    # Now we have processed **all** X-Y pairs\n",
    "    return numeric_dict\n",
    "\n",
    "    \n",
    "def get_dynamic_ticks(min_val: float, max_val: float):\n",
    "    \"\"\"\n",
    "    Generates y-axis tick values divided into 10 equal steps based on min and max values.\n",
    "    \"\"\"\n",
    "    if max_val <= min_val:\n",
    "        return [min_val]  # Edge case\n",
    "\n",
    "    step = (max_val - min_val) // 10\n",
    "    ticks = [round(min_val + i * step, 2) for i in range(11)]  # 11 ticks = 10 steps\n",
    "    return ticks\n",
    "\n",
    "def convert_to_bengali(num):\n",
    "    bengali_digits = '‡ß¶‡ßß‡ß®‡ß©‡ß™‡ß´‡ß¨‡ß≠‡ßÆ‡ßØ'\n",
    "    return ''.join(bengali_digits[int(digit)] if digit.isdigit() else digit for digit in str(num))\n",
    "\n",
    "\n",
    "# # Extract pairs and units\n",
    "# xy_pairs, xy_units, unit_list = extract_xy_pairs(predictions)\n",
    "\n",
    "# # Convert Y values to numeric format\n",
    "# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n",
    "\n",
    "# # Normalize units\n",
    "# normalized_values, normalized_units = normalize_units(numeric_values, unit_list)\n",
    "\n",
    "# # Print results\n",
    "# for idx, (x, y) in enumerate(normalized_values.items(), 1):\n",
    "#     print(f\"{x} -> {y}\")\n",
    "\n",
    "# # Print unit mapping\n",
    "# for idx, unit in enumerate(normalized_units, 1):\n",
    "#     print(f\"unit{idx} -> {unit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert Y values to numeric format\n",
    "# numeric_values = convert_bengali_text_to_number(xy_pairs, xy_units)\n",
    "\n",
    "# Convert values to Bengali numerals for display\n",
    "x_values = ['‡¶ó‡¶§ ‡¶¶‡¶∂‡¶ï‡ßá', '‡¶Ü‡¶ó‡¶æ‡¶Æ‡ßÄ ‡¶™‡¶æ‡¶Å‡¶ö ‡¶¨‡¶õ‡¶∞‡ßá']\n",
    "y_values = [75,200]\n",
    "unit_list = ['‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®', '‡¶Æ‡¶ø‡¶≤‡¶ø‡ßü‡¶®']\n",
    "\n",
    "y_values_bengali = [convert_to_bengali(value) + \" \" + unit for value, unit in zip(y_values, unit_list)]\n",
    "\n",
    "max_val = max(y_values) if y_values else 0\n",
    "min_val = min(y_values) if y_values else 0\n",
    "print(min_val)\n",
    "print(max_val)\n",
    "\n",
    "y_tick_vals = get_dynamic_ticks(min_val, max_val)\n",
    "y_tick_text_bengali = [convert_to_bengali(val) for val in y_tick_vals]\n",
    "\n",
    "# Create the Plotly bar chart\n",
    "fig = go.Figure([go.Bar(\n",
    "    x=x_values,\n",
    "    y=y_values,\n",
    "    width=0.6,\n",
    "    text=y_values_bengali,\n",
    "    textfont=dict(size=15),\n",
    "    textposition='auto',\n",
    "    marker=dict(color='blue')\n",
    ")])\n",
    "\n",
    "# Update layout with Bengali tick labels and fonts\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        tickfont=dict(size=13),\n",
    "        title=dict(text=\"‡¶®‡¶æ‡¶Æ\", font=dict(size=13))  # X-axis label in Bengali\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickvals=y_tick_vals,\n",
    "        ticktext=y_tick_text_bengali,\n",
    "        tickfont=dict(size=15),\n",
    "        title=dict(text=\"‡¶Æ‡¶æ‡¶®\", font=dict(size=13))  # Y-axis label in Bengali\n",
    "    ),\n",
    "    width=600,\n",
    "    height=400,\n",
    "    margin=dict(l=50, r=50, t=50, b=50)\n",
    ")\n",
    "\n",
    "# Show and save the plot\n",
    "fig.show()\n",
    "fig.write_html('generated_chart.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "(Moyna has 53 taka. Rohima has 159 taka. Mohona has 318 taka. Hena has 79.5 taka.)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6710212,
     "sourceId": 11124909,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7063017,
     "sourceId": 11299613,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 65478,
     "modelInstanceId": 47680,
     "sourceId": 56830,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
